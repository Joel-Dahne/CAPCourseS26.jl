var documenterSearchIndex = {"docs":
[{"location":"week-9-lab/#Week-9-Lab:-TODO","page":"Week 9 Lab: TODO","title":"Week 9 Lab: TODO","text":"","category":"section"},{"location":"week-11-lecture-2/#Week-11-Lecture-2:-TODO","page":"Week 11 Lecture 2: TODO","title":"Week 11 Lecture 2: TODO","text":"","category":"section"},{"location":"week-9-lecture-1/#Week-9-Lecture-1:-TODO","page":"Week 9 Lecture 1: TODO","title":"Week 9 Lecture 1: TODO","text":"","category":"section"},{"location":"week-4-lecture-2/#Week-4-Lecture-2:-TODO","page":"Week 4 Lecture 2: TODO","title":"Week 4 Lecture 2: TODO","text":"","category":"section"},{"location":"week-9-lecture-2/#Week-9-Lecture-2:-TODO","page":"Week 9 Lecture 2: TODO","title":"Week 9 Lecture 2: TODO","text":"","category":"section"},{"location":"week-10-lecture-1/#Week-10-Lecture-1:-TODO","page":"Week 10 Lecture 1: TODO","title":"Week 10 Lecture 1: TODO","text":"","category":"section"},{"location":"week-8-lecture-1/#Week-8-Lecture-1:-TODO","page":"Week 8 Lecture 1: TODO","title":"Week 8 Lecture 1: TODO","text":"","category":"section"},{"location":"week-12-lecture-2/#Week-12-Lecture-2:-TODO","page":"Week 12 Lecture 2: TODO","title":"Week 12 Lecture 2: TODO","text":"","category":"section"},{"location":"week-14-lab/#Week-14-Lab:-TODO","page":"Week 14 Lab: TODO","title":"Week 14 Lab: TODO","text":"","category":"section"},{"location":"week-13-lecture-2/#Week-13-Lecture-2:-TODO","page":"Week 13 Lecture 2: TODO","title":"Week 13 Lecture 2: TODO","text":"","category":"section"},{"location":"week-2-lab/#Week-2-Lab:-Julia-basics","page":"Week 2 Lab: Julia basics","title":"Week 2 Lab: Julia basics","text":"While the course doesn't directly assume any familiarity with Julia, we also won't have the time for a thorough introduction of the entire Julia language. The goal of this lab is to build some familiarity with the basics of Julia.\n\nSome of the things we will look at in this lab are:\n\nFunctions\nTypes\nLoops\nPlotting\n\nFor this we will use the Pluto notebook lab-2.jl that you can find in the notebooks directory. You can start this notebook in the same way as you started the lab-1.jl notebook in Lab 1. Navigate to the notebooks directory of this repository and start Julia. You can then activate the directory project and start Pluto with\n\nusing Pkg\nPkg.activate(\".\")\nusing Pluto\nPluto.run()\n\ntip: Tip\nIf you get an error of the form \"Package Pluto not found, but a package named Pluto is available from a registry.\" then you might have activated the wrong project. Make sure that you start Julia from the notebooks directory.\n\nIf you can't find the lab-2.jl file inside the notebooks directory you might not have the latest version of this repository. You can update to the latest version by running\n\ngit pull\n\nIf you have made any changes you might need to first remove those before it allows you to pull. You can revert all the changes with\n\ngit restore --hard\n\nor you can revert individual files with\n\ngit restore name_of_file","category":"section"},{"location":"week-2-lab/#High-level-overview-of-Julia","page":"Week 2 Lab: Julia basics","title":"High-level overview of Julia","text":"While not necessary to start using Julia, it can be beneficial to know some of the things that make Julia what it is. Many of these points require experience with other programming languages to have something to compare to.\n\nDynamically typed: You don't have to specify the types of variables, but you can if you want to. This differs from for example C where you always have to specify the types, and Python where you most of the time do not specify the type (modern Python has some support for types though).\nJIT (Just In Time) compiled: Julia compiles the code before it runs it, which allows it to generate optimized code. Compared to many other languages the compilation is however not done in a separate step, but rather the compilation happens as you are running the code. This gives you the performance of a compiled language, but the flexibility of a dynamic language. There are of course downsides to this as well, the most notable one being that sometimes it can take quite some time to compile the code.\nGarbage collected: In some programming languages, most notably C, the programmer is in charge of managing the memory that the program uses. Most modern languages (with some notable exceptions) defer the memory handling to a process known as garbage collection. This is convenient when writing the code, but in some cases comes with performance issues. We will see some of these issues when working with high precision intervals later in the course.\n1-based indexing: Different programming languages use different conventions regarding whether 0 or 1 is the first index in an array. Julia, together with for example Matlab and Fortran, uses 1-based indexing, meaning that the first index in an array is 1. Python and C (and many other languages) instead use 0-based indexing, where the first index is 0. In mathematics we usually switch indexing depending on context, e.g. matrices are indexed starting from 1, whereas polynomial coefficients are indexed from 0. For some reason people have strong opinions about this.\nMultiple dispatch: An important part of what makes Julia Julia is that it makes use of multiple dispatch for function overloading. This allows you to define multiple versions of a function. Which version is being used is determined based on the type of the input arguments. This is in particular very useful for rigorous numerics since it makes it relatively easy to write code that works for both non-rigorous floating point numbers as well as rigorous interval arithmetic. We will see more examples of how this works in practice later in the course.","category":"section"},{"location":"week-6-lecture-1/#Week-6-Lecture-1:-TODO","page":"Week 6 Lecture 1: TODO","title":"Week 6 Lecture 1: TODO","text":"","category":"section"},{"location":"week-12-lecture-1/#Week-12-Lecture-1:-TODO","page":"Week 12 Lecture 1: TODO","title":"Week 12 Lecture 1: TODO","text":"","category":"section"},{"location":"week-14-lecture-2/#Week-14-Lecture-2:-TODO","page":"Week 14 Lecture 2: TODO","title":"Week 14 Lecture 2: TODO","text":"","category":"section"},{"location":"week-14-lecture-1/#Week-14-Lecture-1:-TODO","page":"Week 14 Lecture 1: TODO","title":"Week 14 Lecture 1: TODO","text":"","category":"section"},{"location":"week-1-lab/#Week-1-Lab:-Setup","page":"Week 1 Lab: Setup","title":"Week 1 Lab: Setup","text":"The main goal of this lab is to:\n\nInstall Julia\nStart the Julia REPL and run basic commands\nDownload the course package\nRun a Pluto notebook\nBonus: Build this documentation\nInstall an editor for working with Julia files\nBonus: Reproduce a computer-assisted proof","category":"section"},{"location":"week-1-lab/#Install-Julia","page":"Week 1 Lab: Setup","title":"Install Julia","text":"The Julia website has a page about install Julia. The recommended way to do this is to use juliaup. Exactly how to install juliaup depends on your operating system. On Linux and Mac it should be enough to run\n\ncurl -fsSL https://install.julialang.org | sh\n\nOn Windows you can install it through the Windows store. Alternatively you can use WSL and install Julia inside the WSL environment.","category":"section"},{"location":"week-1-lab/#Julia-REPL","page":"Week 1 Lab: Setup","title":"Julia REPL","text":"The most common way to use Julia is through the interactive command-line REPL (real-eval-print-loop). See the Julia REPL documentation for much more details than we'll cover here.\n\nIf you start Julia (by running julia from a terminal) you will be greeted with a prompt where you can run Julia commands. You can for example try running the following commands\n\n1 + 1\nsin(2)\nf(x) = x - exp(x)\nf(0.1)\nf(-1)\nM = [1 2; 3 4]\ninv(M)\nb = [5, 6]\na = M \\ b\nM * a","category":"section"},{"location":"week-1-lab/#REPL-modes","page":"Week 1 Lab: Setup","title":"REPL modes","text":"When you start the Julia REPL you enter what is called the \"Julian mode\", where you can run Julia code and see the results. The REPL also has 3 other modes which are used in certain cases:\n\nHelp mode: You enter this mode by writing ? when at an empty line in the Julian mode. You can then write the name of a function to get the documentation for that function. Try writing for example ?sin and ?Int.\nPkg mode: You can enter this mode by writing ]. It is used for installing and interacting with packages, which we will do soon. Note that you can exit the mode by typing backspace.\nShell mode: You can enter this mode by writing ;. It us used for running simple terminal commands directly from Julia. We won't make much use of it directly, but it is occasionally useful.","category":"section"},{"location":"week-1-lab/#Download-course-package","page":"Week 1 Lab: Setup","title":"Download course package","text":"This documentation is structured as a Julia package, with the name CAPCourseS26.jl. It is available in a Github repository. Most of the code we will work on in the course will be contained in this package. To use the package we first have to download it.\n\nTo download the package we want to use git, the version control tool that Github derives it names from.\n\nIf you are running Linux you probably already have git installed, on Debian based systems (e.g. Ubuntu) you can otherwise install it with sudo apt install git.\nOn Mac you should be able to install it by just running git in the terminal and follow the prompts. You can also install it in other ways, see Mac install instructions.\nOn Windows you can use the git installer linked from the Windows install instructions.\n\nOnce you have git installed you should be able to download the repository. To do so navigate to the directory where you want to put the repository and run\n\ngit clone https://github.com/Joel-Dahne/CAPCourseS26.jl.git\n\nThis should create a directory with the name CAPCourseS26.jl. If you navigate into CAPCourseS26.jl and start Julia you should then be able to use the following code to run the tests in the package.\n\nusing Pkg\nPkg.activate(\".\") # Activate the package CAPCourseS26\nPkg.instantiate() # Install all dependencies of CAPCourseS26\nPkg.test() # Run the tests for CAPCourseS26\n\nIf everything is working as it should this should, after some time, end with something of the form.\n\n     Testing Running tests...\nTest Summary: | Pass  Total  Time\nCAPCourses26  |    1      1  0.0s\n     Testing CAPCourseS26 tests passed\n","category":"section"},{"location":"week-1-lab/#Pluto-notebook","page":"Week 1 Lab: Setup","title":"Pluto notebook","text":"In addition to the Julia REPL we will make use of Pluto notebooks as a tool for writing, running and interacting with Julia code. Another common notebook tool which some of you might have encountered is Jupyter. Jupyter also works with Julia (that is the Ju in Jupyter), but for this course we will mainly make use of Pluto, which is Julia only.\n\nTo start Pluto navigate to the notebooks directory of this repository and start Julia. You can activate the directory project and start Pluto with\n\nusing Pkg\nPkg.activate(\".\")\nusing Pluto\nPluto.run()\n\nThis should print a link which you can open in your browser. From the browser you should then start the lab-1.jl notebook and follow the directions in it.","category":"section"},{"location":"week-1-lab/#Bonus:-Build-this-documentation","page":"Week 1 Lab: Setup","title":"Bonus: Build this documentation","text":"You can build the documentation that you are currently reading by running the following code from the root of this directory.\n\njulia --project=docs docs/make.jl\n\nThis will build an HTML version of the package. To be able to read this in a convenient way requires some more tools. If you have Python installed you should be able to do so with\n\ncd docs/build\npython3 -m http.server --bind localhost\n\nand open http://127.0.0.1:8000/ in your browser. Alternatively you can build a pdf version of the documentation with\n\njulia --project=docs docs/make.jl latex\n\nand open the PDF-file docs/build/CAPCourseS26.jl.pdf.","category":"section"},{"location":"week-1-lab/#Install-an-editor","page":"Week 1 Lab: Setup","title":"Install an editor","text":"While it is possible to write Julia code in any text editor, one usually uses an editor which has tools for working with Julia code.\n\nIf you don't know which editor to use you should probably use VS Code. Follow the instructions on their website to install it.\n\nPersonally, I use Emacs as my editor. Unless you have previous experience with Emacs I would however not recommend starting with this. If you do want to use Emacs, these are the most relevant parts of my configuration. Feel free to ask for more details in case you are interested.\n\n(use-package julia-mode\n  :ensure julia-mode\n  :bind (\"C-c f\" . jd/julia-format)\n  :config (setq julia-max-block-lookback 50000))\n\n(use-package julia-ts-mode\n  :ensure t\n  :mode (\"\\\\.jl\" . julia-ts-mode)\n  :hook (julia-ts-mode . auto-revert-mode))\n\n(defun jd/processor-count ()\n  \"Get the number of processors using nproc. If nproc is not found it returns 1\"\n  (if (executable-find \"nproc\")\n      (with-temp-buffer\n        (call-process (executable-find \"nproc\") nil t nil)\n        (string-to-number (buffer-string)))\n    1))\n\n(defun jd/julia-repl-activate-cd-parent ()\n  \"Run julia-repl-activate-parent and also cd to the directory of the project file.\"\n  (interactive)\n  (progn\n    (julia-repl-activate-parent nil)\n    (if-let ((projectfile (julia-repl--find-projectfile)))\n        (progn\n          (message \"cd-ing to %s\" projectfile)\n          (julia-repl--send-string\n           (concat \"cd(\\\"\"\n                   (expand-file-name (file-name-directory projectfile)) \"\\\")\")))\n      (message \"could not find project file\"))))\n\n(use-package vterm\n    :ensure t\n    :config (progn (setq vterm-kill-buffer-on-exit nil)))\n\n(use-package julia-repl\n  :ensure t\n  :hook julia-mode\n  :config\n  (julia-repl-set-terminal-backend 'vterm)\n  (setenv \"JULIA_NUM_THREADS\" (number-to-string (/ (jd/processor-count) 2)))\n  (define-key julia-repl-mode-map (kbd \"C-c C-a\") 'jd/julia-repl-activate-cd-parent))\n\nFor most people the editor is likely the place where they most of the time interact with Julia. It is therefore a good idea to get accustomed to the editor and the tools it offers. When showing Julia code throughout the course I'll try to switch between using VS Code and Emacs, to give you a feeling for how things differ between editors.\n\nWe will take a closer look at how to use the editor for working with Julia code next week.","category":"section"},{"location":"week-1-lab/#Bonus:-Reproduce-a-computer-assisted-proof","page":"Week 1 Lab: Setup","title":"Bonus: Reproduce a computer-assisted proof","text":"If you feel inclined you can try to use the things you have learned above to reproduce a computer-assisted proof from a very recent paper. At the time of writing the paper has been submitted to Arxiv, but is still waiting to appear. At this point we have of course not learned enough to actually understand the proof, but you can nevertheless run the code.\n\nIn the Github repository for the paper is all the code used for the computer-assisted proof. The proofs are presented in Pluto notebooks and our goal will be to run one of these notebooks. To do this we need to:\n\nClone the repository\nStart Julia from the repository\nInstall dependencies and start the Pluto notebook\n\nTo clone the repository you can navigate to the directory where you want to place it and run\n\ngit clone https://github.com/Joel-Dahne/SpectralRegularPolygon.jl.git\n\nNext step is to start Julia from the SpectralRegularPolygon.jl directory. Here we need to make one adjustment from before however. At the time of writing the latest Julia version is 1.12.4 and this is the one that was automatically installed when you installed juliaup earlier. The computer-assisted proof was however done using Julia 1.11.8, so we need to run that version. You can install the version with\n\njuliaup add 1.11.8\n\nand to start the correct Julia version you then run\n\njulia +1.11.8\n\nNote the extra +1.11.8! After you have started the right Julia version you should be able to follow the instructions in the README of the repository. Which is to first run\n\nusing Pkg\nPkg.activate(\".\")\nPkg.instantiate()\nPkg.test()\n\nto install the dependencies and run the package tests. This will likely take some time! Next you want to start Pluto with\n\nusing Pluto\nPluto.run()\n\nThis should open Pluto in your browser. From your browser window you can then open the notebooks inside the proofs directory. There is quite a few of them and some of them take some time to run. I would recommend you open the one named proposition_3_1.jl, which has the nicest plots!\n\nnote: Note\nThe instructions here are slightly different from the earlier ones about how to start Pluto. In the earlier instructions you should navigate to the notebooks subdirectory and run Pkg.activate(\".\") from there. In these instructions you should run Pkg.activate(\".\") from the root of the directory. The reason for this difference is that Julia 1.12 introduced some features that simplifies some parts of project managements. Since we are here using Julia 1.11.8 these tools are not in place and the approach is slightly different.\n\nIf you have managed to get this far and run the above mentioned notebook, then congratulations! You have just reproduced a computer-assisted proof!","category":"section"},{"location":"week-3-lab/#Week-3-Lab:-A-simple-computer-assisted-proof-(and-VS-Code)","page":"Week 3 Lab: A simple computer-assisted proof (and VS Code)","title":"Week 3 Lab: A simple computer-assisted proof (and VS Code)","text":"The goal of this lab is to reproduce a simple computer-assisted proof by Alex and Mitch. In doing so we will use VS Code in order to get used to working with Julia from that interface.","category":"section"},{"location":"week-3-lab/#VS-Code-setup","page":"Week 3 Lab: A simple computer-assisted proof (and VS Code)","title":"VS Code setup","text":"Start VS Code\nOpen the course folder. You can find Open Folder under File (shortcut Ctrl-K Ctrl-O).\nCreate a new file lab-3.jl in the directory scratch/labs (which you will have to create). You can find New File under File (shortcut Ctrl-Alt-Super-N).\nCopy the following code into the file\nusing Arblib\n\nx = Arb(π)\ny = Arb(3)\n\nprintln(x)\n\nThere are multiple different ways to run the code in the file.\n\nExecute entire file in a separate process. Use the shortcut Ctrl-F5.\nExecute entire file in REPL. Press the arrow in the top right corner.\nExecute the current line in the REPL. Use the shortcut Ctrl-Enter. You can also select multiple lines and it will execute all of them.\nExecute the current line in the REPL and move to the next line. Use the shortcut Shift-Enter.\n\nTry all of them and see how they work! Which of these alternatives is best depends on your workflow. In my case I almost exclusively make use of alternatives 3 and 4.\n\nFor this lab we will write all of our code inside this one file and use the above commands for running it. For larger projects it is usually a good idea to split the work into multiple files and in many cases it is beneficial to structure the project like a Julia package. We will get back to this later in the course.","category":"section"},{"location":"week-3-lab/#A-simple-computer-assisted-proof","page":"Week 3 Lab: A simple computer-assisted proof (and VS Code)","title":"A simple computer-assisted proof","text":"Our goal will be to prove Proposition II.2 in paper mentioned above. For this we need the following polynomial\n\nP = 1 - 3alpha^2 + alpha^4 - frac11149alpha^6 + frac143294alpha^8 - frac753693311957764alpha^10 allowbreak + frac459817233147460365316alpha^12 - frac30028809212865451520327364608478700alpha^14 allowbreak + frac4975014185899222712487856750603488800alpha^16\n\nand the function\n\nE(alpha) = frac6alpha^915 - 20alpha left( 1 + sqrt3alpha + sqrt2alpha^2 + fracsqrt147alpha^3 + fracsqrt25842alpha^4 + fracsqrt19688373458alpha^5 + fracsqrt10652579931122alpha^6 + frac2sqrt2129312323981473624696345alpha^7 + fracsqrt1836431197552144544997570760alpha^8 right) + frac9alpha^18(15 - 20alpha)^2\n\nWe want to prove the following proposition\n\nnote: Proposition\nThe function P(alpha) + E(alpha) is negative at alpha = 061 and the function P(alpha) + E(alpha) is positive at alpha = 057.\n\nWhat they are actually trying to prove in the paper is that a function v(alpha) has a zero on the interval 057 061. The polynomial P is an approximation of v and E(alpha) gives an upper bound of the error for the approximation.\n\nnote: Note\nThe proof in the paper is not quite rigorous. It uses regular floating points together with results that gives you bound on the rounding errors for evaluating polynomials with floating points. It does however not quite account for all the places where rounding errors are introduced.Moreover, the expression for E(alpha) seems to not quite be correct. This is being looked into.","category":"section"},{"location":"week-3-lab/#A-not-quite-correct-implementation","page":"Week 3 Lab: A simple computer-assisted proof (and VS Code)","title":"A not quite correct implementation","text":"Our first task will be to implement P and E in Julia. A direct (but as we will see slightly problematic) conversion of the above expressions for P and E to Julia is given below. You can copy the code into your lab-3.jl file (you can also remove what is already there if you want).\n\nP(α) =\n    1 - 3α^2 + α^4 - 111 / 49 * α^6 + 143 / 294 * α^8 - 7536933 / 11957764 * α^10 +\n    4598172331 / 47460365316 * α^12 - 30028809212865451 / 520327364608478700 * α^14 +\n    49750141858992227 / 12487856750603488800 * α^16\n\nE(α) =\n    6α^9 / (15 - 20α) * (\n        1 +\n        sqrt(3) * α +\n        sqrt(2) * α^2 +\n        sqrt(14) / 7 * α^3 +\n        sqrt(258) / 42 * α^4 +\n        sqrt(1_968_837) / 3458 * α^5 +\n        sqrt(106_525_799) / 31_122 * α^6 +\n        2sqrt(2_129_312_323_981_473) / 624_696_345 * α^7 +\n        sqrt(183_643_119_755_214_454) / 4_997_570_760 * α^8\n    ) + 9α^18 / (15 - 20α)^2\n\nWe can now implement functions computing lower and upper bounds of v as\n\nv_lower(α) = P(α) - E(α)\nv_upper(α) = P(α) + E(α)\n\nTry evaluating v_lower at alpha = 057 and v_upper at alpha = 061. You should get\n\nv_lower(0.57)\nv_upper(0.61)\n\nOne can show (which we will soon do) that\n\nP(057) - E(057) = 002857494349754851842380087961496259058289166470274019410736704418784480353dots\n\nand\n\nP(061) + E(061) = -0018830737780969134528206854817563710242534970158515810047488029826855742148dots\n\nTo what extent do your computed values above agree with these values? If they disagree, what could be the reason? We can also evaluate the functions to higher precision in Julia using BigFloat, Julia's standard type for arbitrary precision floating points.\n\nv_lower(BigFloat(0.57))\nv_upper(BigFloat(0.61))\n\nTo what extent do these values agree with the above given ones? What could be the reason for them not agreeing?","category":"section"},{"location":"week-3-lab/#An-improved-implementation","page":"Week 3 Lab: A simple computer-assisted proof (and VS Code)","title":"An improved implementation","text":"The issue with the above implementation of P(alpha) and E(alpha) is that Julia by default uses Float64 for numerical computations unless told otherwise. This happens for example when dividing integers with each other and when computing square roots of integers.\n\n1 / 3\nsqrt(2)\n\nTo make our implementation of P and E work for rigorous numerics we need to make sure that all intermediate computations are also done in a rigorous way. For integer division one way of achieving this is to represent the coefficient as a rational number, which is an exact representation.\n\n1 / 3 # Performs floating point arithmetic\n1 // 3 # Represents it as a rational number\n\nThis plays well with for example BigFloat (and rigorous numerics as well)\n\n(1 / 3) * BigFloat(3) # Not close to 1\n(1 // 3) * BigFloat(3) # Very close to 1\n\nLet us define a new function P_correct which represents the coefficients as rational numbers.\n\nP_correct(α) =\n    1 - 3α^2 + α^4 - 111 // 49 * α^6 + 143 // 294 * α^8 - 7536933 // 11957764 * α^10 +\n    4598172331 // 47460365316 * α^12 - 30028809212865451 // 520327364608478700 * α^14 +\n    49750141858992227 // 12487856750603488800 * α^16\n\nFor the square roots we don't have a way of exactly representing the values (at least not without using any external package). Instead we take the approach of converting the integer to the right type before computing the square root.\n\nBigFloat(sqrt(2))^2 # This gives large errors\nsqrt(BigFloat(2))^2 # This is very precise\n\nFor the function E the type we want to convert the integers is given by the type of the argument α. For this you can use the function oftype.\n\na = BigFloat(2)\noftype(a, 1 // 3) # Convert 1 // 3 to the type of a\n\nUsing this for E we get\n\nE_correct(α) =\n    6α^9 / (15 - 20α) * (\n        1 +\n        sqrt(oftype(α, 3)) * α +\n        sqrt(oftype(α, 2)) * α^2  +\n        sqrt(oftype(α, 14)) / 7 * α^3  +\n        sqrt(oftype(α, 258)) / 42 * α^4  +\n        sqrt(oftype(α, 1_968_837)) / 3458 * α^5  +\n        sqrt(oftype(α, 106_525_799)) / 31_122 * α^6  +\n        2sqrt(oftype(α, 2_129_312_323_981_473)) / 624_696_345 * α^7  +\n        sqrt(oftype(α, 183_643_119_755_214_454)) / 4_997_570_760 * α^8\n    ) + 9α^18 / (15 - 20α)^2\n\nNote that we are not converting the integers we are dividing by. Why do we not need to do that?\n\nLet us finally define\n\nv_lower_correct(α) = P_correct(α) - E_correct(α)\nv_upper_correct(α) = P_correct(α) + E_correct(α)\n\nAfter this we can now compute\n\nv_lower_correct(BigFloat(0.57))\nv_upper_correct(BigFloat(0.61))\n\nHow does this compare to the values given above? What is the issue now?\n\nThe last issue comes from using the constants 0.57 and 0.61. We have\n\nBigFloat(0.57)\nBigFloat(0.61)\nBigFloat(\"0.57\")\nBigFloat(\"0.61\")\n\nFinally we thus compute\n\nv_lower_correct(BigFloat(\"0.57\"))\nv_upper_correct(BigFloat(\"0.61\"))\n\nNow it should agree with the values given above!\n\nnote: Note\nThis last fix, using BigFloat(\"0.57\") instead of BigFloat(0.57), is not strictly necessary for a computer-assisted proof. There is nothing special about the value 057 = 57  100, we could just as well use the value 056999999999999995115018691649311222136020660400390625 which is the exact value for the Float64 number that is closest to 057 = 57  100. Of course, the statement in the paper would look slightly awkward: \"it is positive at alpha = 056999999999999995115018691649311222136020660400390625\" doesn't look quite as nice as \"it is positive at alpha = 057\"","category":"section"},{"location":"week-3-lab/#A-computer-assisted-proof","page":"Week 3 Lab: A simple computer-assisted proof (and VS Code)","title":"A computer-assisted proof","text":"So far we have of course not proved anything, we have still only evaluated the functions using floating point arithmetic which will give us rounding errors. With the P_correct and E_correct implementations it is however very straightforward to fully prove the result. For this we will use the Arblib.jl package which implements one type of interval arithmetic. To load the package you can add this to the top of your file (it doesn't have to be at the top, but you usually put packages there)\n\nusing Arblib\n\nWe can now evaluate our functions using interval arithmetic\n\nv_lower_correct(Arb(\"0.57\"))\nv_upper_correct(Arb(\"0.61\"))\n\nThis gives us fully rigorous enclosures for the values. From these enclosures we immediately see that P(057) - E(057) is positive and P(061) + E(061) is negative. We have thus finished the proof!","category":"section"},{"location":"week-3-lab/#Bonus:-Tighten-enclosure-of-root","page":"Week 3 Lab: A simple computer-assisted proof (and VS Code)","title":"Bonus: Tighten enclosure of root","text":"So far we have proved that there is a root in the interval 057 061. Can you compute a tighter enclosure of the root?","category":"section"},{"location":"week-3-lecture-1/#Week-3-Lecture-1:-Computer-assisted-proofs-for-continuous-problems","page":"Week 3 Lecture 1: Computer-assisted proofs for continuous problems","title":"Week 3 Lecture 1: Computer-assisted proofs for continuous problems","text":"Last week we looked at some examples of computer-assisted proofs for discrete problems, this included\n\nThe four color theorem\nThe boolean Pythagorean triples problem\nGoldbach's weak conjecture\n\nThis week we will look at computer-assisted proofs applied to continuous problems. Since computers are inherently discrete machines, working with continuous problems requires a slightly different approach.\n\nComputer-assisted proofs are not applicable to all types of problems. Throughout the course we will talk about what type of problems are good candidates for computer-assisted proofs and by the end of the course my hope is that you will have at least a rough idea for when a computer-assisted proof could be applied to a problem. I have three guiding principles as to which problems are good candidates for computer-assisted proofs. For a computer-assisted proof to be applicable to a problem the result should be:\n\nNumerically obvious\nStable under perturbations\nOn a compact domain\n\nExpanding a bit on these points:\n\nRigorous numerical methods will never give you better results than classical non-rigorous methods. If you cannot produce very convincing numerical evidence that the result is true, then any attempt to rigorously prove it is doomed to fail.\nNumerical methods will always produce approximations. Rigorous numerical methods can give you bounds for the errors of these approximations, but cannot eliminate them. When setting up the problem there is however usually a lot of freedom in what type of perturbations the result should be stable under.\nIn the end the result needs to reduce to a finite computation and that requires some sort of compactness. If you have an infinite domain it is in many cases possible to compactify it, usually at the cost of introducing singularities at the boundaries that need to be dealt with.\n\nThe example we will look at in this lecture is from a recent paper from my own research and is related to the field of spectral geometry. We will look at what spectral geometry is, the problem the paper handles and how it relates to the guiding principles mentioned above. The paper in question is:\n\nMonotonicity of the first Dirichlet eigenvalue of regular polygons (code)\n\nTwo other papers that make use of similar methods, and which we might look more at later in the course, are:\n\nComputation of Tight Enclosures for Laplacian Eigenvalues (code)\nA counterexample to Payne’s nodal line conjecture with few holes (code)","category":"section"},{"location":"week-3-lecture-1/#Spectral-geometry","page":"Week 3 Lecture 1: Computer-assisted proofs for continuous problems","title":"Spectral geometry","text":"In spectral geometry one studies how the eigenvalues and eigenfunctions of the Laplacian depend on the domain. In our case we are interested in problems of the form\n\nbegincases\n  -Delta u = lambda u textinquad Omega\n  u = 0 textonquad partialOmega\nendcases\n\nHere -Delta u = lambda u means that u is an eigenfunction of the Laplacian, with associated eigenvalue lambda. Recall that in the plane, Delta is just the sum of the second derivatives\n\nDelta u = fracpartial^2 upartial x^2 + fracpartial^2 upartial y^2\n\nThe eigenvalue equation is then combined with a boundary condition, in this case u = 0 on the boundary of Omega. One can consider other types of boundary conditions, but for our purposes here we will stay with u = 0, corresponding to a zero Dirichlet boundary condition.\n\nIf Omega is reasonably well behaved there is a countable sequence of eigenvalues, lambda_1  lambda_2 leq lambda_3 leq dots, all with an associated eigenfunction u_k. In spectral geometry we are interested in how these eigenvalues and eigenfunctions depend on the domain Omega.","category":"section"},{"location":"week-3-lecture-1/#Monotonicity-of-the-first-Dirichlet-eigenvalue-of-regular-polygons","page":"Week 3 Lecture 1: Computer-assisted proofs for continuous problems","title":"Monotonicity of the first Dirichlet eigenvalue of regular polygons","text":"In this case we consider domains mathbbP_N which are regular polygons and we are interested in how the first eigenvalue of these polygons, lambda_1(mathbbP_N) depend on N. For our purposes we want to normalize the polygons so that they have area pi.\n\nIf we plot lambda_1(mathbbP_N) as a function of N we have the following picture\n\n(Image: First eigenvalue of regular polygons)\n\nFrom this picture one could maybe conjecture that the eigenvalues are decreasing with N. This picture is however maybe not the most convincing, we only go up to N = 64 and for the larger values the graph mostly looks flat in this plot. To get a slightly better figure let us start by noting that as N goes to infinity we would have that the polygons approach the unit circle. For the unit circle the eigenvalue can be explicitly computed; it's the first root of the Bessel function J_0. If we plot the difference between lambda(mathbbP_N) and this value and give the y-axis a logarithmic scale we get (ignore the error bound for now, we will get back to that).\n\n(Image: First eigenvalue of regular polygons compared to unit disc)\n\nThis is pretty strong numerical evidence that eigenvalues are monotone, at least up to N = 64. Of course, a lot of the details are in how these values are actually computed. There are approximations errors in these values, can we trust that they are small enough not to change the conclusion?\n\nLet us consider this problem in terms of the above discussed guidelines for computer-assisted proofs. Let us start with considering the project only for the finite set N = 3 4 dots 64.\n\nNumerically obvious: The above figure gives fairly convincing numerical results.\nStable under perturbations: If we perturb the eigenvalues slightly the monotonicity should still hold.\nOn a compact domain: When restricted to N = 3 4 dots 64 the domain is clearly compact.\n\nThis indicates that a computer-assisted proof might be viable. Of course, for a complete result we need to also handle all N geq 64. How can we achieve that? The rough idea is to see the eigenfunctions not as a function of N, but as a function of eta = frac1N. We then have to prove monotonicity of the eigenvalues for all eta in 0 164. This gives a compact domain, at the cost of very singular behavior at eta = 0.","category":"section"},{"location":"week-3-lecture-1/#Handling-N-3,-4,-\\dots,-64","page":"Week 3 Lecture 1: Computer-assisted proofs for continuous problems","title":"Handling N = 3 4 dots 64","text":"Let us focus on the finite case of proving that lambda(mathbbP_N) for N = 3 4 dots 64. Handling the infinite limit builds on some of the same ideas, but requires significantly more work.\n\nThe question to ask is, how were the approximate eigenvalues in the figures above computed? There are many different ways of numerically computing eigenvalues, some of them more suited for rigorous verification.\n\nIn this case the computations were done using the Method of Particular Solutions, see the paper Reviving the Method of Particular Solutions for a more thorough overview of the method. Recall that we are trying to solve the equation\n\nbegincases\n  -Delta u = lambda u textinquad Omega\n  u = 0 textonquad partialOmega\nendcases\n\nThe idea is to approximate u using a linear combination of basis functions,\n\nu_app(x y) = sum_i = 1^M c_i phi_i(x y)\n\nHow should the basis functions phi_i and the coefficients c_i be chosen? The basis functions we choose so that they satisfy the equation -Deltaphi_i = lambda_appphi_i exactly for the approximate eigenvalue lambda_app, but they will not satisfy any specific boundary condition. We then choose the coefficients c_i as to make the resulting linear combination as close on the boundary as possible. This gives us an approximation u_app, lambda_app with\n\nbegincases\n  -Delta u_app = lambda_app u_app textinquad Omega\n  u_app approx 0 textonquad partialOmega\nendcases\n\nThe precise choice of phi_i depends highly on the domain and we won't go into the details here. For this specific case the approximation used is\n\nu_app(x y) = J_0left(rsqrtlambdaright)\n+ a_2J_Nleft(rsqrtlambdaright)cos Ntheta\n+ sum_n = 1^N left(\n  b_1J_alphaleft(r_nsqrtlambdaright)sin alphatheta_n\n  + b_2J_2alphaleft(r_nsqrtlambdaright)sin 2alphatheta_n\nright)\n\nHere alpha = pi  N, r is the distance from (x y) to the center of the domain, and (r_n theta_n) are the polar coordinates of the point (x y) when centered around vertex n and the orientation taken so that theta_n = 0 corresponds to the boundary segment between vertex n and n + 1.\n\nSo far there is nothing in this approach which is related to rigorous numerics. This is just a classical numerical approach which we can use to compute approximations of eigenvalues and eigenfunctions. What makes this approach suitable for a computer-assisted proof is the following theorem by Fox, Henrici and Moler.\n\nnote: Theorem\nLet OmegasubsetmathbbR^n be bounded. Let lambda_app and u_app be an approximate eigenvalue and eigenfunction–-that is, they satisfy Delta u_app+lambda_app u_app=0 in Omega but not necessarily u_app = 0 on~partialOmega. Define  mu = fracsqrtOmegasup_x in partial Omegau_app(x)u_app_2where Omega is the area of the domain. Then there exists an eigenvalue lambda such that  fraclambda_app - lambdalambda leq mu\n\nThe value mu measures how close to zero the approximate eigenfunction is on the boundary. If mu is very small, meaning that the approximate eigenfunction is very close to zero on the boundary, then the theorem guarantees us that there is a true eigenvalue lambda close to our approximate eigenvalue lambda_app.\n\nIf we can find a bound for mu, which only requires us to bound an explicit function on an explicit domain, we can then get upper bounds for the error in our approximations of the eigenvalues. If these upper bounds are sufficiently small we can still verify the monotonicity of the eigenvalues! The approach for bounding mu is something we will talk more about later in the course.\n\nThe proof for the monotonicity then follows from the following figure. It shows the difference lambda_1(mathbbP_N) - lambda_1(mathbbP_N + 1) for the approximate eigenvalues. For the eigenvalues to be monotone this difference must be positive. It also shows the error for each point, coming from the sum of the error for the two eigenvalues. Since the error is smaller than the value, the difference must be positive also for the true eigenvalues.\n\n(Image: Difference between eigenvalues of regular polygons)","category":"section"},{"location":"week-6-lecture-2/#Week-6-Lecture-2:-TODO","page":"Week 6 Lecture 2: TODO","title":"Week 6 Lecture 2: TODO","text":"","category":"section"},{"location":"#Topics-course-in-computer-assisted-proofs-Spring-2026","page":"Overview","title":"Topics course in computer-assisted proofs Spring 2026","text":"This website, together with the associated repository, contains the material for a topics course in computer-assisted proofs and rigorous numerics given at the University of Minnesota Spring 2026.\n\nnote: Note\nThis material is primarily meant to serve as lecture notes for the course. It is made public with the hope that it could potentially be useful for others as well. The material is, however, intended to be used together with the lectures and labs in the course and could at times be difficult to follow by itself. It's written in a fairly informal style and some of the content is opinionated in nature.\n\nThe general idea of computer-assisted proofs (in analysis), is to build on the massive success of numerical methods in applied mathematics and other sciences and apply them also for mathematical proofs. Classical numerical methods are however not suitable for direct use in proofs, since they introduce errors (rounding and discretization errors). These errors hinder their use in proofs, which require fully rigorous arguments. The area of rigorous numerics tackles these issues by introducing methods to control the errors in a fully rigorous way, that allows for the results to be used in proofs.\n\nAn early, and by now classical, example of a computer-assisted proof is the proof of the existence of the Lorenz attractor in 1999 by Tucker. Over the more than two decades since Tucker's proof, there has been an increase in the adoption of computer assisted proofs in analysis. An example of a recent breakthrough result building on computer-assisted proofs is the proof of blowup for the 3D Euler equation.\n\nThe course is split into 3 parts distributed over 15 weeks. The 3 parts are\n\nIntroduction to computer-assisted proofs (≈ Week 1-4)\nIntroduction to rigorous numerics (≈ Week 5-10)\nComputer-assisted proofs in practice (≈ Week 11-15)\n\nA rough schedule for the first two parts is given below, the precise details for the third part are yet to be determined.\n\nWeek Topic\n1 Introduction to computer-assisted proofs\n2 Discrete problems\n3 Continuous problems\n4 Formal proofs\n5 Floating points and interval arithmetic\n6 Floating points and interval arithmetic\n7 Basic rigorous numerics\n8 Automatic differentiation\n9 Improved rigorous numerics\n10 Improved rigorous numerics\n11 TBD\n12 TBD\n13 TBD\n14 TBD\n15 TBD","category":"section"},{"location":"#Part-1:-Introduction-to-computer-assisted-proofs","page":"Overview","title":"Part 1: Introduction to computer-assisted proofs","text":"The first part of the course will be a general introduction to computer-assisted proofs, from the point of view of rigorous numerics. We will consider both discrete problems, such as the proof of the Four color theorem, and continuous problems, such as the proof of existence of the Lorenz attractor.\n\nComputer-assisted proofs are often mixed up with formal proofs, such as those produced by Lean and Rocq (previously known as Coq), indeed some authors use the term computer-assisted proofs to refer to either. We will take a brief look at formal proofs, with a focus on the differences and similarities between formal proofs and computer-assisted proofs.","category":"section"},{"location":"#Part-2:-Introduction-to-rigorous-numerics","page":"Overview","title":"Part 2: Introduction to rigorous numerics","text":"The second part focuses on the machinery required for constructing computer-assisted proofs in analysis, known as rigorous numerics. This introduction will be partially based on the book Validated Numerics by Warwick Tucker. At the heart of rigorous numerics lies interval arithmetic, indeed the field of rigorous numerics is sometimes just referred to as interval arithmetic.\n\nThe practical parts will primarily be done in Julia, using the packages Arblib.jl and IntervalArithmetic.jl as the base for the interval arithmetic.\n\nHere is an example of how interval arithmetic looks like in practice, here using IntervalArithmetic.jl.\n\nusing IntervalArithmetic\na = interval(1, 2) # The interval [1, 2]\na^2 # Interval gotten from squaring all numbers in [1, 2]\nsin(a) # Interval gotten from applying sin to all numbers in [1, 2]\n\nThe content we will cover includes:\n\nMathematical foundations of floating point arithmetic: floating point formats, rounding\nBasics of interval arithmetic: basic arithmetic, elementary functions, special functions\nBasic rigorous numerics: isolating roots, computing integrals, enclosing extrema\nAutomatic differentiation: forward (and backwards) differentiation, Taylor arithmetic\nImproved rigorous numerics: isolating roots, computing integrals, enclosing extrema","category":"section"},{"location":"#Part-3:-Computer-assisted-proofs-in-practice","page":"Overview","title":"Part 3: Computer-assisted proofs in practice","text":"In the third and final part of the course we will look at what it takes to go from what we have learned about interval arithmetic and rigorous numerics to actually creating a computer-assisted proof. The list of topics covered would depend on the interests of the participants in the course. Possible topics would include the use of computer-assisted proofs in:\n\nSpectral geometry\nDynamical systems\nFluid mechanics\nAnalytic combinatorics\n\nOne could also dive deeper into different algorithms for computer-assisted proofs:\n\nRigorous integration of ODEs\nFinite element methods\nSpectral methods\nPhysics-Informed Neural Networks (PINNs)\n\nAlternatively, one can study the lower level details of interval arithmetic, more related to the field of computer algebra.","category":"section"},{"location":"week-15-lecture-1/#Week-15-Lecture-1:-TODO","page":"Week 15 Lecture 1: TODO","title":"Week 15 Lecture 1: TODO","text":"","category":"section"},{"location":"week-7-lab/#Week-7-Lab:-TODO","page":"Week 7 Lab: TODO","title":"Week 7 Lab: TODO","text":"","category":"section"},{"location":"week-8-lecture-2/#Week-8-Lecture-2:-TODO","page":"Week 8 Lecture 2: TODO","title":"Week 8 Lecture 2: TODO","text":"","category":"section"},{"location":"week-2-lecture-2/#Week-2-Lecture-2:-Integer-arithmetic","page":"Week 2 Lecture 2: Integer arithmetic","title":"Week 2 Lecture 2: Integer arithmetic","text":"In this lecture we will focus not on any specific computer-assisted proof, but rather on one of the fundamental building blocks for computer-assisted proofs, integer arithmetic. The lecture will also serve as a bit of introduction to parts of the Julia programming language. It could be useful to open a Julia REPL on the side while reading this and experiment with some of the code shown.\n\nTo make use of computers for proofs we need to be able to trust the computations they do. For numerical analysis, which relies on floating point numbers and approximations, trusting the computer requires a bit of work. This is what we will get to later in the course when we talk about rigorous numerics. For integer arithmetic most people would however probably agree that the computer can be trusted to do it right.\n\nWhen we in a couple of weeks get to rigorous numerics and more closely study floating points we will see that everything in the end reduces to integer arithmetic. So it makes sense to look a bit closer at this and understand how it works.","category":"section"},{"location":"week-2-lecture-2/#Integer-types","page":"Week 2 Lecture 2: Integer arithmetic","title":"Integer types","text":"There are many different ways to represent integers on the computer, the most important aspects in the choice of representation being the size of the integers and if one needs negative integers or not. For our purposes we will mostly work with integer types that do allow negative numbers, called signed integers. We will briefly talk about unsigned integers at the end of this section.\n\nFor the size one could either have a fixed predetermined size that can represent integers up to some fixed bound, or a variable size that can represent arbitrarily large integers (until your computer runs out of memory). Julia has the types Int8, Int16, Int32, Int64 and Int128 for representing fixed-width integers, where the number indicates the number of bits used to store the integer. The minimum and maximum integer that is representable using these types can be found using typemin and typemax respectively.\n\ntypemin(Int8)\ntypemax(Int8)\n\ntypemin(Int16), typemax(Int16)\n\ntypemin(Int32), typemax(Int32)\n\ntypemin(Int64), typemax(Int64)\n\ntypemin(Int128), typemax(Int128)\n\nThe default integer type on most modern computers is Int64, the Int type in Julia is a shorthand for the default type. If you directly write an integer it will be of type Int.\n\nInt\ntypeof(5)\n\nFor representing arbitrary-sized integers Julia has the type BigInt. In this case there is no typemin or typemax defined\n\ntypemin(BigInt)\ntypemax(BigInt)\n\nYou can convert an integer from one type to any other type. If the integer is too large to be represented in the new type an error is thrown.\n\nInt16(5)\ntypeof(Int16(5))\n\nBigInt(1000)\nbig(1000) # Shorthand for BigInt(1000) in this case\n\nInt8(300) # 300 doesn't fit in one Int8\n\nYou can see exactly what bits are used to represent an integer using bitstring\n\nbitstring(7)\nbitstring(2^14)\nbitstring(Int32(2^14))\nbitstring(Int16(2^14))\nbitstring(Int8(2^14)) # Too small to fit 2^14\nbitstring(BigInt(2^14)) # bitstring doesn't work for BigInt\n\nFor fixed-width integers negative values are represented using two's complement, whereas arbitrary-sized integers usually have a separate bit that keeps track of the sign. The details here are not so important for our purposes though.\n\nbitstring(-7)\nbitstring(Int32(-7))\nbitstring(Int16(-7))","category":"section"},{"location":"week-2-lecture-2/#Arithmetic","page":"Week 2 Lecture 2: Integer arithmetic","title":"Arithmetic","text":"How arithmetic of integers is done depends on their type.\n\nFor Int32 and Int64 basic arithmetic is typically implemented directly in hardware on the CPU. So you don't write a program for multiplying two such integers, you use the implementation on the CPU. The correctness of this procedure is therefore dependent on the correctness of the CPU, which for even remotely modern CPU you can assume.\n\nFor smaller integer types, e.g. Int8 and Int16, arithmetic is often times handled by converting them to Int32 or Int64, doing the operation and then converting back. One would in general not expect these operations to be any faster than those for Int32 and Int64. Some hardware might have specialized instructions for these, in which case it could be much faster though.\n\nFor BigInt the arithmetic is implemented in software. Internally they are built up of a list of Int64 that is treated as one large integer. Operations are then implemented by combining several operations for Int64 values. For addition this is relatively simple and something you could implement yourself with a bit of time. For multiplication this becomes extremely complicated and requires both highly sophisticated mathematical methods and carefully crafted implementations to achieve top performance. The BigInt type in Julia is internally based on the GMP library which contains highly specialized code for operating on such integers.","category":"section"},{"location":"week-2-lecture-2/#Overflow","page":"Week 2 Lecture 2: Integer arithmetic","title":"Overflow","text":"For fixed-width integer types we have to somehow handle when the value is too large to be represented by the type. For conversion to integer types we have already seen that it throws an error if the value doesn't fit.\n\nInt8(300) # 300 doesn't fit in one Int8\n\nWhen doing arithmetic on integers it does however not throw an error, instead the result wraps around.\n\ntypemin(Int8), typemax(Int8) # Recall these values\ntypemax(Int8) + Int8(1)\n\ntypemin(Int64), typemax(Int64) # Recall these values\ntypemax(Int64) + 1\n\nWhen this happens it is called integer overflow. The behavior here does however depend on the programming language used. For example in C, integer overflow is considered undefined behavior and a program which exhibits overflow is not guaranteed to work as expected. In Julia, and many other languages, the behavior is well defined. For Int64 the behavior is isomorphic to mathbbZ_2^64, with the representatives centered around zero.\n\nThis overflow behavior means that you have to be careful when working with integers for mathematical purposes. For fixed-width integer arithmetic to faithfully represent integer arithmetic you have to ensure (prove) that your operations never overflow. In many cases overflow is not a problem. For example, computing Pythagorean triples up to 1784 using Int64 will clearly not give you any issues with overflow. But if you want to verify Goldbach's weak conjecture up to 10^30 approx 2^100 you won't be able to do it using Int64. If you want to be on the safe side you can always use BigInt, which never overflows (it will just crash if you run out of memory). In some programming languages, e.g. Python, the default is that integers are represented using an arbitrary-sized representation.\n\nWith these issues coming from overflow, why would one not simply use BigInt all the time? The answer is performance, it is significantly slower.","category":"section"},{"location":"week-2-lecture-2/#Performance","page":"Week 2 Lecture 2: Integer arithmetic","title":"Performance","text":"Let us take a brief look how performance for various integer types compare. Let us consider the problem of computing\n\nsum_n = 1^N n^b\n\nfor some integers N and b. In Julia this could be implemented as\n\nfunction f(N::Integer, b::Integer)\n    # Writing just \"0\" would give us an Int64, zero(N) gives us a zero of the same type as N\n    S = zero(N)\n    # Same with one(N) here\n    for n in one(N):N\n        S += n^b\n    end\n    return S\nend\n\nLet us take b = 2, we can evaluate the function using a variety of types\n\nf(1000, 2)\nf(Int32(1000), Int32(2))\nf(Int16(1000), Int16(2)) # This overflows!\nf(BigInt(1000), BigInt(2))\n\nTo benchmark these different versions we use the Julia package BenchmarkTools.jl.\n\nusing BenchmarkTools\n@benchmark f($1000, $2) samples = 10000 evals = 10\n@benchmark f(Int32($1000), Int32($2)) samples = 10000 evals = 10\n@benchmark f(Int16($1000), Int16($2)) samples = 10000 evals = 10\n@benchmark f(BigInt($1000), BigInt($2)) samples = 5000 evals = 1\n\nnote: Note\nThe $-signs in the code below are part of the BenchmarkTools interface and are there to avoid the compiler being too clever and optimizing away what we want to measure.The extra arguments samples and evals are not required. If you run the code yourself you can remove them. They are here to reduce the time to build this documentation.\n\nThe most important number in the above benchmarks is the minimum time. The mean time is also important in practice, but is affected by variables we are not controlling for here.","category":"section"},{"location":"week-2-lecture-2/#Flint","page":"Week 2 Lecture 2: Integer arithmetic","title":"Flint","text":"While we are discussing integer arithmetic it is also natural to introduce the library that will be the foundation for a lot of the rigorous numerics we will get to later in the course. The FLINT library is a C library with high performance implementations of many computer algebra algorithms. FLINT stands for \"Fast Library for Number Theory\", but much of the functionality is useful outside of number theory as well.\n\nWe will not make use of Flint directly, instead we will use it through the Julia package Arblib.jl that wraps (most of) the parts of the library related to rigorous numerics. In fact, there is not really any need for you to know about the Flint library at all for what we will do in the course.\n\nnote: Note\nThe reason for the library being called Flint but the Julia package being called Arblib is that the parts of Flint that Arblib wraps were previously a separate library called Arb (Arbitrary precision Real Balls). The Arb library, and several others, were merged into Flint in 2023.\n\nThe Flint library implements many standard computer algebra algorithms over a large variety of different rings. Some of the rings it implements are\n\nIntegers\nRational numbers\nIntegers mod n\nReal and complex numbers (this is the rigorous numerics part)\nExact real and complex numbers (this is more symbolical in nature)\nFinite fields\np-adic numbers\n\nFor our purposes we will primarily deal with the real and complex numbers, though these internally depend on the integers and rational numbers. For these rings it implements a number of different algorithms related to e.g. polynomials, matrices and special functions. Unless specified otherwise, the Flint library can be assumed to always return mathematically rigorous results.\n\nIn many areas the Flint implementations are the state of the art and sometimes greatly outperform other implementations. It's used by many programs for computationally heavy computations, for example Sage uses it internally for many things.","category":"section"},{"location":"week-10-lecture-2/#Week-10-Lecture-2:-TODO","page":"Week 10 Lecture 2: TODO","title":"Week 10 Lecture 2: TODO","text":"","category":"section"},{"location":"week-11-lab/#Week-11-Lab:-TODO","page":"Week 11 Lab: TODO","title":"Week 11 Lab: TODO","text":"","category":"section"},{"location":"week-6-lab/#Week-6-Lab:-TODO","page":"Week 6 Lab: TODO","title":"Week 6 Lab: TODO","text":"","category":"section"},{"location":"week-5-lecture-2/#Week-5-Lecture-2:-TODO","page":"Week 5 Lecture 2: TODO","title":"Week 5 Lecture 2: TODO","text":"","category":"section"},{"location":"week-10-lab/#Week-10-Lab:-TODO","page":"Week 10 Lab: TODO","title":"Week 10 Lab: TODO","text":"","category":"section"},{"location":"week-4-lecture-1/#Week-4-Lecture-1:-TODO","page":"Week 4 Lecture 1: TODO","title":"Week 4 Lecture 1: TODO","text":"","category":"section"},{"location":"week-15-lab/#Week-15-Lab:-TODO","page":"Week 15 Lab: TODO","title":"Week 15 Lab: TODO","text":"","category":"section"},{"location":"week-15-lecture-2/#Week-15-Lecture-2:-TODO","page":"Week 15 Lecture 2: TODO","title":"Week 15 Lecture 2: TODO","text":"","category":"section"},{"location":"week-5-lecture-1/#Week-5-Lecture-1:-TODO","page":"Week 5 Lecture 1: TODO","title":"Week 5 Lecture 1: TODO","text":"","category":"section"},{"location":"week-1-lecture-1/#Week-1-Lecture-1-Introduction-to-computer-assisted-proofs","page":"Week 1 Lecture 1 - Introduction to computer-assisted proofs","title":"Week 1 Lecture 1 - Introduction to computer-assisted proofs","text":"The main goal of this first lecture is to give you an idea of what to expect about the course. We will take a look at the structure of the course and an overview of the content we will cover.\n\nThe course consists of 15 weeks which we will split into 3 different parts. The 3 parts are\n\nIntroduction to computer-assisted proofs (≈ Week 1-4)\nIntroduction to rigorous numerics (≈ Week 5-10)\nComputer-assisted proofs in practice (≈ Week 11-15)\n\nEach week will follow roughly the same pattern. The Monday and Wednesday sessions will be lectures where I present material, the Friday session will be a computer lab where you get to try out what we have discussed during the lectures.","category":"section"},{"location":"week-1-lecture-1/#Computer-labs","page":"Week 1 Lecture 1 - Introduction to computer-assisted proofs","title":"Computer labs","text":"For the computer labs we will make use of Julia. We will talk more about Julia during the first computer lab on Friday. A very brief description is that the Julia language is designed for high performance scientific computing. Version 1.0 was released in 2018, so it is a relatively young language compared to for example C, Fortran, Python and Matlab.\n\nThe main goal for the Friday session is to make sure that you are all able to install Julia and get it up and running. In general the course will not assume familiarity with Julia, it should hopefully be possible for you to pick it up as we go.\n\nThis web page you are reading now is generated using Julia. In fact, the repository associated with this course is structured as a Julia package. We will talk more about this later on.","category":"section"},{"location":"week-1-lecture-1/#Part-1:-Introduction-to-computer-assisted-proofs","page":"Week 1 Lecture 1 - Introduction to computer-assisted proofs","title":"Part 1: Introduction to computer-assisted proofs","text":"What is a computer-assisted proof? This is not an obvious question and the answer will depend on who you ask. For the purposes of this course there will be an answer to this question, but getting there requires a bit of background.\n\nLet us start by considering a slightly different question, what is computer-assisted mathematics? In this case it could be more or less anything that involves a computer. Examples could include\n\nNumerical simulations, such as solving some PDE using numerical methods or computing images of the Cantor set.\nAnalysing data, in particular in applied mathematics you might have actual datasets you want to analyse. But you could also analyse large databases of knots.\nSymbolical computations, for example computing integrals or handling very large expressions using e.g. Sage, WolframAlpha, Mathematica or Maple.\nUsing LLMs or other similar tools for solving mathematical problems or writing manuscripts.\n\nMore or less anything where you make use of the computer to help you in your mathematical research. One could even include things like:\n\nUsing Arxiv to find articles.\nWriting your article in LaTeX.\nCommunicating with your collaborators using email.\n\nWe are however interested in something slightly different, not computer-assisted mathematics but computer-assisted proofs. The name implies that it should involve proofs, but that is true for most of mathematics so doesn't restrict us much. We will however mean something more specific, for our purposes a computer-assisted proof is a mathematical proof of some mathematical statement which requires a computer to verify. The key here is that the computer is involved in the verification, not only the construction of the proof. Some things which are not computer-assisted proofs with this definition are:\n\nUsing an LLM to generate a human readable mathematical proof.\nUsing numerical simulations to generate an hypothesis which is then proved using pen and paper.\nUsing symbolical computations to compute a nasty integral, where the answer can be verified by hand.\n\nWhat would be an example of a computer-assisted proof then? What does it mean to use a computer to verify a proof? The prototypical example would be something that requires a lot of calculations. Showing that the fifth decimal in pi is 9 you could do by hand, showing that the millionth decimal is 1 you probably couldn't. There is, however, nothing fundamentally different between computing the fifth decimal or the millionth decimal. In theory you could compute the millionth decimal by hand, it would just take you a veeery long time. For the computer it takes less than a second.\n\nFor the first part of the course we will look at three different kinds of computer-assisted proofs:\n\nComputer-assisted proofs for discrete problems\nComputer-assisted proofs for continuous problems\nFormal proofs\n\nFor discrete problems it is relatively easy to imagine that computers could be helpful. An example is problems which reduce to checking a finite number of cases. The most famous problem in this setting is probably the four color theorem, which says that any map can be colored using four colors in such a way that no two adjacent regions have the same color. This was proved in 1976 using a computer-assisted proof. For this they reduced the problem, using pen and paper, to checking 1834 possible counterexamples. These 1834 possible counterexamples were then checked to be four colorable with the help of the computer.\n\nFor continuous problems it is not as obvious how computers could be used. Numerical analysis is a field of mathematics which deals with computing approximate solutions to continuous problems. When numerically solving a problem you introduce discretization errors and rounding errors and these make it so that the final result cannot be fully trusted. A good numerical method will give you a good approximation most of the time, but it is in general not proved to always do so. For mathematical proofs these errors are problematic, it is not enough for the result to be approximately correct. How to deal with this is what most of this course will be about. We look at a subfield of numerical analysis called rigorous numerical analysis, which allows us to control these errors.\n\nFinally, we will talk about formal proofs. Wikipedia gives the following description of formal proofs\n\nIn logic and mathematics, a formal proof or derivation is a finite sequence of sentences (known as well-formed formulas when relating to formal language), each of which is an axiom, an assumption, or follows from the preceding sentences in the sequence, according to the rule of inference. It differs from a natural language argument in that it is rigorous, unambiguous and mechanically verifiable.\n\nFormal proofs are not necessarily computer-assisted, though for anything non-trivial the size of the expressions quickly outgrow anything a human could verify and in practice formal proofs hence require computers for the verification. For writing formal proofs one makes use of special purpose software called proof assistants or interactive theorem provers, example of such softwares are:\n\nIsabelle - Cambridge 1986\nRocq (previously named Coq) - Inria 1989\nAgda - Chalmers 2007 (1999)\nIdris - Edwin Brady 2007\nLean - Microsoft Research 2013\n\nIn particular the last one, Lean, has gained a lot of moment in the last couple of years.\n\nUsing the term computer-assisted for a formal proof is however maybe slightly misleading. In this case the computer is not merely assisting in verifying the proof, it is doing the entire verification completely by itself. We will talk a little bit about formal proofs later in the course, but only with the goal of understanding the difference between a formal proof and a regular computer-assisted proof.\n\nnote: Note\nThe definition of a computer-assisted proof that we use in here, that the computer is used in the verification of the proof, not only the construction, is not universally used by all mathematicians. For example Terence Tao has a talk on what he calls Machine-Assisted Proofs, where he includes both what we in this course call computer-assisted proofs but also for example using LLMs for generating proofs. He also uses the terminology machine-assisted rather than computer-assisted, the reason being that the word computer originates from the name for human computers doing computations.","category":"section"},{"location":"week-1-lecture-1/#Part-2:-Introduction-to-rigorous-numerics","page":"Week 1 Lecture 1 - Introduction to computer-assisted proofs","title":"Part 2: Introduction to rigorous numerics","text":"The second part of the course is where we will actually start learning how to build computer-assisted proofs. We will look at the field of rigorous numerical analysis, which is a subfield of numerical analysis. The goal of numerical analysis is to compute approximations, this is also true for rigorous numerical analysis. The difference with rigorous numerical analysis is that in addition to computing an approximation you also compute rigorous upper bounds for the error of your approximation.\n\nAs a simple example, consider the problem of computing e^2. In classical analysis you would do\n\nexp(2)\n\nWhich tells you that e^2 approx 738905609893065 In rigorous numerics you would also compute an approximation, but you would include an upper bound on the error of your approximation.\n\nusing Arblib\nexp(Arb(2, prec = 53))\n\nWhich again tells you that e^2 approx 738905609893065, but it now includes the extra information that the error of this approximation is at most 139 cdot 10^-15, so that e^2 in 738905609893065 pm 139 cdot 10^-15. How is this upper bound for the error computed? That is what we will talk about! It is both fairly technical and surprisingly easy.\n\nTo achieve this we will make use of something called interval arithmetic. In regular numerical analysis one works with floating points, these are inherently approximations since they cannot represent most real numbers exactly. In interval arithmetic one works with pairs of floating points, one representing a lower bound and one representing an upper bound. So instead of e.g. pi approx 3141592653589793 we would have pi in 3141592653589793 31415926535897936. Another format is to use one floating point representing the midpoint and another the radius, so pi in 314159265358979 pm 334 cdot 10^-15. When doing this it is not a problem that most real numbers cannot be represented by floating points, we can always pick the bounds to be floating points. Once we have an interval representation we will still need to do computations with them, exactly how to do this in a rigorous way is probably the most technical part of rigorous numerics.\n\nThe content we will cover over the six weeks are:\n\nMathematical foundations of floating point arithmetic: floating point formats, rounding\nBasics of interval arithmetic: basic arithmetic, elementary functions, special functions\nBasic rigorous numerics: isolating roots, computing integrals, enclosing extrema\nAutomatic differentiation: forward (and backwards) differentiation, Taylor arithmetic\nImproved rigorous numerics: isolating roots, computing integrals, enclosing extrema\n\nA sneak peak at some of the things we will learn how to do, in this case using the Arblib.jl package for interval arithmetic package and some algorithms implemented in ArbExtras.jl.\n\nEnclose roots of functions. For example finding the unique zero of x + e^x on the interval -06 -05\n\nusing Arblib, ArbExtras\nArbExtras.refine_root(x -> x + exp(x), Arb((-3 // 5, -1 // 2)))\n\nEnclose integrals of analytic functions. For example enclosing int_0^5 sin(e^x) dx\n\nusing Arblib\nArblib.integrate(x -> sin(exp(x)), 0, 5)\n\nEnclose the minimum of the function. For example enclosing the minimum of the Bessel function J_4(x) on the interval 1 2.\n\nusing Arblib, ArbExtras, SpecialFunctions\nArbExtras.minimum_enclosure(x -> besselj(Arb(4), x), Arf(1), Arf(2))","category":"section"},{"location":"week-1-lecture-1/#Part-3:-Computer-assisted-proofs-in-practice","page":"Week 1 Lecture 1 - Introduction to computer-assisted proofs","title":"Part 3: Computer-assisted proofs in practice","text":"In the last part of the course we will look at how computer-assisted proofs are actually used in the literature. The goal will be to look at examples of papers making use of computer-assisted proofs. Exactly how we do this and what we will look is however yet to be determined and will depend on your interests. Some areas we could take a closer look at are:\n\nPDEs: This is the field most of my research takes place in.\nDynamical systems: This is probably the field with the longest history of computer-assisted proofs and there is a number of interesting things we could look at here.\n\nOne could also discuss things on a more meta level:\n\nWhat exactly does it take to publish a paper with a computer-assisted proof? How does one prepare the code? How does one publish the code? How does one connect the paper and the code?\nWhat type of problems are amendable to a computer-assisted approach? These are things we will touch upon during the course, but one could maybe gain something from discussing it in more detail.\n\nOne could also dive deeper into different algorithms for computer-assisted proofs:\n\nRigorous integration of ODEs\nFinite element methods\nSpectral methods\nPhysics-Informed Neural Networks (PINNs)\n\nAlternatively, one can study the lower level details of interval arithmetic, more related to the field of computer algebra.\n\nWe don't have to decide what to do yet, but as we get further into the course we'll come back to this.","category":"section"},{"location":"week-12-lab/#Week-12-Lab:-TODO","page":"Week 12 Lab: TODO","title":"Week 12 Lab: TODO","text":"","category":"section"},{"location":"week-7-lecture-1/#Week-7-Lecture-1:-TODO","page":"Week 7 Lecture 1: TODO","title":"Week 7 Lecture 1: TODO","text":"","category":"section"},{"location":"week-11-lecture-1/#Week-11-Lecture-1:-TODO","page":"Week 11 Lecture 1: TODO","title":"Week 11 Lecture 1: TODO","text":"","category":"section"},{"location":"week-2-lecture-1/#Week-2-Lecture-1:-Discrete-problems","page":"Week 2 Lecture 1: Discrete problems","title":"Week 2 Lecture 1: Discrete problems","text":"In this lecture we'll take a look at some high profile computer-assisted proofs for discrete problems. The goal here is not to fully understand these proofs, but rather to see examples of problems for which computer-assistance could be beneficial.\n\nWe will look at three different problems:\n\nThe four color theorem\nThe boolean Pythagorean triples problem\nGoldbach's weak conjecture\n\nSome takeaways from these examples are:\n\nInfinite problems can sometimes be reduced to finite problems using a pen and paper analysis. In some cases combined with a computer-assisted part.\nModern computers are very fast and can handle a huge number of computations.\nFor some problems the number of computations scale extremely fast.\nComputers are getting faster and faster, what was a monumental effort in the 70's is trivial today.","category":"section"},{"location":"week-2-lecture-1/#The-four-color-theorem","page":"Week 2 Lecture 1: Discrete problems","title":"The four color theorem","text":"This problem was mentioned already in the first lecture and is one of the first, and likely the most famous, computer-assisted proofs.\n\nnote: Four color theorem (1976)\nAny map can be colored using four colors in such a way that no two adjacent nodes have the same color.\n\n(Image: Four colored map of US states)\n\nInitially this doesn't look like a good problem for a computer-assisted proofs. It is straightforward to check if a specific map is four colorable. One simply writes a program that searches for four colorings. There are, however, an infinite number of maps and there is no way to check all of them with the computer.\n\nThe first, and probably most important, part of the proof is therefore to reduce it to a finite number of cases. We will not go into details on how this is done, but the general idea is to start by assuming that there is a minimal counterexample. They then make use of two related concepts:\n\nShow there exists a finite unavoidable set. A set of configurations such that every map that satisfies some necessary conditions for being a minimal counterexample must have at least one configuration from this set.\nReducible configurations, a configuration that cannot occur in a minimal counterexample. If a map contains a reducible configuration then the map can be reduced to a smaller map which, if it is four colorable, implies that the original map is four colorable. This in particular implies that the original map could not be a minimal counterexample.\n\nThe result then follows if one could show that all of the configurations in the finite unavoidable set are reducible configurations. In the original proof the unavoidable set consisted of 1834 configurations and was later reduced to 1,482. Each of these configurations then had to be checked to be reducible. Just finding the set of unavoidable configurations required a significant amount of work, but to my understanding was in large part done by hand. Checking that each of the unavoidable configurations were reducible was however done by the computer. Since this was still in the very early days of computers there was also a lot of manual labor involved in the process at this point.\n\nIn 2005 the proof was formalized in Rocq (previously called Coq), which we will talk more about in Week 4.","category":"section"},{"location":"week-2-lecture-1/#The-boolean-Pythagorean-triple-problem","page":"Week 2 Lecture 1: Discrete problems","title":"The boolean Pythagorean triple problem","text":"The Pythagorean triple problem asks whether it is possible to color each of the positive integers either red or blue, so that no Pythagorean triple of integers (a b c) satisfying a^2 + b^2 = c^2 are all the same color? This was shown to be false in 2016, see also this website with some more information. More precisely we have the following theorem.\n\nnote: Boolean Pythagorean triples theorem (2016)\nThe set 1 dots 7824 can be partitioned into two parts, such that no part contains a Pythagorean triple, while this is impossible for 1 dots 7825.\n\nThis problem is inherently finite, and it is maybe easier to imagine that it could be done through a computer-assisted proof.\n\nLet us start with the first part, showing that 1 dots 7824 can be partitioned into two parts such that no part contains a Pythagorean triple. If we are given a partitioning then it is a relatively straightforward exercise to verify that it satisfies the condition. There is around 10000 Pythagorean triples below 7824, so a very dedicated person could even do it by hand given enough time. In practice this check is computer-assisted. Of course, one first has to find a candidate partitioning, this uses a tool known as a SAT solver. SAT solvers are very useful tools for computer-assisted proofs, but tend to not play big role in analysis problems so we won't talk about it in this course.\n\nThe second part, showing that this is impossible for 1 dots 7825, is again a finite problem. In this case it is however not enough to find one partitioning, instead we have to verify that it is impossible for any partitioning. There are, however, 2^7825 approx 363 times 10^2355 different ways to partition this set and checking that all of these partitions contain a Pythagorean triple is simply not feasible.\n\nnote: Note\nIn cryptography one usually assumes that 2^128 approx 34 times 10^38 is larger than what anyone could bruteforce. Using all the energy in the observable universe one could get as far as around 2^320.\n\nHowever, the problem has a lot of symmetry and they managed to reduce the problem to around a trillion cases. The proof that none of these trillion partitions contain a Pythagorean triple consists of around 200 terabytes of propositional logic. This made it the largest proof ever. This does however compress to a mere 68 gigabytes in the end. Again, the actual calculation is done using a SAT solver.","category":"section"},{"location":"week-2-lecture-1/#Goldbach's-weak-conjecture","page":"Week 2 Lecture 1: Discrete problems","title":"Goldbach's weak conjecture","text":"Goldbach's weak conjecture is a famous conjecture in number theory.\n\nnote: Goldbach's weak conjecture\nEvery odd number greater than 5 can be written as the sum of three primes.\n\nSimilar to the Four color theorem this is again a problem that a priori requires checking an infinite number of cases. In this case it was however reduced to a finite computation already in 1956, though with an upper bound e^e^16038 approx 8 times 10^4008659, way too large to make a bruteforce approach of the finite number of remaining numbers feasible.\n\nIn 2013 the conjecture was computationally confirmed up to 8 875 694 145 621 773 516 800 000 000 000 approx 8875 cdot 10^30, see this paper. There is also a number of earlier results not going quite as far. The computation makes use of a number of tricks to reduce the computational time, but eventually boils down to a large brute force check requiring about 40 000 core hours.\n\nSimultaneously there was progress on improving the bound after which the conjecture could be proved to hold for all odd numbers. This number was eventually brought down to 10^27 by Harald Helfgott in 2013. Together with the computation above this gave a full proof of the result. The paper was accepted for publication in Annals in 2015, though it seems like the final version has not actually been finished. Reducing the bound to 10^27 does by itself rely on a computer-assisted proof. It uses tools from analysis and in this case it therefore makes use of rigorous numerics for the computations. The theory is however fairly involved and not something we will dive deeper in.","category":"section"},{"location":"week-4-lab/#Week-4-Lab:-TODO","page":"Week 4 Lab: TODO","title":"Week 4 Lab: TODO","text":"","category":"section"},{"location":"week-13-lab/#Week-13-Lab:-TODO","page":"Week 13 Lab: TODO","title":"Week 13 Lab: TODO","text":"","category":"section"},{"location":"week-13-lecture-1/#Week-13-Lecture-1:-TODO","page":"Week 13 Lecture 1: TODO","title":"Week 13 Lecture 1: TODO","text":"","category":"section"},{"location":"week-3-lecture-2/#Week-3-Lecture-2:-Computer-assisted-proofs-for-continuous-problems","page":"Week 3 Lecture 2: Computer-assisted proofs for continuous problems","title":"Week 3 Lecture 2: Computer-assisted proofs for continuous problems","text":"In the previous lecture, we examined an example of a computer-assisted proof. The goal was to prove that the first Dirichlet eigenvalue of regular polygons, mathbbP_N, is decreasing with respect to the number of vertices, N. We focused on proving this for a finite number of polygons, specifically N = 3 4 dots 64.\n\nIn this lecture, we will look at another example of a computer-assisted proof, this time focusing on waves. Before proceeding, let us briefly review the example from the previous lecture and discuss the main takeaways.","category":"section"},{"location":"week-3-lecture-2/#Looking-back-at-regular-polygons","page":"Week 3 Lecture 2: Computer-assisted proofs for continuous problems","title":"Looking back at regular polygons","text":"We were interested in demonstrating that the first eigenvalue of regular polygons is decreasing for N = 3 4 dots 64. The approach was split into two parts:\n\nCompute approximate eigenvalues (and eigenfunctions) for each  polygon using the Method of Particular Solutions.\nCompute error bounds on the approximate eigenvalues using a  theorem by Fox, Henrici, and Moler.\n\nFrom the approximations and the error bounds, it is relatively straightforward to verify that the eigenvalue is decreasing. Step 1 utilizes classical numerics, whereas Step 2 introduces rigorous numerics. To obtain a bound for the error, we required an upper bound for:\n\nmu = fracsqrtOmegasup_x in partial Omegau_app(x)u_app_2\n\nThis, in turn, requires an upper bound for sup_x in partial Omegau_app(x) and a lower bound for u_app_2. We have therefore reduced the problem of computing error bounds for our approximation to computing bounds for specific properties of that approximation—in this case, the error on the boundary and the L^2 norm.\n\nMany computer-assisted proofs follow a path similar to the one above:\n\nCompute a numerical approximation using classical numerical  methods.\nUsing pen and paper, prove that there exists a solution within a  certain distance of the approximation, contingent on bounding  certain properties of the approximation.\nCompute bounds for these properties using rigorous numerical  methods.\nDepending on the problem, use the computed bounds to verify the  result (e.g., that eigenvalues are decreasing).\n\nThe difficulty of these parts depends highly on the specific problem. The first two steps can range from trivial to extremely difficult. After all, almost the entire field of numerical analysis is devoted to Step 1 and, to some extent, Step 2. The main part of this course will focus on Step 3: how to actually bound the necessary properties.\n\nThe goal of this week is to provide examples of how an interesting mathematical problem can be reduced to a rigorous numerics problem. We are starting from the \"top\"—the mathematical problem—and moving downwards to identify what must be bounded. In the second half of the course, Introduction to rigorous numerics, we will invert this approach. We will start from the \"bottom\"—floating-point arithmetic—and slowly work our way up to using it to bound quantities like sup_x in partial Omegau_app(x).","category":"section"},{"location":"week-3-lecture-2/#Highest-cusped-waves","page":"Week 3 Lecture 2: Computer-assisted proofs for continuous problems","title":"Highest cusped waves","text":"Let us now examine another computer-assisted proof, this time related to waves. This material is based on two papers:\n\nHighest Cusped Waves for the Burgers-Hilbert   Equation\nHighest cusped waves for the fractional KdV   equations\n\nThese papers largely follow the same approach but address different parameter values. We will primarily focus on the second paper.\n\nWe are interested in the fractional Korteweg-de Vries (KdV) equations in the periodic setting, given by:\n\nf_t + f f_x = D^alphaf_xquad text for  (x t) in mathbbT times mathbbR\n\nHere, D^alpha is the Fourier multiplier operator defined by:\n\nwidehatD^alphaf(xi) = xi^alphawidehatf(xi)\n\nwhere the parameter alpha can generally take any real value. For alpha = 2 and alpha = 1, the equation reduces to the classical KdV and Benjamin-Ono equations, respectively. For alpha = -2, one obtains the reduced Ostrovsky equation. For alpha = -1, it reduces to the Burgers-Hilbert equation. The first paper cited above treats the case alpha = -1, while the second treats alpha in (-1 0).\n\nWe are interested in traveling waves—solutions of the form f(x t) = varphi(x - ct), where c  0 denotes the wave speed. In this case, the equation reduces to:\n\n-c varphi + varphivarphi = D^alpha varphi\n\nThis equation possesses a branch of even, 2pi-periodic, smooth traveling wave solutions bifurcating from constant solutions. By numerically following this branch, we obtain a sequence of traveling waves. The result resembles the following figure, sourced from this paper.\n\n(Image: Traveling waves along bifurcation branch)\n\nNumerically, we observe that the waves approach a profile with a cusp at the peak. The existence of a wave with such a cusp is what we aim to prove. Below is a plot showing what these waves look like for varying values of alpha.\n\n(Image: Highest cusped traveling waves)\n\nOur proof of their existence is computer-assisted and is the subject of this section.","category":"section"},{"location":"week-3-lecture-2/#Proof-of-existence","page":"Week 3 Lecture 2: Computer-assisted proofs for continuous problems","title":"Proof of existence","text":"The procedure follows the same three steps as the earlier example:\n\nCompute a numerical approximation using classical numerical  methods.\nUsing pen and paper, prove that there exists a solution within a  certain distance of the approximation, assuming we can bound  certain properties of the approximation.\nCompute bounds for these properties using rigorous numerical  methods.\n\nBefore proceeding, we must massage the equation to make it more tractable. If we let u(x) = c - varphi(x), we can write the equation as:\n\nfrac12u^2 = -mathcalH^alphau\n\nwhere mathcalH is the operator:\n\nmathcalH^alphau(x) = D^alphau(x) - D^alphau(0)","category":"section"},{"location":"week-3-lecture-2/#Step-1:-Finding-a-numerical-approximation","page":"Week 3 Lecture 2: Computer-assisted proofs for continuous problems","title":"Step 1: Finding a numerical approximation","text":"Obtaining a high-quality numerical approximation requires significant effort; in particular, the asymptotic behavior near x = 0 requires careful analysis. However, the details are not critical for our current goal. Ultimately, we obtain an approximation of the form:\n\nu_alpha(x) = a_alpha0tildeC_1 - alpha(x)\n  + sum_j = 1^N_alpha0 a_alphajtildeC_1 - alpha + jp_alpha(x)\n  + sum_n = 1^N_alpha1 b_alphan(cos(nx) - 1)\n\nHere, the function tildeC_s is a variant of the Clausen functions. The coefficients a_alphaj and b_alphan are carefully chosen to ensure a good numerical approximation.","category":"section"},{"location":"week-3-lecture-2/#Step-2:-Error-bounds-for-approximation","page":"Week 3 Lecture 2: Computer-assisted proofs for continuous problems","title":"Step 2: Error bounds for approximation","text":"The next step is to determine how to proceed from our numerical approximate solution to the existence of a true solution. Like many computer-assisted proofs, this relies on a fixed-point argument.\n\nFirst, we write our solution u as the approximation u_alpha plus a weighted perturbation. For our purposes, we can take the weight to be x, though in practice this must be adjusted based on alpha. Thus, we write u as:\n\nu(x) = u_alpha(x) + xv(x)\n\nwhere u_alpha is our numerical approximation and v is a perturbation. Our goal is to prove that there exists a v such that this expression satisfies the equation. Inserting this ansatz into the equation and solving for v yields:\n\nv + frac1xu_alphamathcalH^alphaxv =\n-frac1xu_alphaleft(\n  mathcalH^alphau_alpha + frac12u_alpha^2\nright) - fracx2u_alphav^2\n\nBy introducing the operators and functions:\n\nT_alphav = -frac1xu_alphamathcalH^alphaxvquad\nF_alpha(x) = frac1xu_alpha(x)left(mathcalH^alphau_alpha(x) + frac12u_alpha(x)^2right)quad\nN_alpha(x) = fracx2u_alpha(x)\n\nthe equation can be rewritten as:\n\n(I - T_alpha)v = -F_alpha - N_alphav^2\n\nAssuming that I - T_alpha is invertible, we have:\n\nv = (I - T_alpha)^-1left(-F_alpha - N_alphav^2right) = G_alphav\n\nProving the existence of a solution thus reduces to proving the existence of a fixed point for the operator G_alpha.\n\nFinally, one can show that if we let:\n\nn_alpha = N_alpha_L^infty(mathbbT)quad\ndelta_alpha = F_alpha_L^infty(mathbbT)quad\nD_alpha = T_alpha\n\nthen G_alpha has a fixed point provided D_alpha  1 (ensuring I - T_alpha is invertible) and:\n\ndelta_alpha  frac(1 - D_alpha)^24n_alpha\n\nNote that delta_alpha, D_alpha, and n_alpha depend only on our approximation u_alpha. We have thus completed Step 2: we have reduced the problem of proving existence to bounding specific properties of our numerical approximation.","category":"section"},{"location":"week-3-lecture-2/#Step-3:-Bounding-\\delta_{\\alpha},-D_{\\alpha}-and-n_{\\alpha}","page":"Week 3 Lecture 2: Computer-assisted proofs for continuous problems","title":"Step 3: Bounding delta_alpha, D_alpha and n_alpha","text":"The values delta_alpha, D_alpha, and n_alpha are all given by the supremum of a function on the interval 0 pi. More precisely:\n\nn_alpha = sup_x in 0 pi N_alpha(x)quad\ndelta_alpha = sup_x in 0 pi F_alpha(x)quad\nD_alpha = sup_x in 0 pi mathcalT_alpha(x)\n\nwith N_alpha and F_alpha as defined above, and:\n\nmathcalT_alpha(x) = frac1pi xu_alpha(x)\nint_0^piI_alpha(x y)y dy\n\nComputing bounds therefore reduces to bounding these three functions on the interval 0 pi. How to achieve this is what we will cover later in the course.\n\nnote: Note\nIn this specific case, when the weight is simply x, computing the supremum away from x = 0 is somewhat straightforward. However, near x = 0, more work is required due to the existence of removable singularities.When the weight is not just x, significantly more work is required to handle mathcalT_alpha. Near the endpoints alpha = -1 and alpha = 0, the approach also requires adjustments to succeed.","category":"section"},{"location":"week-5-lab/#Week-5-Lab:-TODO","page":"Week 5 Lab: TODO","title":"Week 5 Lab: TODO","text":"","category":"section"},{"location":"week-8-lab/#Week-8-Lab:-TODO","page":"Week 8 Lab: TODO","title":"Week 8 Lab: TODO","text":"","category":"section"},{"location":"week-7-lecture-2/#Week-7-Lecture-2:-TODO","page":"Week 7 Lecture 2: TODO","title":"Week 7 Lecture 2: TODO","text":"","category":"section"}]
}
