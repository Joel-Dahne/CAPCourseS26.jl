var documenterSearchIndex = {"docs":
[{"location":"week-9-lab/#Week-9-Lab:-TODO","page":"Week 9 Lab: TODO","title":"Week 9 Lab: TODO","text":"","category":"section"},{"location":"week-11-lecture-2/#Week-11-Lecture-2:-TODO","page":"Week 11 Lecture 2: TODO","title":"Week 11 Lecture 2: TODO","text":"","category":"section"},{"location":"week-9-lecture-1/#Week-9-Lecture-1:-TODO","page":"Week 9 Lecture 1: TODO","title":"Week 9 Lecture 1: TODO","text":"","category":"section"},{"location":"week-4-lecture-2/#Week-4-Lecture-2:-Formal-proofs","page":"Week 4 Lecture 2: Formal proofs","title":"Week 4 Lecture 2: Formal proofs","text":"In this lecture we continue our discussion of formal proofs, this time focusing on the foundations on which formal proofs are built. Let us however start by a more informal discussion about formal proofs.","category":"section"},{"location":"week-4-lecture-2/#What-is-the-point-of-formal-proofs?","page":"Week 4 Lecture 2: Formal proofs","title":"What is the point of formal proofs?","text":"For computer-assisted proofs the goal is fairly clear, we want to be able to prove things that require computations that would not be feasible to perform by hand. For formal proofs the goals are perhaps less clear and to a large extent they depend on your point of view.\n\nCorrectness: This is in some sense the primary goal of formal proofs. Many published proofs contain errors, in most cases these errors are likely trivially fixable but not in all cases. Formal proofs could reduce (or to some extent even eliminate) such errors. On a large scale the area of mathematics does however seem to be relatively stable to such errors, see for example the discussions in this Mathoverflow post.\nMutating proofs: Writing a formal proof takes a lot longer than writing a pen and paper proof. Once a formal proof has been written it can however be much more straightforward to make small adjustments to the statements and the proofs. For example for fine-tuning constants. Lean can then tell you exactly where your previous arguments fail and where updates are needed. See for example this Mathstodon post.\nAutomating routine parts: Some proofs require large routine calculations. Even if these calculations are simple, the sheer size could make them difficult to both write and referee. Having Lean automatically generate proofs for such parts could then save a lot of time. The Lean tools for automatically generating proofs are steadily improving, so even if some things are out of reach today they might not stay that way.\nGenerative AI: Combining formal proofs with generative AI has the potential of letting mathematicians write informal proofs which can then be automatically formalized by the AI. This is possible to some extent today and the situation will likely improve. Formal proofs also seem to be a good ground for training AI using reinforcement learning.\nLarge scale collaboration: Formal proofs reduces the need to trust proofs from coauthors. This could allow for more large scale collaboration that we are currently used to in mathematics.","category":"section"},{"location":"week-4-lecture-2/#What-is-a-formal-proof?","page":"Week 4 Lecture 2: Formal proofs","title":"What is a formal proof?","text":"We have now seen some examples of what formal proofs could look like in practice. But how are these proofs actually checked? What foundations are these built on?\n\nTo understand this we will take a brief look at three core concepts: the correspondence between programs and proofs, the language used to express them, and the architecture that allows us to trust the software.","category":"section"},{"location":"week-4-lecture-2/#The-Curry-Howard-Correspondence","page":"Week 4 Lecture 2: Formal proofs","title":"The Curry-Howard Correspondence","text":"The most fundamental concept to grasp is that in formal verification, there is a deep structural link between logic and programming. This is known as the Curry-Howard correspondence (or \"Propositions as Types\").\n\nIn a standard programming language, you have types (like int, string, List) and values or programs that inhabit those types (like 5, \"hello\", [1, 2]).\n\nIn the world of formal proofs:\n\nA mathematical proposition (statement) is a Type.\nA proof of that proposition is a Program (value) of that Type.\n\nTherefore, \"checking a proof\" is reduced to \"type-checking a program.\" If you write a function that claims to return an integer but actually returns a string, the compiler throws a type error. Similarly, if you write a proof that claims to prove Theorem X but contains a logical gap, the formal system throws a type error.\n\nYou can see this correspondence directly in Lean code (at least if you have some experience with functional programming languages). Take for example\n\n/--\n  Axiom 2.4 (Different natural numbers have different successors).\n  Compare with Mathlib's `Nat.succ_inj`.\n-/\ntheorem Nat.succ_cancel {n m:Nat} (hnm: n++ = m++) : n = m := by\n  injection hnm\n\nHere Nat.succ_cancel is a function that takes as input n and m, both of type Nat as well as hnm which is of type n++ = m++ and returns a value of type n = m. In most programming languages treating n = m as a type doesn't make much sense, but in Lean it does. What comes after the := part is what in a normal programming language would correspond to the implementation of the function.\n\nA concrete example of this correspondence is implications. The mathematical statement that P implies Q corresponds to a function that takes as input a value of type P and returns an output of type Q. The proof of the statement corresponds to the implementation of the function.","category":"section"},{"location":"week-4-lecture-2/#Dependent-Type-Theory","page":"Week 4 Lecture 2: Formal proofs","title":"Dependent Type Theory","text":"To make the Curry-Howard correspondence work for complex mathematics one needs a type system that is much richer than what is found in standard languages like C or Julia or even Haskell. There are different approaches for this and different proof assistants use different systems. The system Lean is based on is called Dependent Type Theory.\n\nMost standard programming languages have types like Bool, Int and Float64. Many programming languages have types that depend on other types. For example Julia has the Vector{T} type representing vectors with elements of type T, e.g. Vector{Int} for vectors of type Int. With this you can write functions where the type of the output depends on the type of the input.\n\nfunction singleton_vector(x::T)::Vector{T} where {T}\n  return [x]\nend\n\ntypeof(singleton_vector(5))\n\ntypeof(singleton_vector(\"a\"))\n\nWhat dependent type theory adds to this is that it allows the type of the output to depend on the values of the input. For example in\n\n/--\n  Axiom 2.4 (Different natural numbers have different successors).\n  Compare with Mathlib's `Nat.succ_inj`.\n-/\ntheorem Nat.succ_cancel {n m:Nat} (hnm: n++ = m++) : n = m := by\n  injection hnm\n\nthe output type n = m depends on the values of n and m. This type would be impossible to represent in Julia (and in the vast majority of programming languages).\n\nThis is critical for mathematics because it allows us to encode predicates like \"n = m\" or \"f is continuous\" as types.","category":"section"},{"location":"week-4-lecture-2/#The-Axiomatic-Foundation","page":"Week 4 Lecture 2: Formal proofs","title":"The Axiomatic Foundation","text":"It is important to note that type theory is the language of the logic, but it is not the mathematics itself. Just like in pen-and-paper math, we still assume axioms.\n\nMost formal libraries (like Lean's Mathlib) include axioms equivalent to ZFC (Zermelo-Fraenkel Set Theory with Choice), which is the standard foundation of modern mathematics. It would however be possible, in Lean, to write proofs based on a different axiomatic system.","category":"section"},{"location":"week-4-lecture-2/#The-Lean-Kernel-vs.-The-Lean-Proof-Assistant","page":"Week 4 Lecture 2: Formal proofs","title":"The Lean Kernel vs. The Lean Proof Assistant","text":"One potential issue with formal proofs is \"How do we know the proof software itself doesn't have a bug?\"\n\nSystems like Lean are split into two distinct parts:\n\nThe Proof Assistant (The Elaborator/Tactics): This is the large, complex software layer (millions of lines of code) that helps you write the proof. It includes the VS Code interface, the \"tactics\" (commands like induction or rewrite), and automation. It is \"smart\" but potentially buggy.\nThe Kernel (The Verifier): This is a very small, isolated piece of code (often just a few thousand lines). Its only job is to check the final proof object constructed by the assistant against the rules of the logic (Dependent Type Theory).\n\nCrucially, we do not need to trust the Proof Assistant. We only need to trust the Kernel.\n\nIf the \"smart\" automation in the Assistant makes a mistake or has a bug, it will generate a \"proof object\" that is nonsense. When the Kernel tries to check this object, it will reject it. This architecture drastically reduces the amount of the code you have to trust. You don't need to verify the entire Lean software suite, you only need to verify the small Kernel.\n\nThis isolation of a separate kernel is known as the \"de Bruijn criterion\", see e.g. Type Checking in Lean 4.","category":"section"},{"location":"week-9-lecture-2/#Week-9-Lecture-2:-TODO","page":"Week 9 Lecture 2: TODO","title":"Week 9 Lecture 2: TODO","text":"","category":"section"},{"location":"week-10-lecture-1/#Week-10-Lecture-1:-TODO","page":"Week 10 Lecture 1: TODO","title":"Week 10 Lecture 1: TODO","text":"","category":"section"},{"location":"week-8-lecture-1/#Week-8-Lecture-1:-TODO","page":"Week 8 Lecture 1: TODO","title":"Week 8 Lecture 1: TODO","text":"","category":"section"},{"location":"week-12-lecture-2/#Week-12-Lecture-2:-TODO","page":"Week 12 Lecture 2: TODO","title":"Week 12 Lecture 2: TODO","text":"","category":"section"},{"location":"week-14-lab/#Week-14-Lab:-TODO","page":"Week 14 Lab: TODO","title":"Week 14 Lab: TODO","text":"","category":"section"},{"location":"week-13-lecture-2/#Week-13-Lecture-2:-TODO","page":"Week 13 Lecture 2: TODO","title":"Week 13 Lecture 2: TODO","text":"","category":"section"},{"location":"week-2-lab/#Week-2-Lab:-Julia-basics","page":"Week 2 Lab: Julia basics","title":"Week 2 Lab: Julia basics","text":"While the course doesn't directly assume any familiarity with Julia, we also won't have the time for a thorough introduction of the entire Julia language. The goal of this lab is to build some familiarity with the basics of Julia.\n\nSome of the things we will look at in this lab are:\n\nFunctions\nTypes\nLoops\nPlotting\n\nFor this we will use the Pluto notebook lab-2.jl that you can find in the notebooks directory. You can start this notebook in the same way as you started the lab-1.jl notebook in Lab 1. Navigate to the notebooks directory of this repository and start Julia. You can then activate the directory project and start Pluto with\n\nusing Pkg\nPkg.activate(\".\")\nusing Pluto\nPluto.run()\n\ntip: Tip\nIf you get an error of the form \"Package Pluto not found, but a package named Pluto is available from a registry.\" then you might have activated the wrong project. Make sure that you start Julia from the notebooks directory.\n\nIf you can't find the lab-2.jl file inside the notebooks directory you might not have the latest version of this repository. You can update to the latest version by running\n\ngit pull\n\nIf you have made any changes you might need to first remove those before it allows you to pull. You can revert all the changes with\n\ngit restore --hard\n\nor you can revert individual files with\n\ngit restore name_of_file","category":"section"},{"location":"week-2-lab/#High-level-overview-of-Julia","page":"Week 2 Lab: Julia basics","title":"High-level overview of Julia","text":"While not necessary to start using Julia, it can be beneficial to know some of the things that make Julia what it is. Many of these points require experience with other programming languages to have something to compare to.\n\nDynamically typed: You don't have to specify the types of variables, but you can if you want to. This differs from for example C where you always have to specify the types, and Python where you most of the time do not specify the type (modern Python has some support for types though).\nJIT (Just In Time) compiled: Julia compiles the code before it runs it, which allows it to generate optimized code. Compared to many other languages the compilation is however not done in a separate step, but rather the compilation happens as you are running the code. This gives you the performance of a compiled language, but the flexibility of a dynamic language. There are of course downsides to this as well, the most notable one being that sometimes it can take quite some time to compile the code.\nGarbage collected: In some programming languages, most notably C, the programmer is in charge of managing the memory that the program uses. Most modern languages (with some notable exceptions) defer the memory handling to a process known as garbage collection. This is convenient when writing the code, but in some cases comes with performance issues. We will see some of these issues when working with high precision intervals later in the course.\n1-based indexing: Different programming languages use different conventions regarding whether 0 or 1 is the first index in an array. Julia, together with for example Matlab and Fortran, uses 1-based indexing, meaning that the first index in an array is 1. Python and C (and many other languages) instead use 0-based indexing, where the first index is 0. In mathematics we usually switch indexing depending on context, e.g. matrices are indexed starting from 1, whereas polynomial coefficients are indexed from 0. For some reason people have strong opinions about this.\nMultiple dispatch: An important part of what makes Julia Julia is that it makes use of multiple dispatch for function overloading. This allows you to define multiple versions of a function. Which version is being used is determined based on the type of the input arguments. This is in particular very useful for rigorous numerics since it makes it relatively easy to write code that works for both non-rigorous floating point numbers as well as rigorous interval arithmetic. We will see more examples of how this works in practice later in the course.","category":"section"},{"location":"week-6-lecture-1/#Week-6-Lecture-1:-TODO","page":"Week 6 Lecture 1: TODO","title":"Week 6 Lecture 1: TODO","text":"","category":"section"},{"location":"week-12-lecture-1/#Week-12-Lecture-1:-TODO","page":"Week 12 Lecture 1: TODO","title":"Week 12 Lecture 1: TODO","text":"","category":"section"},{"location":"week-14-lecture-2/#Week-14-Lecture-2:-TODO","page":"Week 14 Lecture 2: TODO","title":"Week 14 Lecture 2: TODO","text":"","category":"section"},{"location":"week-14-lecture-1/#Week-14-Lecture-1:-TODO","page":"Week 14 Lecture 1: TODO","title":"Week 14 Lecture 1: TODO","text":"","category":"section"},{"location":"week-1-lab/#Week-1-Lab:-Setup","page":"Week 1 Lab: Setup","title":"Week 1 Lab: Setup","text":"The main goal of this lab is to:\n\nInstall Julia\nStart the Julia REPL and run basic commands\nDownload the course package\nRun a Pluto notebook\nBonus: Build this documentation\nInstall an editor for working with Julia files\nBonus: Reproduce a computer-assisted proof","category":"section"},{"location":"week-1-lab/#Install-Julia","page":"Week 1 Lab: Setup","title":"Install Julia","text":"The Julia website has a page about install Julia. The recommended way to do this is to use juliaup. Exactly how to install juliaup depends on your operating system. On Linux and Mac it should be enough to run\n\ncurl -fsSL https://install.julialang.org | sh\n\nOn Windows you can install it through the Windows store. Alternatively you can use WSL and install Julia inside the WSL environment.","category":"section"},{"location":"week-1-lab/#Julia-REPL","page":"Week 1 Lab: Setup","title":"Julia REPL","text":"The most common way to use Julia is through the interactive command-line REPL (real-eval-print-loop). See the Julia REPL documentation for much more details than we'll cover here.\n\nIf you start Julia (by running julia from a terminal) you will be greeted with a prompt where you can run Julia commands. You can for example try running the following commands\n\n1 + 1\nsin(2)\nf(x) = x - exp(x)\nf(0.1)\nf(-1)\nM = [1 2; 3 4]\ninv(M)\nb = [5, 6]\na = M \\ b\nM * a","category":"section"},{"location":"week-1-lab/#REPL-modes","page":"Week 1 Lab: Setup","title":"REPL modes","text":"When you start the Julia REPL you enter what is called the \"Julian mode\", where you can run Julia code and see the results. The REPL also has 3 other modes which are used in certain cases:\n\nHelp mode: You enter this mode by writing ? when at an empty line in the Julian mode. You can then write the name of a function to get the documentation for that function. Try writing for example ?sin and ?Int.\nPkg mode: You can enter this mode by writing ]. It is used for installing and interacting with packages, which we will do soon. Note that you can exit the mode by typing backspace.\nShell mode: You can enter this mode by writing ;. It us used for running simple terminal commands directly from Julia. We won't make much use of it directly, but it is occasionally useful.","category":"section"},{"location":"week-1-lab/#Download-course-package","page":"Week 1 Lab: Setup","title":"Download course package","text":"This documentation is structured as a Julia package, with the name CAPCourseS26.jl. It is available in a Github repository. Most of the code we will work on in the course will be contained in this package. To use the package we first have to download it.\n\nTo download the package we want to use git, the version control tool that Github derives it names from.\n\nIf you are running Linux you probably already have git installed, on Debian based systems (e.g. Ubuntu) you can otherwise install it with sudo apt install git.\nOn Mac you should be able to install it by just running git in the terminal and follow the prompts. You can also install it in other ways, see Mac install instructions.\nOn Windows you can use the git installer linked from the Windows install instructions.\n\nOnce you have git installed you should be able to download the repository. To do so navigate to the directory where you want to put the repository and run\n\ngit clone https://github.com/Joel-Dahne/CAPCourseS26.jl.git\n\nThis should create a directory with the name CAPCourseS26.jl. If you navigate into CAPCourseS26.jl and start Julia you should then be able to use the following code to run the tests in the package.\n\nusing Pkg\nPkg.activate(\".\") # Activate the package CAPCourseS26\nPkg.instantiate() # Install all dependencies of CAPCourseS26\nPkg.test() # Run the tests for CAPCourseS26\n\nIf everything is working as it should this should, after some time, end with something of the form.\n\n     Testing Running tests...\nTest Summary: | Pass  Total  Time\nCAPCourses26  |    1      1  0.0s\n     Testing CAPCourseS26 tests passed\n","category":"section"},{"location":"week-1-lab/#Pluto-notebook","page":"Week 1 Lab: Setup","title":"Pluto notebook","text":"In addition to the Julia REPL we will make use of Pluto notebooks as a tool for writing, running and interacting with Julia code. Another common notebook tool which some of you might have encountered is Jupyter. Jupyter also works with Julia (that is the Ju in Jupyter), but for this course we will mainly make use of Pluto, which is Julia only.\n\nTo start Pluto navigate to the notebooks directory of this repository and start Julia. You can activate the directory project and start Pluto with\n\nusing Pkg\nPkg.activate(\".\")\nusing Pluto\nPluto.run()\n\nThis should print a link which you can open in your browser. From the browser you should then start the lab-1.jl notebook and follow the directions in it.","category":"section"},{"location":"week-1-lab/#Bonus:-Build-this-documentation","page":"Week 1 Lab: Setup","title":"Bonus: Build this documentation","text":"You can build the documentation that you are currently reading by running the following code from the root of this directory.\n\njulia --project=docs docs/make.jl\n\nThis will build an HTML version of the package. To be able to read this in a convenient way requires some more tools. If you have Python installed you should be able to do so with\n\ncd docs/build\npython3 -m http.server --bind localhost\n\nand open http://127.0.0.1:8000/ in your browser. Alternatively you can build a pdf version of the documentation with\n\njulia --project=docs docs/make.jl latex\n\nand open the PDF-file docs/build/CAPCourseS26.jl.pdf.","category":"section"},{"location":"week-1-lab/#Install-an-editor","page":"Week 1 Lab: Setup","title":"Install an editor","text":"While it is possible to write Julia code in any text editor, one usually uses an editor which has tools for working with Julia code.\n\nIf you don't know which editor to use you should probably use VS Code. Follow the instructions on their website to install it.\n\nPersonally, I use Emacs as my editor. Unless you have previous experience with Emacs I would however not recommend starting with this. If you do want to use Emacs, these are the most relevant parts of my configuration. Feel free to ask for more details in case you are interested.\n\n(use-package julia-mode\n  :ensure julia-mode\n  :bind (\"C-c f\" . jd/julia-format)\n  :config (setq julia-max-block-lookback 50000))\n\n(use-package julia-ts-mode\n  :ensure t\n  :mode (\"\\\\.jl\" . julia-ts-mode)\n  :hook (julia-ts-mode . auto-revert-mode))\n\n(defun jd/processor-count ()\n  \"Get the number of processors using nproc. If nproc is not found it returns 1\"\n  (if (executable-find \"nproc\")\n      (with-temp-buffer\n        (call-process (executable-find \"nproc\") nil t nil)\n        (string-to-number (buffer-string)))\n    1))\n\n(defun jd/julia-repl-activate-cd-parent ()\n  \"Run julia-repl-activate-parent and also cd to the directory of the project file.\"\n  (interactive)\n  (progn\n    (julia-repl-activate-parent nil)\n    (if-let ((projectfile (julia-repl--find-projectfile)))\n        (progn\n          (message \"cd-ing to %s\" projectfile)\n          (julia-repl--send-string\n           (concat \"cd(\\\"\"\n                   (expand-file-name (file-name-directory projectfile)) \"\\\")\")))\n      (message \"could not find project file\"))))\n\n(use-package vterm\n    :ensure t\n    :config (progn (setq vterm-kill-buffer-on-exit nil)))\n\n(use-package julia-repl\n  :ensure t\n  :hook julia-mode\n  :config\n  (julia-repl-set-terminal-backend 'vterm)\n  (setenv \"JULIA_NUM_THREADS\" (number-to-string (/ (jd/processor-count) 2)))\n  (define-key julia-repl-mode-map (kbd \"C-c C-a\") 'jd/julia-repl-activate-cd-parent))\n\nFor most people the editor is likely the place where they most of the time interact with Julia. It is therefore a good idea to get accustomed to the editor and the tools it offers. When showing Julia code throughout the course I'll try to switch between using VS Code and Emacs, to give you a feeling for how things differ between editors.\n\nWe will take a closer look at how to use the editor for working with Julia code next week.","category":"section"},{"location":"week-1-lab/#Bonus:-Reproduce-a-computer-assisted-proof","page":"Week 1 Lab: Setup","title":"Bonus: Reproduce a computer-assisted proof","text":"If you feel inclined you can try to use the things you have learned above to reproduce a computer-assisted proof from a very recent paper. At the time of writing the paper has been submitted to Arxiv, but is still waiting to appear. At this point we have of course not learned enough to actually understand the proof, but you can nevertheless run the code.\n\nIn the Github repository for the paper is all the code used for the computer-assisted proof. The proofs are presented in Pluto notebooks and our goal will be to run one of these notebooks. To do this we need to:\n\nClone the repository\nStart Julia from the repository\nInstall dependencies and start the Pluto notebook\n\nTo clone the repository you can navigate to the directory where you want to place it and run\n\ngit clone https://github.com/Joel-Dahne/SpectralRegularPolygon.jl.git\n\nNext step is to start Julia from the SpectralRegularPolygon.jl directory. Here we need to make one adjustment from before however. At the time of writing the latest Julia version is 1.12.4 and this is the one that was automatically installed when you installed juliaup earlier. The computer-assisted proof was however done using Julia 1.11.8, so we need to run that version. You can install the version with\n\njuliaup add 1.11.8\n\nand to start the correct Julia version you then run\n\njulia +1.11.8\n\nNote the extra +1.11.8! After you have started the right Julia version you should be able to follow the instructions in the README of the repository. Which is to first run\n\nusing Pkg\nPkg.activate(\".\")\nPkg.instantiate()\nPkg.test()\n\nto install the dependencies and run the package tests. This will likely take some time! Next you want to start Pluto with\n\nusing Pluto\nPluto.run()\n\nThis should open Pluto in your browser. From your browser window you can then open the notebooks inside the proofs directory. There is quite a few of them and some of them take some time to run. I would recommend you open the one named proposition_3_1.jl, which has the nicest plots!\n\nnote: Note\nThe instructions here are slightly different from the earlier ones about how to start Pluto. In the earlier instructions you should navigate to the notebooks subdirectory and run Pkg.activate(\".\") from there. In these instructions you should run Pkg.activate(\".\") from the root of the directory. The reason for this difference is that Julia 1.12 introduced some features that simplifies some parts of project managements. Since we are here using Julia 1.11.8 these tools are not in place and the approach is slightly different.\n\nIf you have managed to get this far and run the above mentioned notebook, then congratulations! You have just reproduced a computer-assisted proof!","category":"section"},{"location":"week-3-lab/#Week-3-Lab:-A-simple-computer-assisted-proof-(and-VS-Code)","page":"Week 3 Lab: A simple computer-assisted proof (and VS Code)","title":"Week 3 Lab: A simple computer-assisted proof (and VS Code)","text":"The goal of this lab is to reproduce a simple computer-assisted proof by Alex and Mitch. In doing so we will use VS Code in order to get used to working with Julia from that interface.","category":"section"},{"location":"week-3-lab/#VS-Code-setup","page":"Week 3 Lab: A simple computer-assisted proof (and VS Code)","title":"VS Code setup","text":"Start VS Code\nOpen the course folder. You can find Open Folder under File (shortcut Ctrl-K Ctrl-O).\nCreate a new file lab-3.jl in the directory scratch/labs (which you will have to create). You can find New File under File (shortcut Ctrl-Alt-Super-N).\nCopy the following code into the file\nusing Arblib\n\nx = Arb(π)\ny = Arb(3)\n\nprintln(x)\n\nThere are multiple different ways to run the code in the file.\n\nExecute entire file in a separate process. Use the shortcut Ctrl-F5.\nExecute entire file in REPL. Press the arrow in the top right corner.\nExecute the current line in the REPL. Use the shortcut Ctrl-Enter. You can also select multiple lines and it will execute all of them.\nExecute the current line in the REPL and move to the next line. Use the shortcut Shift-Enter.\n\nTry all of them and see how they work! Which of these alternatives is best depends on your workflow. In my case I almost exclusively make use of alternatives 3 and 4.\n\nFor this lab we will write all of our code inside this one file and use the above commands for running it. For larger projects it is usually a good idea to split the work into multiple files and in many cases it is beneficial to structure the project like a Julia package. We will get back to this later in the course.","category":"section"},{"location":"week-3-lab/#A-simple-computer-assisted-proof","page":"Week 3 Lab: A simple computer-assisted proof (and VS Code)","title":"A simple computer-assisted proof","text":"Our goal will be to prove Proposition II.2 in paper mentioned above. For this we need the following polynomial\n\nP = 1 - 3alpha^2 + alpha^4 - frac11149alpha^6 + frac143294alpha^8 - frac753693311957764alpha^10 allowbreak + frac459817233147460365316alpha^12 - frac30028809212865451520327364608478700alpha^14 allowbreak + frac4975014185899222712487856750603488800alpha^16\n\nand the function\n\nE(alpha) = frac6alpha^915 - 20alpha left( 1 + sqrt3alpha + sqrt2alpha^2 + fracsqrt147alpha^3 + fracsqrt25842alpha^4 + fracsqrt19688373458alpha^5 + fracsqrt10652579931122alpha^6 + frac2sqrt2129312323981473624696345alpha^7 + fracsqrt1836431197552144544997570760alpha^8 right) + frac9alpha^18(15 - 20alpha)^2\n\nWe want to prove the following proposition\n\nnote: Proposition\nThe function P(alpha) + E(alpha) is negative at alpha = 061 and the function P(alpha) + E(alpha) is positive at alpha = 057.\n\nWhat they are actually trying to prove in the paper is that a function v(alpha) has a zero on the interval 057 061. The polynomial P is an approximation of v and E(alpha) gives an upper bound of the error for the approximation.\n\nnote: Note\nThe proof in the paper is not quite rigorous. It uses regular floating points together with results that gives you bound on the rounding errors for evaluating polynomials with floating points. It does however not quite account for all the places where rounding errors are introduced.Moreover, the expression for E(alpha) seems to not quite be correct. This is being looked into.","category":"section"},{"location":"week-3-lab/#A-not-quite-correct-implementation","page":"Week 3 Lab: A simple computer-assisted proof (and VS Code)","title":"A not quite correct implementation","text":"Our first task will be to implement P and E in Julia. A direct (but as we will see slightly problematic) conversion of the above expressions for P and E to Julia is given below. You can copy the code into your lab-3.jl file (you can also remove what is already there if you want).\n\nP(α) =\n    1 - 3α^2 + α^4 - 111 / 49 * α^6 + 143 / 294 * α^8 - 7536933 / 11957764 * α^10 +\n    4598172331 / 47460365316 * α^12 - 30028809212865451 / 520327364608478700 * α^14 +\n    49750141858992227 / 12487856750603488800 * α^16\n\nE(α) =\n    6α^9 / (15 - 20α) * (\n        1 +\n        sqrt(3) * α +\n        sqrt(2) * α^2 +\n        sqrt(14) / 7 * α^3 +\n        sqrt(258) / 42 * α^4 +\n        sqrt(1_968_837) / 3458 * α^5 +\n        sqrt(106_525_799) / 31_122 * α^6 +\n        2sqrt(2_129_312_323_981_473) / 624_696_345 * α^7 +\n        sqrt(183_643_119_755_214_454) / 4_997_570_760 * α^8\n    ) + 9α^18 / (15 - 20α)^2\n\nWe can now implement functions computing lower and upper bounds of v as\n\nv_lower(α) = P(α) - E(α)\nv_upper(α) = P(α) + E(α)\n\nTry evaluating v_lower at alpha = 057 and v_upper at alpha = 061. You should get\n\nv_lower(0.57)\nv_upper(0.61)\n\nOne can show (which we will soon do) that\n\nP(057) - E(057) = 002857494349754851842380087961496259058289166470274019410736704418784480353dots\n\nand\n\nP(061) + E(061) = -0018830737780969134528206854817563710242534970158515810047488029826855742148dots\n\nTo what extent do your computed values above agree with these values? If they disagree, what could be the reason? We can also evaluate the functions to higher precision in Julia using BigFloat, Julia's standard type for arbitrary precision floating points.\n\nv_lower(BigFloat(0.57))\nv_upper(BigFloat(0.61))\n\nTo what extent do these values agree with the above given ones? What could be the reason for them not agreeing?","category":"section"},{"location":"week-3-lab/#An-improved-implementation","page":"Week 3 Lab: A simple computer-assisted proof (and VS Code)","title":"An improved implementation","text":"The issue with the above implementation of P(alpha) and E(alpha) is that Julia by default uses Float64 for numerical computations unless told otherwise. This happens for example when dividing integers with each other and when computing square roots of integers.\n\n1 / 3\nsqrt(2)\n\nTo make our implementation of P and E work for rigorous numerics we need to make sure that all intermediate computations are also done in a rigorous way. For integer division one way of achieving this is to represent the coefficient as a rational number, which is an exact representation.\n\n1 / 3 # Performs floating point arithmetic\n1 // 3 # Represents it as a rational number\n\nThis plays well with for example BigFloat (and rigorous numerics as well)\n\n(1 / 3) * BigFloat(3) # Not close to 1\n(1 // 3) * BigFloat(3) # Very close to 1\n\nLet us define a new function P_correct which represents the coefficients as rational numbers.\n\nP_correct(α) =\n    1 - 3α^2 + α^4 - 111 // 49 * α^6 + 143 // 294 * α^8 - 7536933 // 11957764 * α^10 +\n    4598172331 // 47460365316 * α^12 - 30028809212865451 // 520327364608478700 * α^14 +\n    49750141858992227 // 12487856750603488800 * α^16\n\nFor the square roots we don't have a way of exactly representing the values (at least not without using any external package). Instead we take the approach of converting the integer to the right type before computing the square root.\n\nBigFloat(sqrt(2))^2 # This gives large errors\nsqrt(BigFloat(2))^2 # This is very precise\n\nFor the function E the type we want to convert the integers is given by the type of the argument α. For this you can use the function oftype.\n\na = BigFloat(2)\noftype(a, 1 // 3) # Convert 1 // 3 to the type of a\n\nUsing this for E we get\n\nE_correct(α) =\n    6α^9 / (15 - 20α) * (\n        1 +\n        sqrt(oftype(α, 3)) * α +\n        sqrt(oftype(α, 2)) * α^2  +\n        sqrt(oftype(α, 14)) / 7 * α^3  +\n        sqrt(oftype(α, 258)) / 42 * α^4  +\n        sqrt(oftype(α, 1_968_837)) / 3458 * α^5  +\n        sqrt(oftype(α, 106_525_799)) / 31_122 * α^6  +\n        2sqrt(oftype(α, 2_129_312_323_981_473)) / 624_696_345 * α^7  +\n        sqrt(oftype(α, 183_643_119_755_214_454)) / 4_997_570_760 * α^8\n    ) + 9α^18 / (15 - 20α)^2\n\nNote that we are not converting the integers we are dividing by. Why do we not need to do that?\n\nLet us finally define\n\nv_lower_correct(α) = P_correct(α) - E_correct(α)\nv_upper_correct(α) = P_correct(α) + E_correct(α)\n\nAfter this we can now compute\n\nv_lower_correct(BigFloat(0.57))\nv_upper_correct(BigFloat(0.61))\n\nHow does this compare to the values given above? What is the issue now?\n\nThe last issue comes from using the constants 0.57 and 0.61. We have\n\nBigFloat(0.57)\nBigFloat(0.61)\nBigFloat(\"0.57\")\nBigFloat(\"0.61\")\n\nFinally we thus compute\n\nv_lower_correct(BigFloat(\"0.57\"))\nv_upper_correct(BigFloat(\"0.61\"))\n\nNow it should agree with the values given above!\n\nnote: Note\nThis last fix, using BigFloat(\"0.57\") instead of BigFloat(0.57), is not strictly necessary for a computer-assisted proof. There is nothing special about the value 057 = 57  100, we could just as well use the value 056999999999999995115018691649311222136020660400390625 which is the exact value for the Float64 number that is closest to 057 = 57  100. Of course, the statement in the paper would look slightly awkward: \"it is positive at alpha = 056999999999999995115018691649311222136020660400390625\" doesn't look quite as nice as \"it is positive at alpha = 057\"","category":"section"},{"location":"week-3-lab/#A-computer-assisted-proof","page":"Week 3 Lab: A simple computer-assisted proof (and VS Code)","title":"A computer-assisted proof","text":"So far we have of course not proved anything, we have still only evaluated the functions using floating point arithmetic which will give us rounding errors. With the P_correct and E_correct implementations it is however very straightforward to fully prove the result. For this we will use the Arblib.jl package which implements one type of interval arithmetic. To load the package you can add this to the top of your file (it doesn't have to be at the top, but you usually put packages there)\n\nusing Arblib\n\nWe can now evaluate our functions using interval arithmetic\n\nv_lower_correct(Arb(\"0.57\"))\nv_upper_correct(Arb(\"0.61\"))\n\nThis gives us fully rigorous enclosures for the values. From these enclosures we immediately see that P(057) - E(057) is positive and P(061) + E(061) is negative. We have thus finished the proof!","category":"section"},{"location":"week-3-lab/#Bonus:-Tighten-enclosure-of-root","page":"Week 3 Lab: A simple computer-assisted proof (and VS Code)","title":"Bonus: Tighten enclosure of root","text":"So far we have proved that there is a root in the interval 057 061. Can you compute a tighter enclosure of the root?","category":"section"},{"location":"week-3-lecture-1/#Week-3-Lecture-1:-Computer-assisted-proofs-for-continuous-problems","page":"Week 3 Lecture 1: Computer-assisted proofs for continuous problems","title":"Week 3 Lecture 1: Computer-assisted proofs for continuous problems","text":"Last week we looked at some examples of computer-assisted proofs for discrete problems, this included\n\nThe four color theorem\nThe boolean Pythagorean triples problem\nGoldbach's weak conjecture\n\nThis week we will look at computer-assisted proofs applied to continuous problems. Since computers are inherently discrete machines, working with continuous problems requires a slightly different approach.\n\nComputer-assisted proofs are not applicable to all types of problems. Throughout the course we will talk about what type of problems are good candidates for computer-assisted proofs and by the end of the course my hope is that you will have at least a rough idea for when a computer-assisted proof could be applied to a problem. I have three guiding principles as to which problems are good candidates for computer-assisted proofs. For a computer-assisted proof to be applicable to a problem the result should be:\n\nNumerically obvious\nStable under perturbations\nOn a compact domain\n\nExpanding a bit on these points:\n\nRigorous numerical methods will never give you better results than classical non-rigorous methods. If you cannot produce very convincing numerical evidence that the result is true, then any attempt to rigorously prove it is doomed to fail.\nNumerical methods will always produce approximations. Rigorous numerical methods can give you bounds for the errors of these approximations, but cannot eliminate them. When setting up the problem there is however usually a lot of freedom in what type of perturbations the result should be stable under.\nIn the end the result needs to reduce to a finite computation and that requires some sort of compactness. If you have an infinite domain it is in many cases possible to compactify it, usually at the cost of introducing singularities at the boundaries that need to be dealt with.\n\nThe example we will look at in this lecture is from a recent paper from my own research and is related to the field of spectral geometry. We will look at what spectral geometry is, the problem the paper handles and how it relates to the guiding principles mentioned above. The paper in question is:\n\nMonotonicity of the first Dirichlet eigenvalue of regular polygons (code)\n\nTwo other papers that make use of similar methods, and which we might look more at later in the course, are:\n\nComputation of Tight Enclosures for Laplacian Eigenvalues (code)\nA counterexample to Payne’s nodal line conjecture with few holes (code)","category":"section"},{"location":"week-3-lecture-1/#Spectral-geometry","page":"Week 3 Lecture 1: Computer-assisted proofs for continuous problems","title":"Spectral geometry","text":"In spectral geometry one studies how the eigenvalues and eigenfunctions of the Laplacian depend on the domain. In our case we are interested in problems of the form\n\nbegincases\n  -Delta u = lambda u textinquad Omega\n  u = 0 textonquad partialOmega\nendcases\n\nHere -Delta u = lambda u means that u is an eigenfunction of the Laplacian, with associated eigenvalue lambda. Recall that in the plane, Delta is just the sum of the second derivatives\n\nDelta u = fracpartial^2 upartial x^2 + fracpartial^2 upartial y^2\n\nThe eigenvalue equation is then combined with a boundary condition, in this case u = 0 on the boundary of Omega. One can consider other types of boundary conditions, but for our purposes here we will stay with u = 0, corresponding to a zero Dirichlet boundary condition.\n\nIf Omega is reasonably well behaved there is a countable sequence of eigenvalues, lambda_1  lambda_2 leq lambda_3 leq dots, all with an associated eigenfunction u_k. In spectral geometry we are interested in how these eigenvalues and eigenfunctions depend on the domain Omega.","category":"section"},{"location":"week-3-lecture-1/#Monotonicity-of-the-first-Dirichlet-eigenvalue-of-regular-polygons","page":"Week 3 Lecture 1: Computer-assisted proofs for continuous problems","title":"Monotonicity of the first Dirichlet eigenvalue of regular polygons","text":"In this case we consider domains mathbbP_N which are regular polygons and we are interested in how the first eigenvalue of these polygons, lambda_1(mathbbP_N) depend on N. For our purposes we want to normalize the polygons so that they have area pi.\n\nIf we plot lambda_1(mathbbP_N) as a function of N we have the following picture\n\n(Image: First eigenvalue of regular polygons)\n\nFrom this picture one could maybe conjecture that the eigenvalues are decreasing with N. This picture is however maybe not the most convincing, we only go up to N = 64 and for the larger values the graph mostly looks flat in this plot. To get a slightly better figure let us start by noting that as N goes to infinity we would have that the polygons approach the unit circle. For the unit circle the eigenvalue can be explicitly computed; it's the first root of the Bessel function J_0. If we plot the difference between lambda(mathbbP_N) and this value and give the y-axis a logarithmic scale we get (ignore the error bound for now, we will get back to that).\n\n(Image: First eigenvalue of regular polygons compared to unit disc)\n\nThis is pretty strong numerical evidence that eigenvalues are monotone, at least up to N = 64. Of course, a lot of the details are in how these values are actually computed. There are approximations errors in these values, can we trust that they are small enough not to change the conclusion?\n\nLet us consider this problem in terms of the above discussed guidelines for computer-assisted proofs. Let us start with considering the project only for the finite set N = 3 4 dots 64.\n\nNumerically obvious: The above figure gives fairly convincing numerical results.\nStable under perturbations: If we perturb the eigenvalues slightly the monotonicity should still hold.\nOn a compact domain: When restricted to N = 3 4 dots 64 the domain is clearly compact.\n\nThis indicates that a computer-assisted proof might be viable. Of course, for a complete result we need to also handle all N geq 64. How can we achieve that? The rough idea is to see the eigenfunctions not as a function of N, but as a function of eta = frac1N. We then have to prove monotonicity of the eigenvalues for all eta in 0 164. This gives a compact domain, at the cost of very singular behavior at eta = 0.","category":"section"},{"location":"week-3-lecture-1/#Handling-N-3,-4,-\\dots,-64","page":"Week 3 Lecture 1: Computer-assisted proofs for continuous problems","title":"Handling N = 3 4 dots 64","text":"Let us focus on the finite case of proving that lambda(mathbbP_N) for N = 3 4 dots 64. Handling the infinite limit builds on some of the same ideas, but requires significantly more work.\n\nThe question to ask is, how were the approximate eigenvalues in the figures above computed? There are many different ways of numerically computing eigenvalues, some of them more suited for rigorous verification.\n\nIn this case the computations were done using the Method of Particular Solutions, see the paper Reviving the Method of Particular Solutions for a more thorough overview of the method. Recall that we are trying to solve the equation\n\nbegincases\n  -Delta u = lambda u textinquad Omega\n  u = 0 textonquad partialOmega\nendcases\n\nThe idea is to approximate u using a linear combination of basis functions,\n\nu_app(x y) = sum_i = 1^M c_i phi_i(x y)\n\nHow should the basis functions phi_i and the coefficients c_i be chosen? The basis functions we choose so that they satisfy the equation -Deltaphi_i = lambda_appphi_i exactly for the approximate eigenvalue lambda_app, but they will not satisfy any specific boundary condition. We then choose the coefficients c_i as to make the resulting linear combination as close on the boundary as possible. This gives us an approximation u_app, lambda_app with\n\nbegincases\n  -Delta u_app = lambda_app u_app textinquad Omega\n  u_app approx 0 textonquad partialOmega\nendcases\n\nThe precise choice of phi_i depends highly on the domain and we won't go into the details here. For this specific case the approximation used is\n\nu_app(x y) = J_0left(rsqrtlambdaright)\n+ a_2J_Nleft(rsqrtlambdaright)cos Ntheta\n+ sum_n = 1^N left(\n  b_1J_alphaleft(r_nsqrtlambdaright)sin alphatheta_n\n  + b_2J_2alphaleft(r_nsqrtlambdaright)sin 2alphatheta_n\nright)\n\nHere alpha = pi  N, r is the distance from (x y) to the center of the domain, and (r_n theta_n) are the polar coordinates of the point (x y) when centered around vertex n and the orientation taken so that theta_n = 0 corresponds to the boundary segment between vertex n and n + 1.\n\nSo far there is nothing in this approach which is related to rigorous numerics. This is just a classical numerical approach which we can use to compute approximations of eigenvalues and eigenfunctions. What makes this approach suitable for a computer-assisted proof is the following theorem by Fox, Henrici and Moler.\n\nnote: Theorem\nLet OmegasubsetmathbbR^n be bounded. Let lambda_app and u_app be an approximate eigenvalue and eigenfunction–-that is, they satisfy Delta u_app+lambda_app u_app=0 in Omega but not necessarily u_app = 0 on~partialOmega. Define  mu = fracsqrtOmegasup_x in partial Omegau_app(x)u_app_2where Omega is the area of the domain. Then there exists an eigenvalue lambda such that  fraclambda_app - lambdalambda leq mu\n\nThe value mu measures how close to zero the approximate eigenfunction is on the boundary. If mu is very small, meaning that the approximate eigenfunction is very close to zero on the boundary, then the theorem guarantees us that there is a true eigenvalue lambda close to our approximate eigenvalue lambda_app.\n\nIf we can find a bound for mu, which only requires us to bound an explicit function on an explicit domain, we can then get upper bounds for the error in our approximations of the eigenvalues. If these upper bounds are sufficiently small we can still verify the monotonicity of the eigenvalues! The approach for bounding mu is something we will talk more about later in the course.\n\nThe proof for the monotonicity then follows from the following figure. It shows the difference lambda_1(mathbbP_N) - lambda_1(mathbbP_N + 1) for the approximate eigenvalues. For the eigenvalues to be monotone this difference must be positive. It also shows the error for each point, coming from the sum of the error for the two eigenvalues. Since the error is smaller than the value, the difference must be positive also for the true eigenvalues.\n\n(Image: Difference between eigenvalues of regular polygons)","category":"section"},{"location":"week-6-lecture-2/#Week-6-Lecture-2:-Interval-Arithmetic","page":"Week 6 Lecture 2: Interval Arithmetic","title":"Week 6 Lecture 2: Interval Arithmetic","text":"In this lecture we will begin our study of interval arithmetic, which will be our primary tool for the rest of the course. We will look at the basics of interval arithmetic, a couple of different flavors of interval arithmetic and discuss how they are implemented on the computer. For this lecture we will stick to basic arithmetic; in the next lecture we will look at how to extend this to general functions.","category":"section"},{"location":"week-6-lecture-2/#Real-interval-arithmetic","page":"Week 6 Lecture 2: Interval Arithmetic","title":"Real interval arithmetic","text":"The basic idea of interval arithmetic is to do arithmetic directly on intervals instead of on individual numbers. We extend the arithmetic operations to intervals by treating them as sets of real numbers. If bma = underlinea overlinea and bmb = underlineb overlineb are two closed intervals then for any operation star of + - cdot  we let\n\nbma star bmb = a star b a in bma b in bmb\n\nA crucial aspect of this definition is that the right-hand side is an interval (assuming 0 notin bmb for division). This is a direct consequence of the operations being continuous. To compute the resulting intervals we only need to make use of the endpoints, more precisely we have the following formulas:\n\nbma + bmb = underlinea + underlineb overlinea + overlineb\nbma - bmb = underlinea - overlineb overlinea - underlineb\nbma cdot bmb = min(underlineaunderlineb underlineaoverlineb overlineaunderlineb overlineaoverlineb) max(underlineaunderlineb underlineaoverlineb overlineaunderlineb overlineaoverlineb)\nbma  bmb = bma cdot 1  overlineb 1  underlineb\n\nnote: Example\nWe havebeginalign*\n  1 2 + 2 pi = 3 2 + pi\n  1 2 - 1 2 = -1 1\n  1 sqrt3 cdot -1 1 = -sqrt3 sqrt3\n  1 3  -2 -1 = -3 -13\nendalign*\n\nIf r(x) is a rational function then we can extend r to work on intervals by using our interval versions of interval arithmetic.\n\nnote: Example\nConsider the rational functionr(x) = 2x - x^2 = 2x - x cdot xApplying this to the interval 0 1 we getr(0 1) = 20 1 - 0 1 cdot 0 1\n= 0 2 - 0 1\n= -1 2Alternatively we can write r asr(x) = x(2 - x)In this case we getr(0 1) = 0 1 cdot (2 - 0 1)\n= 0 1 cdot 1 2\n= 0 2The range (or image) of the interval 0 1 under r ismathcalR(r 0 1) = r(x) x in 0 1 = 0 1\n\nAs the above example shows, directly replacing the regular arithmetic operations with their interval counterparts has problems. Different formulations of the function r that are equivalent when computing with real numbers are no longer equivalent when computing with intervals. One of the most important properties of interval arithmetic is however the following\n\nnote: Proposition\nLet r be a rational function and bma a closed interval. For any formulation (or representation) of r in terms of arithmetic operations we havemathcalR(r bma) subseteq r(bma)as long as r(bma) never involves a division by an interval containing zero.\n\nnote: Remark\nThe reason to avoid division by zero is, of course, not because if we do divide by zero we might get an answer that doesn't satisfy the specified property. We simply haven't defined what division by zero should do. In the implementations we'll see, division by zero will return something similar to NaN for floating points.\n\nnote: Remark\nThe specification \"For any formulation of r\" is not something that is commonly encountered in mathematics. Usually we don't care how a function is specified, just how it maps inputs to outputs. The canonical way to specify an interval version of a function would be to let r(bma) be the convex hull of mathcalR(r bma). In this case the function is uniquely determined from how r maps inputs to outputs. However, actually implementing this function on a computer would in general not be possible. Since the main goal of interval arithmetic is precisely to implement the functions on a computer, we are stuck with having to take into account the formulation/representation of the function r.","category":"section"},{"location":"week-6-lecture-2/#Flavors-of-interval-arithmetic","page":"Week 6 Lecture 2: Interval Arithmetic","title":"Flavors of interval arithmetic","text":"We will look at three different aspects of interval arithmetic that play a role in how you work and think about it:\n\nReal numbers or floating points\nIntervals or balls\nWide intervals or thin intervals","category":"section"},{"location":"week-6-lecture-2/#Real-numbers-or-floating-points","page":"Week 6 Lecture 2: Interval Arithmetic","title":"Real numbers or floating points","text":"In the previous section we looked at interval arithmetic over the real numbers. For actual computations we will therefore have to work with intervals where the endpoints are floating point numbers. When performing arithmetic operations we will have to take into account the rounding errors from floating points.\n\nIf bma = underlinea overlinea and bmb = underlineb overlineb are intervals with endpoints given by floating points then\n\nbma + bmb = nabla(underlinea + underlineb) Delta(overlinea + overlineb)\nbma - bmb = nabla(underlinea - overlineb) Delta(overlinea - underlineb)\nbma cdot bmb = min(nabla(underlineaunderlineb) nabla(underlineaoverlineb) nabla(overlineaunderlineb) nabla(overlineaoverlineb)) max(Delta(underlineaunderlineb) Delta(underlineaoverlineb) Delta(overlineaunderlineb) Delta(overlineaoverlineb))\nbma  bmb = bma cdot nabla(1  overlineb) Delta(1  underlineb)\n\nIn many contexts the distinction between the endpoints being real numbers or floating points is however not that important. Even when working with real numbers the computed intervals will contain overestimations, see the example in the previous section. The extra overestimations coming from the floating point rounding do therefore not qualitatively change the behavior. For many purposes it is therefore useful to think about interval arithmetic over real numbers, even if in the end you have to implement it in floating points.","category":"section"},{"location":"week-6-lecture-2/#Intervals-or-balls","page":"Week 6 Lecture 2: Interval Arithmetic","title":"Intervals or balls","text":"So far we have represented our intervals by using a lower and upper bound, bma = underlinea overlinea. It is of course mathematically equivalent to represent them with a midpoint and a radius, bma = m pm r. How to perform arithmetic operations does however change slightly. If bma = m_1 pm r_1 and bmb = m_2 pm r_2 then\n\nbma + bmb = (m_1 + m_2) pm (r_1 + r_2)\nbma - bmb = (m_1 - m_2) pm (r_1 + r_2)\n\nFor multiplication and division it gets slightly more complicated. In general we do not have that the midpoint of bma cdot bmb is given by m_1m_2. If we still take m = m_1m_2 as the midpoint for the product then we need to find r such that bma cdot bmb subseteq m pm r. If t_1 t_2 in -1 1 then\n\nm_1m_2 - (m_1 + t_1r_1)(m_2 + t_2r_2)\n= m_1t_2r_2 + m_2t_1r_1 + t_1t_2r_1r_2\nleq m_1r_2 + m_2r_1 + r_1r_2\n\nThis means that\n\nbma cdot bmb subseteq m_1m_2 pm (m_1r_2 + m_2r_1 + r_1r_2)\n\nFor division, one similarly has (assuming r_2  m_2 to avoid division by zero)\n\nleftfracm_1m_2 - fracm_1 + t_1r_1m_2 + t_2r_2right\nleq leftfracm_1t_2r_2 - m_2t_1r_1m_2(m_2 + t_2r_2)right\nleq fracm_1r_2 + m_2r_1m_2(m_2 - r_2)\n\nhence\n\nbma  bmb subseteq leftfracm_1m_2 pm fracm_1r_2 + m_2r_1m_2(m_2 - r_2)right\n\nFor floating point numbers you have to make sure that the radius is rounded upwards and you also need to add the rounding error made in the computation of the midpoint.\n\nIt might seem like the ball representation is just more complicated than the version with lower and upper bounds. It does however have an important technical benefit: when working in high precision you can still use a low precision representation for the radius. For high precision it is therefore up to a factor 2 faster than working with the lower and upper bounds separately.\n\nThe ball version naturally lends itself to treating the midpoint as your approximation and the radius as a small error that can be treated perturbatively. This naturally leads us to the next aspect to consider.","category":"section"},{"location":"week-6-lecture-2/#Wide-intervals-or-thin-intervals","page":"Week 6 Lecture 2: Interval Arithmetic","title":"Wide intervals or thin intervals","text":"One of the things that I believe makes the biggest difference to how you should conceptually approach interval arithmetic is whether you are working with wide intervals or thin intervals. The methods and algorithms you want to use often heavily depend on this.\n\nIn general, whether an interval is considered wide or not depends on the ratio between the midpoint and the radius. If the radius is a factor 10^-10 smaller one could consider it a thin interval, if the radius is not more than a factor 10^-2 smaller then it could be considered wide. In between these two factors you get a hybrid region where both viewpoints are useful. These numbers of course depend on the context.\n\nFor thin intervals it is usually useful to take the ball arithmetic approach, where you think of the midpoint as your approximation and the radius as a small error. Since the radius is small you can use perturbative methods for bounds, which is usually simpler.\n\nFor wide intervals the lower and upper bound version is often more useful. Perturbative error bounds usually give very bad results in this case. Often, you want to rely on monotonicity to be able to compute accurate enclosures.","category":"section"},{"location":"#Topics-course-in-computer-assisted-proofs-Spring-2026","page":"Overview","title":"Topics course in computer-assisted proofs Spring 2026","text":"This website, together with the associated repository, contains the material for a topics course in computer-assisted proofs and rigorous numerics given at the University of Minnesota Spring 2026.\n\nnote: Note\nThis material is primarily meant to serve as lecture notes for the course. It is made public with the hope that it could potentially be useful for others as well. The material is, however, intended to be used together with the lectures and labs in the course and could at times be difficult to follow by itself. It's written in a fairly informal style and some of the content is opinionated in nature.\n\nThe general idea of computer-assisted proofs (in analysis), is to build on the massive success of numerical methods in applied mathematics and other sciences and apply them also for mathematical proofs. Classical numerical methods are however not suitable for direct use in proofs, since they introduce errors (rounding and discretization errors). These errors hinder their use in proofs, which require fully rigorous arguments. The area of rigorous numerics tackles these issues by introducing methods to control the errors in a fully rigorous way, that allows for the results to be used in proofs.\n\nAn early, and by now classical, example of a computer-assisted proof is the proof of the existence of the Lorenz attractor in 1999 by Tucker. Over the more than two decades since Tucker's proof, there has been an increase in the adoption of computer assisted proofs in analysis. An example of a recent breakthrough result building on computer-assisted proofs is the proof of blowup for the 3D Euler equation.\n\nThe course is split into 3 parts distributed over 15 weeks. The 3 parts are\n\nIntroduction to computer-assisted proofs (≈ Week 1-4)\nIntroduction to rigorous numerics (≈ Week 5-10)\nComputer-assisted proofs in practice (≈ Week 11-15)\n\nA rough schedule for the first two parts is given below, the precise details for the third part are yet to be determined.\n\nWeek Topic\n1 Introduction to computer-assisted proofs\n2 Discrete problems\n3 Continuous problems\n4 Formal proofs\n5 Floating points and interval arithmetic\n6 Floating points and interval arithmetic\n7 Basic rigorous numerics\n8 Automatic differentiation\n9 Improved rigorous numerics\n10 Improved rigorous numerics\n11 TBD\n12 TBD\n13 TBD\n14 TBD\n15 TBD","category":"section"},{"location":"#Part-1:-Introduction-to-computer-assisted-proofs","page":"Overview","title":"Part 1: Introduction to computer-assisted proofs","text":"The first part of the course will be a general introduction to computer-assisted proofs, from the point of view of rigorous numerics. We will consider both discrete problems, such as the proof of the Four color theorem, and continuous problems, such as the proof of existence of the Lorenz attractor.\n\nComputer-assisted proofs are often mixed up with formal proofs, such as those produced by Lean and Rocq (previously known as Coq), indeed some authors use the term computer-assisted proofs to refer to either. We will take a brief look at formal proofs, with a focus on the differences and similarities between formal proofs and computer-assisted proofs.","category":"section"},{"location":"#Part-2:-Introduction-to-rigorous-numerics","page":"Overview","title":"Part 2: Introduction to rigorous numerics","text":"The second part focuses on the machinery required for constructing computer-assisted proofs in analysis, known as rigorous numerics. This introduction will be partially based on the book Validated Numerics by Warwick Tucker. At the heart of rigorous numerics lies interval arithmetic, indeed the field of rigorous numerics is sometimes just referred to as interval arithmetic.\n\nThe practical parts will primarily be done in Julia, using the packages Arblib.jl and IntervalArithmetic.jl as the base for the interval arithmetic.\n\nHere is an example of how interval arithmetic looks like in practice, here using IntervalArithmetic.jl.\n\nusing IntervalArithmetic\na = interval(1, 2) # The interval [1, 2]\na^2 # Interval gotten from squaring all numbers in [1, 2]\nsin(a) # Interval gotten from applying sin to all numbers in [1, 2]\n\nThe content we will cover includes:\n\nMathematical foundations of floating point arithmetic: floating point formats, rounding\nBasics of interval arithmetic: basic arithmetic, elementary functions, special functions\nBasic rigorous numerics: isolating roots, computing integrals, enclosing extrema\nAutomatic differentiation: forward (and backwards) differentiation, Taylor arithmetic\nImproved rigorous numerics: isolating roots, computing integrals, enclosing extrema","category":"section"},{"location":"#Part-3:-Computer-assisted-proofs-in-practice","page":"Overview","title":"Part 3: Computer-assisted proofs in practice","text":"In the third and final part of the course we will look at what it takes to go from what we have learned about interval arithmetic and rigorous numerics to actually creating a computer-assisted proof. The list of topics covered would depend on the interests of the participants in the course. Possible topics would include the use of computer-assisted proofs in:\n\nSpectral geometry\nDynamical systems\nFluid mechanics\nAnalytic combinatorics\n\nOne could also dive deeper into different algorithms for computer-assisted proofs:\n\nRigorous integration of ODEs\nFinite element methods\nSpectral methods\nPhysics-Informed Neural Networks (PINNs)\n\nAlternatively, one can study the lower level details of interval arithmetic, more related to the field of computer algebra.","category":"section"},{"location":"week-15-lecture-1/#Week-15-Lecture-1:-TODO","page":"Week 15 Lecture 1: TODO","title":"Week 15 Lecture 1: TODO","text":"","category":"section"},{"location":"week-7-lab/#Week-7-Lab:-TODO","page":"Week 7 Lab: TODO","title":"Week 7 Lab: TODO","text":"","category":"section"},{"location":"week-8-lecture-2/#Week-8-Lecture-2:-TODO","page":"Week 8 Lecture 2: TODO","title":"Week 8 Lecture 2: TODO","text":"","category":"section"},{"location":"week-2-lecture-2/#Week-2-Lecture-2:-Integer-arithmetic","page":"Week 2 Lecture 2: Integer arithmetic","title":"Week 2 Lecture 2: Integer arithmetic","text":"In this lecture we will focus not on any specific computer-assisted proof, but rather on one of the fundamental building blocks for computer-assisted proofs, integer arithmetic. The lecture will also serve as a bit of introduction to parts of the Julia programming language. It could be useful to open a Julia REPL on the side while reading this and experiment with some of the code shown.\n\nTo make use of computers for proofs we need to be able to trust the computations they do. For numerical analysis, which relies on floating point numbers and approximations, trusting the computer requires a bit of work. This is what we will get to later in the course when we talk about rigorous numerics. For integer arithmetic most people would however probably agree that the computer can be trusted to do it right.\n\nWhen we in a couple of weeks get to rigorous numerics and more closely study floating points we will see that everything in the end reduces to integer arithmetic. So it makes sense to look a bit closer at this and understand how it works.","category":"section"},{"location":"week-2-lecture-2/#Integer-types","page":"Week 2 Lecture 2: Integer arithmetic","title":"Integer types","text":"There are many different ways to represent integers on the computer, the most important aspects in the choice of representation being the size of the integers and if one needs negative integers or not. For our purposes we will mostly work with integer types that do allow negative numbers, called signed integers. We will briefly talk about unsigned integers at the end of this section.\n\nFor the size one could either have a fixed predetermined size that can represent integers up to some fixed bound, or a variable size that can represent arbitrarily large integers (until your computer runs out of memory). Julia has the types Int8, Int16, Int32, Int64 and Int128 for representing fixed-width integers, where the number indicates the number of bits used to store the integer. The minimum and maximum integer that is representable using these types can be found using typemin and typemax respectively.\n\ntypemin(Int8)\ntypemax(Int8)\n\ntypemin(Int16), typemax(Int16)\n\ntypemin(Int32), typemax(Int32)\n\ntypemin(Int64), typemax(Int64)\n\ntypemin(Int128), typemax(Int128)\n\nThe default integer type on most modern computers is Int64, the Int type in Julia is a shorthand for the default type. If you directly write an integer it will be of type Int.\n\nInt\ntypeof(5)\n\nFor representing arbitrary-sized integers Julia has the type BigInt. In this case there is no typemin or typemax defined\n\ntypemin(BigInt)\ntypemax(BigInt)\n\nYou can convert an integer from one type to any other type. If the integer is too large to be represented in the new type an error is thrown.\n\nInt16(5)\ntypeof(Int16(5))\n\nBigInt(1000)\nbig(1000) # Shorthand for BigInt(1000) in this case\n\nInt8(300) # 300 doesn't fit in one Int8\n\nYou can see exactly what bits are used to represent an integer using bitstring\n\nbitstring(7)\nbitstring(2^14)\nbitstring(Int32(2^14))\nbitstring(Int16(2^14))\nbitstring(Int8(2^14)) # Too small to fit 2^14\nbitstring(BigInt(2^14)) # bitstring doesn't work for BigInt\n\nFor fixed-width integers negative values are represented using two's complement, whereas arbitrary-sized integers usually have a separate bit that keeps track of the sign. The details here are not so important for our purposes though.\n\nbitstring(-7)\nbitstring(Int32(-7))\nbitstring(Int16(-7))","category":"section"},{"location":"week-2-lecture-2/#Arithmetic","page":"Week 2 Lecture 2: Integer arithmetic","title":"Arithmetic","text":"How arithmetic of integers is done depends on their type.\n\nFor Int32 and Int64 basic arithmetic is typically implemented directly in hardware on the CPU. So you don't write a program for multiplying two such integers, you use the implementation on the CPU. The correctness of this procedure is therefore dependent on the correctness of the CPU, which for even remotely modern CPU you can assume.\n\nFor smaller integer types, e.g. Int8 and Int16, arithmetic is often times handled by converting them to Int32 or Int64, doing the operation and then converting back. One would in general not expect these operations to be any faster than those for Int32 and Int64. Some hardware might have specialized instructions for these, in which case it could be much faster though.\n\nFor BigInt the arithmetic is implemented in software. Internally they are built up of a list of Int64 that is treated as one large integer. Operations are then implemented by combining several operations for Int64 values. For addition this is relatively simple and something you could implement yourself with a bit of time. For multiplication this becomes extremely complicated and requires both highly sophisticated mathematical methods and carefully crafted implementations to achieve top performance. The BigInt type in Julia is internally based on the GMP library which contains highly specialized code for operating on such integers.","category":"section"},{"location":"week-2-lecture-2/#Overflow","page":"Week 2 Lecture 2: Integer arithmetic","title":"Overflow","text":"For fixed-width integer types we have to somehow handle when the value is too large to be represented by the type. For conversion to integer types we have already seen that it throws an error if the value doesn't fit.\n\nInt8(300) # 300 doesn't fit in one Int8\n\nWhen doing arithmetic on integers it does however not throw an error, instead the result wraps around.\n\ntypemin(Int8), typemax(Int8) # Recall these values\ntypemax(Int8) + Int8(1)\n\ntypemin(Int64), typemax(Int64) # Recall these values\ntypemax(Int64) + 1\n\nWhen this happens it is called integer overflow. The behavior here does however depend on the programming language used. For example in C, integer overflow is considered undefined behavior and a program which exhibits overflow is not guaranteed to work as expected. In Julia, and many other languages, the behavior is well defined. For Int64 the behavior is isomorphic to mathbbZ_2^64, with the representatives centered around zero.\n\nThis overflow behavior means that you have to be careful when working with integers for mathematical purposes. For fixed-width integer arithmetic to faithfully represent integer arithmetic you have to ensure (prove) that your operations never overflow. In many cases overflow is not a problem. For example, computing Pythagorean triples up to 1784 using Int64 will clearly not give you any issues with overflow. But if you want to verify Goldbach's weak conjecture up to 10^30 approx 2^100 you won't be able to do it using Int64. If you want to be on the safe side you can always use BigInt, which never overflows (it will just crash if you run out of memory). In some programming languages, e.g. Python, the default is that integers are represented using an arbitrary-sized representation.\n\nWith these issues coming from overflow, why would one not simply use BigInt all the time? The answer is performance, it is significantly slower.","category":"section"},{"location":"week-2-lecture-2/#Performance","page":"Week 2 Lecture 2: Integer arithmetic","title":"Performance","text":"Let us take a brief look how performance for various integer types compare. Let us consider the problem of computing\n\nsum_n = 1^N n^b\n\nfor some integers N and b. In Julia this could be implemented as\n\nfunction f(N::Integer, b::Integer)\n    # Writing just \"0\" would give us an Int64, zero(N) gives us a zero of the same type as N\n    S = zero(N)\n    # Same with one(N) here\n    for n in one(N):N\n        S += n^b\n    end\n    return S\nend\n\nLet us take b = 2, we can evaluate the function using a variety of types\n\nf(1000, 2)\nf(Int32(1000), Int32(2))\nf(Int16(1000), Int16(2)) # This overflows!\nf(BigInt(1000), BigInt(2))\n\nTo benchmark these different versions we use the Julia package BenchmarkTools.jl.\n\nusing BenchmarkTools\n@benchmark f($1000, $2) samples = 10000 evals = 10\n@benchmark f(Int32($1000), Int32($2)) samples = 10000 evals = 10\n@benchmark f(Int16($1000), Int16($2)) samples = 10000 evals = 10\n@benchmark f(BigInt($1000), BigInt($2)) samples = 5000 evals = 1\n\nnote: Note\nThe $-signs in the code below are part of the BenchmarkTools interface and are there to avoid the compiler being too clever and optimizing away what we want to measure.The extra arguments samples and evals are not required. If you run the code yourself you can remove them. They are here to reduce the time to build this documentation.\n\nThe most important number in the above benchmarks is the minimum time. The mean time is also important in practice, but is affected by variables we are not controlling for here.","category":"section"},{"location":"week-2-lecture-2/#Flint","page":"Week 2 Lecture 2: Integer arithmetic","title":"Flint","text":"While we are discussing integer arithmetic it is also natural to introduce the library that will be the foundation for a lot of the rigorous numerics we will get to later in the course. The FLINT library is a C library with high performance implementations of many computer algebra algorithms. FLINT stands for \"Fast Library for Number Theory\", but much of the functionality is useful outside of number theory as well.\n\nWe will not make use of Flint directly, instead we will use it through the Julia package Arblib.jl that wraps (most of) the parts of the library related to rigorous numerics. In fact, there is not really any need for you to know about the Flint library at all for what we will do in the course.\n\nnote: Note\nThe reason for the library being called Flint but the Julia package being called Arblib is that the parts of Flint that Arblib wraps were previously a separate library called Arb (Arbitrary precision Real Balls). The Arb library, and several others, were merged into Flint in 2023.\n\nThe Flint library implements many standard computer algebra algorithms over a large variety of different rings. Some of the rings it implements are\n\nIntegers\nRational numbers\nIntegers mod n\nReal and complex numbers (this is the rigorous numerics part)\nExact real and complex numbers (this is more symbolical in nature)\nFinite fields\np-adic numbers\n\nFor our purposes we will primarily deal with the real and complex numbers, though these internally depend on the integers and rational numbers. For these rings it implements a number of different algorithms related to e.g. polynomials, matrices and special functions. Unless specified otherwise, the Flint library can be assumed to always return mathematically rigorous results.\n\nIn many areas the Flint implementations are the state of the art and sometimes greatly outperform other implementations. It's used by many programs for computationally heavy computations, for example Sage uses it internally for many things.","category":"section"},{"location":"week-10-lecture-2/#Week-10-Lecture-2:-TODO","page":"Week 10 Lecture 2: TODO","title":"Week 10 Lecture 2: TODO","text":"","category":"section"},{"location":"week-11-lab/#Week-11-Lab:-TODO","page":"Week 11 Lab: TODO","title":"Week 11 Lab: TODO","text":"","category":"section"},{"location":"week-6-lab/#Week-6-Lab:-IntervalArithmetic.jl-and-Arblib.jl","page":"Week 6 Lab: IntervalArithmetic.jl and Arblib.jl","title":"Week 6 Lab: IntervalArithmetic.jl and Arblib.jl","text":"In this lab we will look at the two Julia packages IntervalArithmetic.jl and Arblib.jl. IntervalArithmetic.jl implements \"regular\" interval arithmetic, where the intervals are represented by their lower and upper bounds. Arblib.jl implements ball arithmetic, where the intervals are represented by a midpoint and a radius. We will make use of both of these packages throughout the course, though likely Arblib.jl to a larger extent (mostly because I'm more used to that package).\n\nFor the first part of the lab (intro to IntervalArithmetic.jl and Arblib.jl) we will use the Julia REPL and the instructions below. For the second part (computing sin) we will use the lab-6.jl Pluto notebook that you can find in the notebooks directory. The instructions for the second part are also included below, but we will use the ones in the notebook.","category":"section"},{"location":"week-6-lab/#Intro-to-IntervalArithmetic.jl","page":"Week 6 Lab: IntervalArithmetic.jl and Arblib.jl","title":"Intro to IntervalArithmetic.jl","text":"Let us start by taking a close look at IntervalArithmetic.jl.","category":"section"},{"location":"week-6-lab/#Construction","page":"Week 6 Lab: IntervalArithmetic.jl and Arblib.jl","title":"Construction","text":"Intervals are constructed using the interval function. With interval(a, b) we can construct the interval a b and with interval(a) we get the thin interval a a.\n\nusing IntervalArithmetic\n\ninterval(1, 2) # We will get back to what _com means\n\ninterval(0.1, 0.2)\n\ninterval(2)\n\nBy default it will create intervals where the endpoints are of type Float64.\n\ntypeof(interval(1))\n\nWe can create intervals with endpoints of different types by giving the type as first argument.\n\ninterval(BigFloat, 1, 2) # Note the ₂₅₆ in the output, this is the BigFloat precision\n\ntypeof(interval(BigFloat, 1, 2))\n\ninterval(Rational{Int}, 1, 2)\n\ntypeof(interval(Rational{Int}, 1, 2))\n\nIn practice we will mostly use Float64 and sometimes BigFloat. Having endpoints which are rational numbers can sometimes be useful, but we won't see it too much.\n\nFinally, you can create an interval from a string with its decimal representation using parse. This avoid issues with rounding of floating points and guarantees that the given number is contained in the interval.\n\nparse(Interval{Float64}, \"0.1\") # This printing of this is weird, see next section\n\nparse(Interval{Float64}, \"[0.1, 0.2]\")","category":"section"},{"location":"week-6-lab/#Printing","page":"Week 6 Lab: IntervalArithmetic.jl and Arblib.jl","title":"Printing","text":"There are a couple of different options for how intervals are printed. The default can be set with setdisplay, see its documentation for more details. The default is setdisplay(:infsup).\n\nBy default it only rounds to 6 significant digits, with no special handling of the different endpoints. This means that if the endpoints share the first 6 significant digits then they print the same.\n\nusing IntervalArithmetic # hide\nsetdisplay(:infsup)\ninterval(π)\ninterval(1, 1 + 1e-10)\ninterval(Float64, 1 // 3) # Just interval(1 // 3) gives us a Rational one\n\nAs you can see in the above examples it also prints _com after the interval. This is a decoration. They keep track of extra information regarding the functions used to compute the interval. We won't care about these for now, but might come back to them later in the course.\n\nTo print everything you can use setdisplay(:full).\n\nusing IntervalArithmetic # hide\nsetdisplay(:full)\ninterval(1, 2)\ninterval(π)\ninterval(1, 1 + 1e-10)\ninterval(Float64, 1 // 3)\n\nIn this case the full endpoints are printed. It should however be noted that the printing doesn't take into account rounding. For example the printing of the following interval makes it look like it would contain 01 = 1  10.\n\nusing IntervalArithmetic # hide\nsetdisplay(:full)\ninterval(0.1, 0.2)\n\nBut this is an artefact of 01 not being exactly representable in Float64. We can print more digits by creating an interval of type BigFloat, and then we can see that the interval does in fact not contain 01 (it does contain 02 though).\n\nusing IntervalArithmetic # hide\nsetdisplay(:full)\ninterval(BigFloat, 0.1, 0.2)\n\nYou should be careful with reading information of intervals from their printed representation. It is usually better to use a predicate to check something directly\n\nusing IntervalArithmetic # hide\nin_interval(1 // 10, interval(0.1, 0.2)) # 1 // 10 is exactly 0.1","category":"section"},{"location":"week-6-lab/#Useful-tools","page":"Week 6 Lab: IntervalArithmetic.jl and Arblib.jl","title":"Useful tools","text":"You can get the lower and upper bounds of the interval using inf and sup.\n\nusing IntervalArithmetic # hide\nx = interval(1, 2)\ninf(x)\nsup(x)\n\nYou can get the midpoint and radius using mid and radius, or both using midradius. You can also get the diameter with diam.\n\nusing IntervalArithmetic # hide\nx = interval(1, 2)\nmid(x)\nradius(x)\nmidradius(x)\ndiam(x)\n\nNote that these functions return floating points and that they therefore round. For radius and diam the rounding is outwards, so they should always be at least as large as the true value. For the midpoint the rounding is to nearest. Since they round you should however be a bit careful with using these.\n\nusing IntervalArithmetic # hide\nx = interval(1e-100, 1)\nmid(x)\nradius(x)\nmidradius(x)\ndiam(x)","category":"section"},{"location":"week-6-lab/#Intro-to-Arblib.jl","page":"Week 6 Lab: IntervalArithmetic.jl and Arblib.jl","title":"Intro to Arblib.jl","text":"","category":"section"},{"location":"week-6-lab/#Construction-2","page":"Week 6 Lab: IntervalArithmetic.jl and Arblib.jl","title":"Construction","text":"For Arblib.jl balls are constructed using the Arb constructor. With Arb(x) we get a ball enclosing the value of x, with Arb((a, b)) we get a ball enclosing the interval a b.\n\nusing Arblib\nArb(1)\nArb(1 // 3)\nArb(π)\nArb(\"0.1\")\nArb(\"[0.1 +/- 1e-10]\")\n\nArb((1, 2)) # Note that this prints like [+/- 2.01], we'll get back to this\nArb((-1, 1))\nArb((1 // 3, π))\n\nAn alternative, slightly lower level, constructor is setball. We have that setball(Arb, m, r) creates a ball with midpoint m and radius r.\n\nusing Arblib # hide\nsetball(Arb, 0, 1)\nsetball(Arb, 1, 1e-10)\n\nWith setball the midpoint m is first rounded to a floating point. This for example means that setball(Arb, 1 // 3, 0) will not actually contain the number 1 / 3.\n\nusing Arblib # hide\nx = setball(Arb, 1 // 3, 0)\nradius(x)\n\nIf rounding for the midpoint needs to be taken into account add_error is more useful, add_error(x::Arb, err) will return the ball x with err added to its radius.\n\nusing Arblib # hide\nx = Arb(1 // 3)\ny = add_error(x, Arb(\"1e-50\"))","category":"section"},{"location":"week-6-lab/#Printing-2","page":"Week 6 Lab: IntervalArithmetic.jl and Arblib.jl","title":"Printing","text":"Similar to for IntervalArithmetic.jl there are a couple of options when printing Arb values. For Arblib.jl there is however no global setting, instead the string function is used with different arguments. See the documentation of string for all the options.\n\nArb values are printed on the form [m +/- r] and by default the printed value is guaranteed to enclose the true interval. The output for the midpoint is rounded so that the value is correct up to 1 ulp (unit in the last decimal place). The following example from the documentation of string shows that this behavior in some cases can be slightly confusing\n\nusing Arblib # hide\nx = Arb((1, 2))\nstring(x)\nstring(x, more = true)\nstring(x, digits = 5, more = true)\n\nCompared to IntervalArithmetic.jl you can hence trust that the true value is contained in the printed output. As the above example shows there are however often large overestimations in the printed value, in particular when no or very few significant digits of the output can be determined.","category":"section"},{"location":"week-6-lab/#Useful-tools-2","page":"Week 6 Lab: IntervalArithmetic.jl and Arblib.jl","title":"Useful tools","text":"You can get the midpoint and radius of a ball using midpoint and radius or getball.\n\nusing Arblib # hide\nx = Arb(1 // 3)\nmidpoint(x)\nradius(x)\ngetball(x)\n\nBy default the midpoint and radius are returned as floating points (Arf for the midpoint, Mag for the radius). You can get them as Arb values (with radius zero)\n\nusing Arblib # hide\nx = Arb(1 // 3)\nmidpoint(Arb, x)\nradius(Arb, x)\ngetball(Arb, x)\n\nThis is often more useful if you want to later do calculations with them.\n\nYou can get lower and upper bounds with lbound, ubound and getinterval.\n\nusing Arblib # hide\nx = Arb(1 // 3)\nlbound(x)\nubound(x)\ngetinterval(x)\n\nBy default these are returned as floating points, you can get them as Arb values (with radius zero)\n\nusing Arblib # hide\nx = Arb(1 // 3)\nlbound(Arb, x)\nubound(Arb, x)\ngetinterval(Arb, x)\n\nNote that these values are not the exact lower and upper bounds of the interval, they are floating point values given by rounding the true values outwards.\n\nThe Arblib.rel_accuracy_bits function can be used to get the relative accuracy of a ball.\n\nusing Arblib # hide\nx = Arb(1 // 3)\nArblib.rel_accuracy_bits(x)\ny = add_error(x, Arb(1e-16))\nArblib.rel_accuracy_bits(y)","category":"section"},{"location":"week-6-lab/#Computing-\\sin","page":"Week 6 Lab: IntervalArithmetic.jl and Arblib.jl","title":"Computing sin","text":"We are now ready to compute sin using both IntervalArithmetic.jl and Arblib.jl!\n\nAs before we will use a Taylor expansion around zero. Compared to before we will however not use a fixed degree for the expansion, but allow it to be easily changed. Recall that\n\nsin(x) = sum_n = 0^infty (-1)^nfracx^2n + 1(2n + 1)\n\nSince the series is alternating the error we get if we only sum up to term N is bounded by term N + 1, i.e.\n\nleftsin(x) - sum_n = 0^N (-1)^nfracx^2n + 1(2n + 1)right leq fracx^2N + 3(2N + 3)\n\nThis bound can be improved, but it is good enough for our purposes. Note that we don't put any restrictions on the value of x here. For large values of x the error bound will of course be very large, but it is still valid.\n\nHere is an implementation of this approach for Float64. Note that it doesn't actually do anything with the computed error bound.\n\nfunction my_sin(x::Float64; N::Integer = 6)\n    y = 0.0\n    for n in 0:N\n        y += (-1)^n * x^(2n + 1) / factorial(2n + 1)\n    end\n    # For Float64 we can't actually do anything with this value...\n    err = abs(x)^(2N + 3) / factorial(2N + 3)\n    return y\nend\n\nLet us check that it seems to work\n\nmy_sin(1.0)\nsin(1.0)\nmy_sin(1.0) ≈ sin(1.0)\n\nmy_sin(10.0) # For large values of x we would need more terms\nsin(10.0)\n\nLet us now implement Interval{Float64} and Arb versions of this function! Here we do want to correctly handle the error bounds as well.\n\nfunction my_sin(x::Interval{Float64}; N::Integer = 6)\n    # TASK: Implement this\nend\n\nfunction my_sin(x::Arb; N::Integer = 6)\n    # TASK: Implement this\nend\n\ndetails: Solution\nusing Arblib, IntervalArithmetic # hide\n\nfunction my_sin(x::Interval{Float64}; N::Integer = 6)\n    y = zero(x)\n    for n in 0:N\n        y += (-1)^n * x^(2n + 1) / factorial(2n + 1)\n    end\n    err = abs(x)^(2N + 3) / factorial(2N + 3)\n    return y + interval(-sup(err), sup(err))\nend\n\nfunction my_sin(x::Arb; N::Integer = 6)\n    y = zero(x)\n    for n in 0:N\n        y += (-1)^n * x^(2n + 1) / factorial(2n + 1)\n    end\n    err = abs(x)^(2N + 3) / factorial(2N + 3)\n    return add_error(y, err)\nend\n\nsetdisplay(:full) # To see the full output\nmy_sin(interval(1))\nsin(interval(1))\nissubset_interval(sin(interval(1)), my_sin(interval(1)))\n\nmy_sin(Arb(1))\nsin(Arb(1))\nArblib.contains(my_sin(Arb(1)), sin(Arb(1)))\n\nmy_sin(interval(10)) # For large values of x we get enormous error bounds\nmy_sin(Arb(10))\n\nWith these two implementations, some of the things we can look at are:\n\nHow does the radius of the Interval and Arb versions compare? For example for x = 1.\nWhat happens if you increase N? Hint: At some point you will get an error and need to adjust the code.\nWhat happens for wide input? x = 0999 1001 09 11 0 1 0 2? Or even larger!\nCan we do better for wide input? There are two reasonable approaches, one better for Interval and one for Arb.\nFor large values of x you first want to reduce the argument to a smaller value using the periodicity. How could this be done?","category":"section"},{"location":"week-5-lecture-2/#Week-5-Lecture-2:-Floating-point-numbers","page":"Week 5 Lecture 2: Floating point numbers","title":"Week 5 Lecture 2: Floating point numbers","text":"In this lecture, we will continue our study of floating point numbers. In particular we will talk about the evaluation of elementary functions and how this interacts with rounding.","category":"section"},{"location":"week-5-lecture-2/#Issues-with-floating-point-arithmetic","page":"Week 5 Lecture 2: Floating point numbers","title":"Issues with floating point arithmetic","text":"Already in the last lecture we started talking about some of the issues that arise from floating point arithmetic not satisfying the usual properties we expect from arithmetic.\n\nMost of these issues come from the operations not being associative. For example (x + y) + z and x + (y + z) are in general different floating point numbers. This non-associativity can make floating point arithmetic behave in a non-deterministic way. Each individual operation is of course deterministic; it computes the exact value of bigcirc(x + y), but in many cases the order these operations are performed is not always clear and in some cases not even deterministic.\n\nnote: Example: `@fastmath` and summation order\nLet us consider the problem of computing a sumS = sum_n = 1^N a_nin floating point arithmetic. We could implement this in Julia asfunction strict_sum(as)\n    S = zero(eltype(as))\n    for a in as\n        S += a # Strictly sequential addition\n    end\n    return S\nendThis will perform the sum from left to right. If we take a_n = 1  10 (or rather a_n = square(1  10)) and N = 10^6, this gives usas = fill(1 / 10, 10^6);\nstrict_sum(as)Let us next implement the same sum but put @fastmath before it, i.e.function fast_sum(as)\n    S = zero(eltype(as))\n    @fastmath for a in as\n        S += a # Compiler is allowed to reorder these additions\n    end\n    return S\nendIn this case we getfast_sum(as)which is slightly different from before! What happens here is that @fastmath tells the compiler that it doesn't have to follow the usual associativity rules for floating point arithmetic. It is free to reorder the operations if this will make the code faster. The exact result you get here will depend on your processor, as different orderings are faster depending on the precise architecture of the CPU. We can see that the performance differs quite a lot here!using BenchmarkTools\n@benchmark strict_sum($as) samples = 1000 evals = 1\n@benchmark fast_sum($as) samples = 1000 evals = 1\n\nAs the above example shows we can get slightly different results by changing the implementation, even if we mathematically are computing the same thing. Some of the things that can affect the results are:\n\nYour code\nThe code in the libraries that you are using\nThe compiler you are using\nThe processor you are using\nThe number of threads you are using\n\nThe most common reason for changes in the result is that operations are reordered and the non-associativity hence means that we get a different result.","category":"section"},{"location":"week-5-lecture-2/#Correctly-rounded-functions","page":"Week 5 Lecture 2: Floating point numbers","title":"Correctly rounded functions","text":"Consider a function f mathbbR to mathbbR. Given a rounding mode bigcirc we define bigcirc(f)  mathbbF_p to mathbbF_p as\n\nbigcirc(f)(x) = bigcirc(f(x))\n\nWe say that a floating point implementation of a function f is correctly rounded (according to the specified rounding mode) if it computes bigcirc(f).\n\nThis definition naturally generalizes to multivariable functions f mathbbR^n to mathbbR. The standard floating point arithmetic operations are examples of correctly rounded two variable functions.\n\nThe three variable function operatornamefma (fused-multiply-add) is commonly used for floating points. It implements a correctly rounded version of x cdot y + z. Evaluating this using standard floating point arithmetic would give you bigcirc(bigcirc(x cdot y) + z). The operatornamefma function instead gives you operatornamefma(x y z) = bigcirc(x cdot y + z), it avoids rounding the intermediate result x cdot y. Due to hardware support this operation can be both faster than performing the multiplication and addition separately and at the same time give better accuracy.","category":"section"},{"location":"week-5-lecture-2/#Elementary-functions","page":"Week 5 Lecture 2: Floating point numbers","title":"Elementary functions","text":"The floating point functions we have looked at so far, + - cdot  and operatornamefma, are primitive floating point functions in the sense that they are either implemented directly in hardware or computed directly through the x cdot 2^y representation of floating points. The square root function and the reciprocal square root function (1  sqrtx) are two other examples of functions which are usually implemented directly in hardware, or where the implementation uses the x cdot 2^y representation directly. Most other floating point functions are themselves implemented using floating point functions.\n\nIdeally we want a floating point implementation to be correctly rounded, i.e. for a function f and a floating point number x we want it to return bigcirc(f(x)). This, however, turns out to be too much to ask in many cases. The vast majority of functions implemented in floating points are not correctly rounded. We will look at\n\nHow Julia computes sin\nLibraries for correctly rounded elementary functions\nWhy correct rounding doesn't matter too much","category":"section"},{"location":"week-5-lecture-2/#Evaluating-\\sin","page":"Week 5 Lecture 2: Floating point numbers","title":"Evaluating sin","text":"How does Julia compute sin(x)? Finding out is surprisingly simple and the code is relatively easy to understand. You can find the code implementing sin by running @less sin(0.0). This shows us the following function.\n\nfunction sin(x::T) where T<:Union{Float32, Float64}\n    absx = abs(x)\n    if absx < T(pi)/4 #|x| ~<= pi/4, no need for reduction\n        if absx < sqrt(eps(T))\n            return x\n        end\n        return sin_kernel(x)\n    elseif isnan(x)\n        return x\n    elseif isinf(x)\n        sin_domain_error(x)\n    end\n    n, y = rem_pio2_kernel(x)\n    n = n&3\n    if n == 0\n        return sin_kernel(y)\n    elseif n == 1\n        return cos_kernel(y)\n    elseif n == 2\n        return -sin_kernel(y)\n    else\n        return -cos_kernel(y)\n    end\nend\n\nIt checks some special cases, very small x (less than sqrt(eps())), NaN values and infinity values. Otherwise it does an argument reduction, reducing it to a value y in the range -pi4 pi4 and information about which quadrant we are in. Depending on the quadrant it computes either sin_kernel(y) or cos_kernel(y). Let us look closer at sin_kernel (cos_kernel is similar). The relevant code here is\n\n# Coefficients in 13th order polynomial approximation on [0; π/4]\n#     sin(x) ≈ x + S1*x³ + S2*x⁵ + S3*x⁷ + S4*x⁹ + S5*x¹¹ + S6*x¹³\n# D for double, S for sin, number is the order of x-1\nconst DS1 = -1.66666666666666324348e-01\nconst DS2 = 8.33333333332248946124e-03\nconst DS3 = -1.98412698298579493134e-04\nconst DS4 = 2.75573137070700676789e-06\nconst DS5 = -2.50507602534068634195e-08\nconst DS6 = 1.58969099521155010221e-10\n\n@inline function sin_kernel(y::Float64)\n    y² =  y*y\n    y⁴ =  y²*y²\n    r  =  @horner(y², DS2, DS3, DS4) + y²*y⁴*@horner(y², DS5, DS6)\n    y³ =  y²*y\n    y+y³*(DS1+y²*r)\nend\n\nTo approximate sin on -pi4 pi4 it uses a degree 13 polynomial which is almost, but not quite, the Taylor expansion at zero. The slight adjustments to the coefficients are to minimize the maximum error on the interval, rather than the error close to zero. For evaluating the polynomial it doesn't directly use the form y + S1*y³ + S2*y⁵ + S3*y⁷ + S4*y⁹ + S5*y¹¹ + S6*y¹³, instead it uses a Horner like scheme. The @horner calls here correspond to\n\n@horner(y², DS2, DS3, DS4) = DS2 + y² * (DS3 + y² * DS4)\n@horner(y², DS5, DS6) = DS5 + y² * DS6\n\nAdditionally, it uses fma for the computations. One can check that this corresponds to the polynomial we want to evaluate, but this choice of order reduces the rounding errors.\n\nThis implementation is not correctly rounded, though in the majority of cases it does return the correctly rounded result in practice. One example where it doesn't is\n\nx = 0.17931446656123207\ny1 = sin(x)\ny2 = Float64(sin(BigFloat(x))) # Compute in higher precision and then round\ny1 - y2","category":"section"},{"location":"week-5-lecture-2/#Libraries-for-correctly-rounded-functions","page":"Week 5 Lecture 2: Floating point numbers","title":"Libraries for correctly rounded functions","text":"Implementing correctly rounded functions is hard, even harder is implementing correctly rounded functions that are fast. One has to both control truncation errors coming from the finite expansions, as well as rounding errors when evaluating these expansions. In practice most libraries that implement correctly rounded functions internally make use of higher precision to be able to handle the rounding errors from evaluating the expansions. This then has to be combined with proofs for the truncation errors being sufficiently small. Two libraries that do this are\n\nThe GNU MPFR Library: This implements arbitrary precision floating point arithmetic with full control over rounding. The BigFloat type in Julia uses this library under the hood.\nCR-LIBM: This implements correctly rounded functions for Float64.","category":"section"},{"location":"week-5-lecture-2/#Why-this-doesn't-matter-too-much","page":"Week 5 Lecture 2: Floating point numbers","title":"Why this doesn't matter too much","text":"From a mathematical point of view, asking for correctly rounded elementary functions is natural. In practice, it is however not that important. Even if all the functions you are using are correctly rounded, you will still be combining the results from these functions and introduce rounding errors along the way.\n\nAt a fundamental level, the issue is that functions being correctly rounded \"doesn't compose\". If we have two functions f and g and manage to implement correctly rounded versions of these, i.e.bigcirc(f) and bigcirc(g), then computing bigcirc(f)(bigcirc(g)(x)) corresponds to bigcirc(f(bigcirc(g(x)))) which in general is not the same as bigcirc(f(g(x))).\n\nSolving this composition problem is one of the things that interval arithmetic will allow us to do. It will not allow us to get correctly rounded results, but it will allow us to get results where we have control over the error.","category":"section"},{"location":"week-10-lab/#Week-10-Lab:-TODO","page":"Week 10 Lab: TODO","title":"Week 10 Lab: TODO","text":"","category":"section"},{"location":"week-4-lecture-1/#Week-4-Lecture-1:-Formal-proofs","page":"Week 4 Lecture 1: Formal proofs","title":"Week 4 Lecture 1: Formal proofs","text":"This week we will take a slight detour in our study of computer-assisted proofs and talk about formal proofs. Computer-assisted proofs and formal proofs are related, in the sense that they both make use of the computer for verification. However, they are very different in what they aim to do and practical use of them looks very different.\n\nComputer-assisted proofs aim to tackle the problem of humans being bad (or at least slow) at performing large scale computations. Using pen and paper one reduces the problem to a computational problem, which is then handled by the computer.\n\nFormal proofs aim to tackle the problem of humans making errors in their logic. A formal proof covers the whole proof, from the statement of the theorem to every step in the proof.\n\nThe goal of this week is not to make you experts in formal proofs, but rather to give you enough of an idea of what a formal proof is to be able to compare it to computer-assisted proofs. We will look at the mathematical foundations for formal proofs, historical examples of formal proofs as well as recent developments in the field.\n\ninfo: Info\nAs mentioned in the first lecture, different mathematicians mean different things when they say \"computer-assisted proof\". Some mathematicians would include formal proofs in the general field of computer-assisted proofs.","category":"section"},{"location":"week-4-lecture-1/#Some-recent-developments-and-resources","page":"Week 4 Lecture 1: Formal proofs","title":"Some recent developments and resources","text":"The interest in formal proofs has seen a rise in recent years. This is likely driven by a combination of the tools for formal proofs maturing, making them more accessible, and the rise of generative AI giving hope for semi-automatic translation from pen and paper proofs to formal proofs.\n\nA recent example of the use of formal proofs which is also somewhat relevant to our study of computer-assisted proofs is the Integrated Explicit Analytic Number Theory network. They aim to formally verify effective inequalities with explicit constants that arise in analytic number theory. From their description you can read\n\nWith unspecified constants, such bounds are well understood and can be found in many textbooks (and have even been formalized in several proof assistant languages, including Lean). However, with explicit constants, the results are only contained in a few papers which are full of routine but tedious computations. Furthermore, while bounds with unspecified constants are quite robust with respect to minor typos in the arguments, explicit constant bounds can be extremely fragile, with a single arithmetic error causing all subsequent bounds to be untrustworthy.\n\nIn a Mathstodon post Terence Tao mentions that some of these computations make use of interval arithmetic for some of the internal calculations, for example for computing bounds for log 2. He also mentions in a project log book that some of the original papers contain minor errors in the constants coming from the use of non-rigorous floating point arithmetic.\n\nSome other resources:\n\nFor PDEs there was recently a [Lean for\n\nPDE](https://www.slmath.org/workshops/1180) workshop organized at SLMath.\n\nTerence Tao has some Youtube videos showing what formalizing a proof in Lean could look like in practice.","category":"section"},{"location":"week-4-lecture-1/#A-glimpse-of-Lean","page":"Week 4 Lecture 1: Formal proofs","title":"A glimpse of Lean","text":"Most (but not all) of the recent developments in formal proofs make use of Lean. To get a feeling for what we are dealing with, let us take a brief look at what statements written in Lean could look like.\n\nFor these examples we will make use of the Lean formalization of Analysis I, a formalization of the Analysis I textbook by Terence Tao. The emphasis in the book is on rigour and on foundations and starts with set theory and construction of the natural numbers.\n\nWe will look at some examples of statements from this book. To make use of Lean's functionality for interactively working with statements we will use VS Code to demo this. We do not include the details of the demo here, but the chapters of the book we will look at are\n\nSection 2.1: The Peano axioms (Lean source)\nSection 10.2: Local maxima, local minima, and derivatives (Lean source)\nSection 11.4: Basic properties of the Riemann integral (Lean source): Primarily Theorem 11.4.5.","category":"section"},{"location":"week-15-lab/#Week-15-Lab:-TODO","page":"Week 15 Lab: TODO","title":"Week 15 Lab: TODO","text":"","category":"section"},{"location":"week-15-lecture-2/#Week-15-Lecture-2:-TODO","page":"Week 15 Lecture 2: TODO","title":"Week 15 Lecture 2: TODO","text":"","category":"section"},{"location":"week-5-lecture-1/#Week-5-Lecture-1:-Floating-point-numbers","page":"Week 5 Lecture 1: Floating point numbers","title":"Week 5 Lecture 1: Floating point numbers","text":"This lecture marks the start of our study of rigorous numerics, which will continue for six weeks. On a high level, these weeks are divided into five parts.\n\nMathematical foundations of floating point arithmetic: floating point formats, rounding\nBasics of interval arithmetic: basic arithmetic, elementary functions, special functions\nBasic rigorous numerics: isolating roots, computing integrals, enclosing extrema\nAutomatic differentiation: forward (and backwards) differentiation, Taylor arithmetic\nImproved rigorous numerics: isolating roots, computing integrals, enclosing extrema\n\nWe will start with the basics, floating point numbers, and progressively work our way up to how rigorous numerics can be used to solve problems. At every stage we will make sure that we understand how the computer can perform these computations in a rigorous way.","category":"section"},{"location":"week-5-lecture-1/#Floating-point-numbers","page":"Week 5 Lecture 1: Floating point numbers","title":"Floating point numbers","text":"In rigorous numerics one usually works primarily with intervals and interval arithmetic, in fact the field of rigorous numerics is often referred to as just interval arithmetic. The underlying basis of interval arithmetic is however floating points. To fully understand how interval arithmetic works we therefore have to start by understanding how floating point numbers work. For this we will look at:\n\nThe mathematical definition of floating point numbers\nActual implementation of floating points\nArithmetic with floating point numbers\nRounding","category":"section"},{"location":"week-5-lecture-1/#Definition-of-floating-point-numbers","page":"Week 5 Lecture 1: Floating point numbers","title":"Definition of floating point numbers","text":"There are several different versions of floating point numbers, depending on the precise use case. Let us start with the most mathematical definition.\n\nnote: Floating point number\nA binary floating point number is a rational number of the form x cdot 2^y where x y in mathbbZ and x is odd, or one of the special values zero, plus infinity, negative infinity or NaN (not-a-number).\n\nWe use the notation mathbbF for the set of floating point numbers, i.e.\n\nmathbbF = x cdot 2^y x y in mathbbZ x textodd cup 0 infty -infty textNaN\n\nThere are a number of equivalent ways of representing a floating point. Apart from x cdot 2^y used above the two most common ones are\n\nm cdot 2^e with 1 leq m  2\nm cdot 2^e with 05 leq m  1\n\nThe first one is arguably the most common format and in this case m is called the mantissa and e the exponent. Depending on the context mantissa and exponent could also be used to refer to x and y or m and e.\n\nGenerally we consider floating points of a specified precision p. The set of floating point numbers of precision p is\n\nmathbbF_p = x cdot 2^y x y in mathbbZ 1 leq x  2^p x textodd cup 0 infty -infty textNaN\n\nIf we write the mantissa in binary it is then of the form\n\nm = pm 1b_1 b_2 dots b_p-1\n\nNote that we only need p - 1 bits to store the mantissa since the first digit is implicitly defined to be a one. In the x cdot 2^y representation we also only need p - 1 bits to store x since it is always odd and we therefore don't have to store the last bit.\n\nThe set mathbbF_p corresponds to the type arf in Flint. In Julia we can get the representations m cdot 2^e using significand and exponent and the representation m cdot 2^e using frexp.\n\nusing Arblib\nf = Arf(3 * 2^4)\nsignificand(f), exponent(f)\nfrexp(f)\n\nMost floating point types are however more restrictive than arf. They generally also put restrictions on the range of the exponent. They correspond to the set\n\nmathbbF_pcheckyhaty = x cdot 2^y x y in mathbbZ 1 leq x  2^p x textodd checky leq y leq haty cup 0 infty -infty textNaN\n\nUsually the bounds for the exponent are given in terms of e. For example for BigFloat they are -2^62 leq e leq 2^62 - 2 and for Float64 they are -1022 leq e leq 1023. We can find this using Julia as\n\nexponent(floatmax(BigFloat))\n2^62 - 2\nexponent(floatmin(BigFloat))\n-2^62\nexponent(floatmax(Float64))\nexponent(floatmin(Float64))\n\nThe definitions we have gone through so far are enough for our study of floating point numbers. Many common floating point types, primarily Float64, do however have a few more details that in some cases are important:\n\nSigned zero: The value zero can also have a sign, meaning that 0 and -0 are distinct values. This is useful in some cases, but mathematically it can give some odd behavior. We will in general not consider signed zeros.\n0.0\n-0.0\n0.0 == -0.0\nisequal(0.0, -0.0)\nSubnormal numbers: In general the mantissa satisfies 1 leq m  2. For the numbers closest to zero it however allows representations where m  1. We won't care about this at all.\nNaN with payload: The NaN values can have extra data associated with them. This is for example used in the R programming language to represent missing values. We won't care about this at all.\n\nNote that the Arf floating point type does not support any of these things.","category":"section"},{"location":"week-5-lecture-1/#The-\"field\"-of-floating-point-numbers-and-rounding","page":"Week 5 Lecture 1: Floating point numbers","title":"The \"field\" of floating point numbers and rounding","text":"We now know what floating point numbers are, the next step is to be able to do computations on floating point numbers. In this lecture we will focus on basic arithmetic, i.e. addition, subtraction, multiplication and division. In the next lecture we will look at powers and elementary functions.\n\nExcept for the special values pm infty and NaN, floating point numbers are a subset of the rational numbers. We therefore have a natural definition for what it means to add, subtract, multiply and divide floating point numbers with each other. There is however a problem, the set of floating point numbers is not closed under these operations.\n\nnote: Example\nConsider the floating point numbers a = 3 cdot 2^0 and b = 5 cdot 2^0 in the set mathbbF_3. We havec = a cdot b = 15 cdot 2^0But 15 cdot 2^0 is not in the set mathbbF_3 since 15  2^3.\n\nTo fix this problem we have to introduce the concept of rounding.","category":"section"},{"location":"week-5-lecture-1/#Rounding","page":"Week 5 Lecture 1: Floating point numbers","title":"Rounding","text":"A rounding function is a function\n\nbigcirc mathbbR to mathbbF_p\n\nTo be a proper rounding function it should satisfy\n\nbigcirc(x) = x for x in mathbbF_p\nbigcirc(x) leq bigcirc(y) if x leq y\n\nWe will look at three different versions of rounding:\n\nRound down: nabla(x)\nRound up: Delta(x)\nRound to nearest (even): square(x)\n\nThe round down and up are defined by\n\nnabla(x) = max y in mathbbF_p y leq x\nDelta(x) = min y in mathbbF_p y geq x\n\nThat is, they return the floating point number just below or just above x. Round to nearest on the other hand returns the floating point number closest to x, with ties being rounded to the one of two which has a 0 in the last digit of the mantissa.\n\nFor general use round to nearest is the by far most common rounding mode to use. For the specific purposes of interval arithmetic round up and round down will however be very important.\n\nnote: Note\nThe rounding function implicitly depends on the precision p of the set we are rounding into. In some cases one might want to use the notation bigcirc_p to make this dependency explicit.","category":"section"},{"location":"week-5-lecture-1/#Arithmetic-with-rounding","page":"Week 5 Lecture 1: Floating point numbers","title":"Arithmetic with rounding","text":"With our new rounding functions we can now define arithmetic on floating points!\n\nWhen adding two floating point numbers x y in mathbbF_p under the rounding mode bigcirc the result is given by z = bigcirc(x + y). Here the operation x + y is the exact addition, corresponding to addition of rational numbers. We similarly have that bigcirc(x - y), bigcirc(x cdot y) and bigcirc(x  y) are the floating point operations corresponding to subtraction, multiplication and division under rounding mode bigcirc. In some cases we will use the notation bigcirc(+) to denote floating point addition under the rounding mode bigcirc (so nabla(+) for round down and Delta(+) for round up).\n\nA natural question is, how does the computer compute bigcirc(a + b)? The value a + b is not (necessarily) representable as a floating point number, how can the computer then work with it? The key is to remember that both a and b are of the form x cdot 2^y. If we write a = x_a cdot 2^y_a and b = x_b cdot 2^y_b then\n\na cdot b = x_ax_b cdot 2^y_a + y_b\n\nComputing this number requires multiplying the two integers x_a and x_b and adding the integers y_a and y_b. The computer can this; it is just integer arithmetic! Rounding the result then requires some manipulation of these integers, but throughout the computations we are only working with integers.\n\nnote: Note\nIn practice most floating point implementations do not compute e.g. x_ax_b directly. They use more optimized routines that handle the multiplication and the rounding in one step. At the most fundamental level everything is however just integer arithmetic.\n\nWith this we finally have a mathematically well defined notion of arithmetic for floating points! This definition does however come with a number of awkward mathematical properties. These issues are a result of the problem that rounding does not compose. For example, bigcirc(bigcirc(x + y) + z) is in general not the same as bigcirc(x + bigcirc(y + z)) and neither of them are (in general) the same as bigcirc(x + y + z). This means that floating point addition is non-associative (it is however commutative).\n\nWorking with non-associative operations is in general very tedious, it means you have to be extremely careful when you specify your expressions to make sure that the order of the operations is well defined.\n\nnote: Example\nConsider the sumS = sum_n = 1^10^4 frac1nIf we were to exactly compute this sum and then round it to a floating point then we would get the value bigcirc(S). If we want to use floating points to compute the sum we round each individual operation. As a first step we can round the divisions, giving usS = sum_n = 1^10^4 bigcirc(1  n)If we were to perform this sum exactly we would get the floating point number bigcirc(S). Of course, we also have rounding when computing the sum. If we sum from the left we getS_l = bigcirc(cdotsbigcirc(bigcirc(bigcirc(1  1) + bigcirc(1  2)) + bigcirc(1  3))cdots)if we sum from the right we getS_r = (cdots bigcirc(bigcirc(1  (10^4 - 2)) + bigcirc(bigcirc(1  (10^4 - 1)) + bigcirc(1  10^4)))cdots)In Julia this would correspond toS_l = 0.0\nfor n in 1:10^4\n    S_l += 1.0 / n\nend\nS_l\n\nS_r = 0.0\nfor n in reverse(1:10^4)\n    S_r += 1.0 / n\nend\nS_rWe can see that these mostly agree, but are not exactly the same. We can also compute the entire sum exactly using rational numbers and then round to a floating point. This gives usS = Float64(sum(n -> 1 // BigInt(n), 1:10^4))We can compare this to S_l and S_rS_l - S\nS_r - SThe sum function in Julia instead does a pairwise summation, this in general gives much smaller rounding errors.S - sum(n -> 1.0 / n, 1:10^4)","category":"section"},{"location":"week-1-lecture-1/#Week-1-Lecture-1-Introduction-to-computer-assisted-proofs","page":"Week 1 Lecture 1 - Introduction to computer-assisted proofs","title":"Week 1 Lecture 1 - Introduction to computer-assisted proofs","text":"The main goal of this first lecture is to give you an idea of what to expect about the course. We will take a look at the structure of the course and an overview of the content we will cover.\n\nThe course consists of 15 weeks which we will split into 3 different parts. The 3 parts are\n\nIntroduction to computer-assisted proofs (≈ Week 1-4)\nIntroduction to rigorous numerics (≈ Week 5-10)\nComputer-assisted proofs in practice (≈ Week 11-15)\n\nEach week will follow roughly the same pattern. The Monday and Wednesday sessions will be lectures where I present material, the Friday session will be a computer lab where you get to try out what we have discussed during the lectures.","category":"section"},{"location":"week-1-lecture-1/#Computer-labs","page":"Week 1 Lecture 1 - Introduction to computer-assisted proofs","title":"Computer labs","text":"For the computer labs we will make use of Julia. We will talk more about Julia during the first computer lab on Friday. A very brief description is that the Julia language is designed for high performance scientific computing. Version 1.0 was released in 2018, so it is a relatively young language compared to for example C, Fortran, Python and Matlab.\n\nThe main goal for the Friday session is to make sure that you are all able to install Julia and get it up and running. In general the course will not assume familiarity with Julia, it should hopefully be possible for you to pick it up as we go.\n\nThis web page you are reading now is generated using Julia. In fact, the repository associated with this course is structured as a Julia package. We will talk more about this later on.","category":"section"},{"location":"week-1-lecture-1/#Part-1:-Introduction-to-computer-assisted-proofs","page":"Week 1 Lecture 1 - Introduction to computer-assisted proofs","title":"Part 1: Introduction to computer-assisted proofs","text":"What is a computer-assisted proof? This is not an obvious question and the answer will depend on who you ask. For the purposes of this course there will be an answer to this question, but getting there requires a bit of background.\n\nLet us start by considering a slightly different question, what is computer-assisted mathematics? In this case it could be more or less anything that involves a computer. Examples could include\n\nNumerical simulations, such as solving some PDE using numerical methods or computing images of the Cantor set.\nAnalysing data, in particular in applied mathematics you might have actual datasets you want to analyse. But you could also analyse large databases of knots.\nSymbolical computations, for example computing integrals or handling very large expressions using e.g. Sage, WolframAlpha, Mathematica or Maple.\nUsing LLMs or other similar tools for solving mathematical problems or writing manuscripts.\n\nMore or less anything where you make use of the computer to help you in your mathematical research. One could even include things like:\n\nUsing Arxiv to find articles.\nWriting your article in LaTeX.\nCommunicating with your collaborators using email.\n\nWe are however interested in something slightly different, not computer-assisted mathematics but computer-assisted proofs. The name implies that it should involve proofs, but that is true for most of mathematics so doesn't restrict us much. We will however mean something more specific, for our purposes a computer-assisted proof is a mathematical proof of some mathematical statement which requires a computer to verify. The key here is that the computer is involved in the verification, not only the construction of the proof. Some things which are not computer-assisted proofs with this definition are:\n\nUsing an LLM to generate a human readable mathematical proof.\nUsing numerical simulations to generate an hypothesis which is then proved using pen and paper.\nUsing symbolical computations to compute a nasty integral, where the answer can be verified by hand.\n\nWhat would be an example of a computer-assisted proof then? What does it mean to use a computer to verify a proof? The prototypical example would be something that requires a lot of calculations. Showing that the fifth decimal in pi is 9 you could do by hand, showing that the millionth decimal is 1 you probably couldn't. There is, however, nothing fundamentally different between computing the fifth decimal or the millionth decimal. In theory you could compute the millionth decimal by hand, it would just take you a veeery long time. For the computer it takes less than a second.\n\nFor the first part of the course we will look at three different kinds of computer-assisted proofs:\n\nComputer-assisted proofs for discrete problems\nComputer-assisted proofs for continuous problems\nFormal proofs\n\nFor discrete problems it is relatively easy to imagine that computers could be helpful. An example is problems which reduce to checking a finite number of cases. The most famous problem in this setting is probably the four color theorem, which says that any map can be colored using four colors in such a way that no two adjacent regions have the same color. This was proved in 1976 using a computer-assisted proof. For this they reduced the problem, using pen and paper, to checking 1834 possible counterexamples. These 1834 possible counterexamples were then checked to be four colorable with the help of the computer.\n\nFor continuous problems it is not as obvious how computers could be used. Numerical analysis is a field of mathematics which deals with computing approximate solutions to continuous problems. When numerically solving a problem you introduce discretization errors and rounding errors and these make it so that the final result cannot be fully trusted. A good numerical method will give you a good approximation most of the time, but it is in general not proved to always do so. For mathematical proofs these errors are problematic, it is not enough for the result to be approximately correct. How to deal with this is what most of this course will be about. We look at a subfield of numerical analysis called rigorous numerical analysis, which allows us to control these errors.\n\nFinally, we will talk about formal proofs. Wikipedia gives the following description of formal proofs\n\nIn logic and mathematics, a formal proof or derivation is a finite sequence of sentences (known as well-formed formulas when relating to formal language), each of which is an axiom, an assumption, or follows from the preceding sentences in the sequence, according to the rule of inference. It differs from a natural language argument in that it is rigorous, unambiguous and mechanically verifiable.\n\nFormal proofs are not necessarily computer-assisted, though for anything non-trivial the size of the expressions quickly outgrow anything a human could verify and in practice formal proofs hence require computers for the verification. For writing formal proofs one makes use of special purpose software called proof assistants or interactive theorem provers, example of such softwares are:\n\nIsabelle - Cambridge 1986\nRocq (previously named Coq) - Inria 1989\nAgda - Chalmers 2007 (1999)\nIdris - Edwin Brady 2007\nLean - Microsoft Research 2013\n\nIn particular the last one, Lean, has gained a lot of moment in the last couple of years.\n\nUsing the term computer-assisted for a formal proof is however maybe slightly misleading. In this case the computer is not merely assisting in verifying the proof, it is doing the entire verification completely by itself. We will talk a little bit about formal proofs later in the course, but only with the goal of understanding the difference between a formal proof and a regular computer-assisted proof.\n\nnote: Note\nThe definition of a computer-assisted proof that we use in here, that the computer is used in the verification of the proof, not only the construction, is not universally used by all mathematicians. For example Terence Tao has a talk on what he calls Machine-Assisted Proofs, where he includes both what we in this course call computer-assisted proofs but also for example using LLMs for generating proofs. He also uses the terminology machine-assisted rather than computer-assisted, the reason being that the word computer originates from the name for human computers doing computations.","category":"section"},{"location":"week-1-lecture-1/#Part-2:-Introduction-to-rigorous-numerics","page":"Week 1 Lecture 1 - Introduction to computer-assisted proofs","title":"Part 2: Introduction to rigorous numerics","text":"The second part of the course is where we will actually start learning how to build computer-assisted proofs. We will look at the field of rigorous numerical analysis, which is a subfield of numerical analysis. The goal of numerical analysis is to compute approximations, this is also true for rigorous numerical analysis. The difference with rigorous numerical analysis is that in addition to computing an approximation you also compute rigorous upper bounds for the error of your approximation.\n\nAs a simple example, consider the problem of computing e^2. In classical analysis you would do\n\nexp(2)\n\nWhich tells you that e^2 approx 738905609893065 In rigorous numerics you would also compute an approximation, but you would include an upper bound on the error of your approximation.\n\nusing Arblib\nexp(Arb(2, prec = 53))\n\nWhich again tells you that e^2 approx 738905609893065, but it now includes the extra information that the error of this approximation is at most 139 cdot 10^-15, so that e^2 in 738905609893065 pm 139 cdot 10^-15. How is this upper bound for the error computed? That is what we will talk about! It is both fairly technical and surprisingly easy.\n\nTo achieve this we will make use of something called interval arithmetic. In regular numerical analysis one works with floating points, these are inherently approximations since they cannot represent most real numbers exactly. In interval arithmetic one works with pairs of floating points, one representing a lower bound and one representing an upper bound. So instead of e.g. pi approx 3141592653589793 we would have pi in 3141592653589793 31415926535897936. Another format is to use one floating point representing the midpoint and another the radius, so pi in 314159265358979 pm 334 cdot 10^-15. When doing this it is not a problem that most real numbers cannot be represented by floating points, we can always pick the bounds to be floating points. Once we have an interval representation we will still need to do computations with them, exactly how to do this in a rigorous way is probably the most technical part of rigorous numerics.\n\nThe content we will cover over the six weeks are:\n\nMathematical foundations of floating point arithmetic: floating point formats, rounding\nBasics of interval arithmetic: basic arithmetic, elementary functions, special functions\nBasic rigorous numerics: isolating roots, computing integrals, enclosing extrema\nAutomatic differentiation: forward (and backwards) differentiation, Taylor arithmetic\nImproved rigorous numerics: isolating roots, computing integrals, enclosing extrema\n\nA sneak peak at some of the things we will learn how to do, in this case using the Arblib.jl package for interval arithmetic package and some algorithms implemented in ArbExtras.jl.\n\nEnclose roots of functions. For example finding the unique zero of x + e^x on the interval -06 -05\n\nusing Arblib, ArbExtras\nArbExtras.refine_root(x -> x + exp(x), Arb((-3 // 5, -1 // 2)))\n\nEnclose integrals of analytic functions. For example enclosing int_0^5 sin(e^x) dx\n\nusing Arblib\nArblib.integrate(x -> sin(exp(x)), 0, 5)\n\nEnclose the minimum of the function. For example enclosing the minimum of the Bessel function J_4(x) on the interval 1 2.\n\nusing Arblib, ArbExtras, SpecialFunctions\nArbExtras.minimum_enclosure(x -> besselj(Arb(4), x), Arf(1), Arf(2))","category":"section"},{"location":"week-1-lecture-1/#Part-3:-Computer-assisted-proofs-in-practice","page":"Week 1 Lecture 1 - Introduction to computer-assisted proofs","title":"Part 3: Computer-assisted proofs in practice","text":"In the last part of the course we will look at how computer-assisted proofs are actually used in the literature. The goal will be to look at examples of papers making use of computer-assisted proofs. Exactly how we do this and what we will look is however yet to be determined and will depend on your interests. Some areas we could take a closer look at are:\n\nPDEs: This is the field most of my research takes place in.\nDynamical systems: This is probably the field with the longest history of computer-assisted proofs and there is a number of interesting things we could look at here.\n\nOne could also discuss things on a more meta level:\n\nWhat exactly does it take to publish a paper with a computer-assisted proof? How does one prepare the code? How does one publish the code? How does one connect the paper and the code?\nWhat type of problems are amendable to a computer-assisted approach? These are things we will touch upon during the course, but one could maybe gain something from discussing it in more detail.\n\nOne could also dive deeper into different algorithms for computer-assisted proofs:\n\nRigorous integration of ODEs\nFinite element methods\nSpectral methods\nPhysics-Informed Neural Networks (PINNs)\n\nAlternatively, one can study the lower level details of interval arithmetic, more related to the field of computer algebra.\n\nWe don't have to decide what to do yet, but as we get further into the course we'll come back to this.","category":"section"},{"location":"week-12-lab/#Week-12-Lab:-TODO","page":"Week 12 Lab: TODO","title":"Week 12 Lab: TODO","text":"","category":"section"},{"location":"week-7-lecture-1/#Week-7-Lecture-1:-TODO","page":"Week 7 Lecture 1: TODO","title":"Week 7 Lecture 1: TODO","text":"","category":"section"},{"location":"week-11-lecture-1/#Week-11-Lecture-1:-TODO","page":"Week 11 Lecture 1: TODO","title":"Week 11 Lecture 1: TODO","text":"","category":"section"},{"location":"week-2-lecture-1/#Week-2-Lecture-1:-Discrete-problems","page":"Week 2 Lecture 1: Discrete problems","title":"Week 2 Lecture 1: Discrete problems","text":"In this lecture we'll take a look at some high profile computer-assisted proofs for discrete problems. The goal here is not to fully understand these proofs, but rather to see examples of problems for which computer-assistance could be beneficial.\n\nWe will look at three different problems:\n\nThe four color theorem\nThe boolean Pythagorean triples problem\nGoldbach's weak conjecture\n\nSome takeaways from these examples are:\n\nInfinite problems can sometimes be reduced to finite problems using a pen and paper analysis. In some cases combined with a computer-assisted part.\nModern computers are very fast and can handle a huge number of computations.\nFor some problems the number of computations scale extremely fast.\nComputers are getting faster and faster, what was a monumental effort in the 70's is trivial today.","category":"section"},{"location":"week-2-lecture-1/#The-four-color-theorem","page":"Week 2 Lecture 1: Discrete problems","title":"The four color theorem","text":"This problem was mentioned already in the first lecture and is one of the first, and likely the most famous, computer-assisted proofs.\n\nnote: Four color theorem (1976)\nAny map can be colored using four colors in such a way that no two adjacent nodes have the same color.\n\n(Image: Four colored map of US states)\n\nInitially this doesn't look like a good problem for a computer-assisted proofs. It is straightforward to check if a specific map is four colorable. One simply writes a program that searches for four colorings. There are, however, an infinite number of maps and there is no way to check all of them with the computer.\n\nThe first, and probably most important, part of the proof is therefore to reduce it to a finite number of cases. We will not go into details on how this is done, but the general idea is to start by assuming that there is a minimal counterexample. They then make use of two related concepts:\n\nShow there exists a finite unavoidable set. A set of configurations such that every map that satisfies some necessary conditions for being a minimal counterexample must have at least one configuration from this set.\nReducible configurations, a configuration that cannot occur in a minimal counterexample. If a map contains a reducible configuration then the map can be reduced to a smaller map which, if it is four colorable, implies that the original map is four colorable. This in particular implies that the original map could not be a minimal counterexample.\n\nThe result then follows if one could show that all of the configurations in the finite unavoidable set are reducible configurations. In the original proof the unavoidable set consisted of 1834 configurations and was later reduced to 1,482. Each of these configurations then had to be checked to be reducible. Just finding the set of unavoidable configurations required a significant amount of work, but to my understanding was in large part done by hand. Checking that each of the unavoidable configurations were reducible was however done by the computer. Since this was still in the very early days of computers there was also a lot of manual labor involved in the process at this point.\n\nIn 2005 the proof was formalized in Rocq (previously called Coq), which we will talk more about in Week 4.","category":"section"},{"location":"week-2-lecture-1/#The-boolean-Pythagorean-triple-problem","page":"Week 2 Lecture 1: Discrete problems","title":"The boolean Pythagorean triple problem","text":"The Pythagorean triple problem asks whether it is possible to color each of the positive integers either red or blue, so that no Pythagorean triple of integers (a b c) satisfying a^2 + b^2 = c^2 are all the same color? This was shown to be false in 2016, see also this website with some more information. More precisely we have the following theorem.\n\nnote: Boolean Pythagorean triples theorem (2016)\nThe set 1 dots 7824 can be partitioned into two parts, such that no part contains a Pythagorean triple, while this is impossible for 1 dots 7825.\n\nThis problem is inherently finite, and it is maybe easier to imagine that it could be done through a computer-assisted proof.\n\nLet us start with the first part, showing that 1 dots 7824 can be partitioned into two parts such that no part contains a Pythagorean triple. If we are given a partitioning then it is a relatively straightforward exercise to verify that it satisfies the condition. There is around 10000 Pythagorean triples below 7824, so a very dedicated person could even do it by hand given enough time. In practice this check is computer-assisted. Of course, one first has to find a candidate partitioning, this uses a tool known as a SAT solver. SAT solvers are very useful tools for computer-assisted proofs, but tend to not play big role in analysis problems so we won't talk about it in this course.\n\nThe second part, showing that this is impossible for 1 dots 7825, is again a finite problem. In this case it is however not enough to find one partitioning, instead we have to verify that it is impossible for any partitioning. There are, however, 2^7825 approx 363 times 10^2355 different ways to partition this set and checking that all of these partitions contain a Pythagorean triple is simply not feasible.\n\nnote: Note\nIn cryptography one usually assumes that 2^128 approx 34 times 10^38 is larger than what anyone could bruteforce. Using all the energy in the observable universe one could get as far as around 2^320.\n\nHowever, the problem has a lot of symmetry and they managed to reduce the problem to around a trillion cases. The proof that none of these trillion partitions contain a Pythagorean triple consists of around 200 terabytes of propositional logic. This made it the largest proof ever. This does however compress to a mere 68 gigabytes in the end. Again, the actual calculation is done using a SAT solver.","category":"section"},{"location":"week-2-lecture-1/#Goldbach's-weak-conjecture","page":"Week 2 Lecture 1: Discrete problems","title":"Goldbach's weak conjecture","text":"Goldbach's weak conjecture is a famous conjecture in number theory.\n\nnote: Goldbach's weak conjecture\nEvery odd number greater than 5 can be written as the sum of three primes.\n\nSimilar to the Four color theorem this is again a problem that a priori requires checking an infinite number of cases. In this case it was however reduced to a finite computation already in 1956, though with an upper bound e^e^16038 approx 8 times 10^4008659, way too large to make a bruteforce approach of the finite number of remaining numbers feasible.\n\nIn 2013 the conjecture was computationally confirmed up to 8 875 694 145 621 773 516 800 000 000 000 approx 8875 cdot 10^30, see this paper. There is also a number of earlier results not going quite as far. The computation makes use of a number of tricks to reduce the computational time, but eventually boils down to a large brute force check requiring about 40 000 core hours.\n\nSimultaneously there was progress on improving the bound after which the conjecture could be proved to hold for all odd numbers. This number was eventually brought down to 10^27 by Harald Helfgott in 2013. Together with the computation above this gave a full proof of the result. The paper was accepted for publication in Annals in 2015, though it seems like the final version has not actually been finished. Reducing the bound to 10^27 does by itself rely on a computer-assisted proof. It uses tools from analysis and in this case it therefore makes use of rigorous numerics for the computations. The theory is however fairly involved and not something we will dive deeper in.","category":"section"},{"location":"week-4-lab/#Week-4-Lab:-Trying-Lean","page":"Week 4 Lab: Trying Lean","title":"Week 4 Lab: Trying Lean","text":"Since this is the only lab we will use Lean for, we will avoid actually installing Lean and instead make use of online versions.\n\nOne way to learn Lean is the Natural Number Game, originally developed by Kevin Buzzard. This introduces you to the basics of Lean by having you prove basic statements related to natural numbers.\n\nFor this lab, however, we will not use this game, but instead look at Lean proofs for some (very) basic analysis results. For this we will use Live Lean to give us access to Lean without installing it.\n\nWe will start by proving that a constant sequence converges. Copy and paste the following code into Live Lean. During the lab we will go through this example together; the details are not contained in these notes.\n\nimport Mathlib.Data.Real.Basic\n\n-- 1. We define what it means for a sequence `s` to converge to a limit `a`.\n--    Definition: For all ε > 0, there exists an N, such that for all n ≥ N, |s(n) - a| < ε.\ndef converges_to (s : ℕ → ℝ) (a : ℝ) :=\n  ∀ ε > 0, ∃ N, ∀ n ≥ N, |s n - a| < ε\n\n-- 2. We state the theorem: The sequence (λ n ↦ c) converges to c.\ntheorem limit_const (c : ℝ) : converges_to (λ n ↦ c) c := by\n  -- Let ε be an arbitrary real number, and assume ε > 0 (hypothesis hε)\n  intro ε hε\n\n  -- We need to find an N. Since the sequence is constant, any N works.\n  -- Let's use N = 0.\n  use 0\n\n  -- Now let n be any natural number, and assume n ≥ 0 (hypothesis hn).\n  intro n hn\n\n  -- State is: |(λ n ↦ c) n - c| < ε\n\n  -- Step 1: Apply the function definition\n  -- `dsimp` performs the beta-reduction: (λ n ↦ c) n ==> c\n  dsimp\n\n  -- State is now: |c - c| < ε\n\n  -- Step 2: Arithmetic simplification\n  -- We rewrite using the theorem `sub_self` (which states ∀ a, a - a = 0)\n  rw [sub_self]\n\n  -- State is now: |0| < ε\n\n  -- (Optional Step 3): Simplify the absolute value\n  -- We rewrite using `abs_zero` (which states |0| = 0)\n  rw [abs_zero]\n\n  -- Now the goal is 0 < ε. This is exactly our hypothesis hε.\n  exact hε\n\nNext we will prove that the identity function is continuous. Copy and paste the following code into what you already have.\n\ndef continuous_at (f : ℝ → ℝ) (x₀ : ℝ) :=\n  ∀ ε > 0, ∃ δ > 0, ∀ x, |x - x₀| < δ → |f x - f x₀| < ε\n\ntheorem continuous_id (x₀ : ℝ) : continuous_at (λ x ↦ x) x₀ := by\n  -- Let ε > 0 be given\n  intro ε hε\n\n  -- Choose δ = ε\n  use ε\n\n  -- We must show δ > 0 and the implication.\n  -- The `constructor` tactic splits the \"and\" in the existence claim\n  -- (∃ δ, (δ > 0) ∧ (condition))\n  constructor\n\n  -- Subgoal 1: Show δ > 0. Since δ = ε and ε > 0, this is true.\n  exact hε\n\n  -- Subgoal 2: Show that if |x - x₀| < δ then |f x - f x₀| < ε\n  intro x hx\n  -- The function is f(x) = x, so |f x - f x₀| is just |x - x₀|.\n  -- We already know |x - x₀| < δ, and δ = ε.\n  dsimp\n  exact hx","category":"section"},{"location":"week-13-lab/#Week-13-Lab:-TODO","page":"Week 13 Lab: TODO","title":"Week 13 Lab: TODO","text":"","category":"section"},{"location":"week-13-lecture-1/#Week-13-Lecture-1:-TODO","page":"Week 13 Lecture 1: TODO","title":"Week 13 Lecture 1: TODO","text":"","category":"section"},{"location":"week-3-lecture-2/#Week-3-Lecture-2:-Computer-assisted-proofs-for-continuous-problems","page":"Week 3 Lecture 2: Computer-assisted proofs for continuous problems","title":"Week 3 Lecture 2: Computer-assisted proofs for continuous problems","text":"In the previous lecture, we examined an example of a computer-assisted proof. The goal was to prove that the first Dirichlet eigenvalue of regular polygons, mathbbP_N, is decreasing with respect to the number of vertices, N. We focused on proving this for a finite number of polygons, specifically N = 3 4 dots 64.\n\nIn this lecture, we will look at another example of a computer-assisted proof, this time focusing on waves. Before proceeding, let us briefly review the example from the previous lecture and discuss the main takeaways.","category":"section"},{"location":"week-3-lecture-2/#Looking-back-at-regular-polygons","page":"Week 3 Lecture 2: Computer-assisted proofs for continuous problems","title":"Looking back at regular polygons","text":"We were interested in demonstrating that the first eigenvalue of regular polygons is decreasing for N = 3 4 dots 64. The approach was split into two parts:\n\nCompute approximate eigenvalues (and eigenfunctions) for each  polygon using the Method of Particular Solutions.\nCompute error bounds on the approximate eigenvalues using a  theorem by Fox, Henrici, and Moler.\n\nFrom the approximations and the error bounds, it is relatively straightforward to verify that the eigenvalue is decreasing. Step 1 utilizes classical numerics, whereas Step 2 introduces rigorous numerics. To obtain a bound for the error, we required an upper bound for:\n\nmu = fracsqrtOmegasup_x in partial Omegau_app(x)u_app_2\n\nThis, in turn, requires an upper bound for sup_x in partial Omegau_app(x) and a lower bound for u_app_2. We have therefore reduced the problem of computing error bounds for our approximation to computing bounds for specific properties of that approximation—in this case, the error on the boundary and the L^2 norm.\n\nMany computer-assisted proofs follow a path similar to the one above:\n\nCompute a numerical approximation using classical numerical  methods.\nUsing pen and paper, prove that there exists a solution within a  certain distance of the approximation, contingent on bounding  certain properties of the approximation.\nCompute bounds for these properties using rigorous numerical  methods.\nDepending on the problem, use the computed bounds to verify the  result (e.g., that eigenvalues are decreasing).\n\nThe difficulty of these parts depends highly on the specific problem. The first two steps can range from trivial to extremely difficult. After all, almost the entire field of numerical analysis is devoted to Step 1 and, to some extent, Step 2. The main part of this course will focus on Step 3: how to actually bound the necessary properties.\n\nThe goal of this week is to provide examples of how an interesting mathematical problem can be reduced to a rigorous numerics problem. We are starting from the \"top\"—the mathematical problem—and moving downwards to identify what must be bounded. In the second half of the course, Introduction to rigorous numerics, we will invert this approach. We will start from the \"bottom\"—floating-point arithmetic—and slowly work our way up to using it to bound quantities like sup_x in partial Omegau_app(x).","category":"section"},{"location":"week-3-lecture-2/#Highest-cusped-waves","page":"Week 3 Lecture 2: Computer-assisted proofs for continuous problems","title":"Highest cusped waves","text":"Let us now examine another computer-assisted proof, this time related to waves. This material is based on two papers:\n\nHighest Cusped Waves for the Burgers-Hilbert   Equation\nHighest cusped waves for the fractional KdV   equations\n\nThese papers largely follow the same approach but address different parameter values. We will primarily focus on the second paper.\n\nWe are interested in the fractional Korteweg-de Vries (KdV) equations in the periodic setting, given by:\n\nf_t + f f_x = D^alphaf_xquad text for  (x t) in mathbbT times mathbbR\n\nHere, D^alpha is the Fourier multiplier operator defined by:\n\nwidehatD^alphaf(xi) = xi^alphawidehatf(xi)\n\nwhere the parameter alpha can generally take any real value. For alpha = 2 and alpha = 1, the equation reduces to the classical KdV and Benjamin-Ono equations, respectively. For alpha = -2, one obtains the reduced Ostrovsky equation. For alpha = -1, it reduces to the Burgers-Hilbert equation. The first paper cited above treats the case alpha = -1, while the second treats alpha in (-1 0).\n\nWe are interested in traveling waves—solutions of the form f(x t) = varphi(x - ct), where c  0 denotes the wave speed. In this case, the equation reduces to:\n\n-c varphi + varphivarphi = D^alpha varphi\n\nThis equation possesses a branch of even, 2pi-periodic, smooth traveling wave solutions bifurcating from constant solutions. By numerically following this branch, we obtain a sequence of traveling waves. The result resembles the following figure, sourced from this paper.\n\n(Image: Traveling waves along bifurcation branch)\n\nNumerically, we observe that the waves approach a profile with a cusp at the peak. The existence of a wave with such a cusp is what we aim to prove. Below is a plot showing what these waves look like for varying values of alpha.\n\n(Image: Highest cusped traveling waves)\n\nOur proof of their existence is computer-assisted and is the subject of this section.","category":"section"},{"location":"week-3-lecture-2/#Proof-of-existence","page":"Week 3 Lecture 2: Computer-assisted proofs for continuous problems","title":"Proof of existence","text":"The procedure follows the same three steps as the earlier example:\n\nCompute a numerical approximation using classical numerical  methods.\nUsing pen and paper, prove that there exists a solution within a  certain distance of the approximation, assuming we can bound  certain properties of the approximation.\nCompute bounds for these properties using rigorous numerical  methods.\n\nBefore proceeding, we must massage the equation to make it more tractable. If we let u(x) = c - varphi(x), we can write the equation as:\n\nfrac12u^2 = -mathcalH^alphau\n\nwhere mathcalH is the operator:\n\nmathcalH^alphau(x) = D^alphau(x) - D^alphau(0)","category":"section"},{"location":"week-3-lecture-2/#Step-1:-Finding-a-numerical-approximation","page":"Week 3 Lecture 2: Computer-assisted proofs for continuous problems","title":"Step 1: Finding a numerical approximation","text":"Obtaining a high-quality numerical approximation requires significant effort; in particular, the asymptotic behavior near x = 0 requires careful analysis. However, the details are not critical for our current goal. Ultimately, we obtain an approximation of the form:\n\nu_alpha(x) = a_alpha0tildeC_1 - alpha(x)\n  + sum_j = 1^N_alpha0 a_alphajtildeC_1 - alpha + jp_alpha(x)\n  + sum_n = 1^N_alpha1 b_alphan(cos(nx) - 1)\n\nHere, the function tildeC_s is a variant of the Clausen functions. The coefficients a_alphaj and b_alphan are carefully chosen to ensure a good numerical approximation.","category":"section"},{"location":"week-3-lecture-2/#Step-2:-Error-bounds-for-approximation","page":"Week 3 Lecture 2: Computer-assisted proofs for continuous problems","title":"Step 2: Error bounds for approximation","text":"The next step is to determine how to proceed from our numerical approximate solution to the existence of a true solution. Like many computer-assisted proofs, this relies on a fixed-point argument.\n\nFirst, we write our solution u as the approximation u_alpha plus a weighted perturbation. For our purposes, we can take the weight to be x, though in practice this must be adjusted based on alpha. Thus, we write u as:\n\nu(x) = u_alpha(x) + xv(x)\n\nwhere u_alpha is our numerical approximation and v is a perturbation. Our goal is to prove that there exists a v such that this expression satisfies the equation. Inserting this ansatz into the equation and solving for v yields:\n\nv + frac1xu_alphamathcalH^alphaxv =\n-frac1xu_alphaleft(\n  mathcalH^alphau_alpha + frac12u_alpha^2\nright) - fracx2u_alphav^2\n\nBy introducing the operators and functions:\n\nT_alphav = -frac1xu_alphamathcalH^alphaxvquad\nF_alpha(x) = frac1xu_alpha(x)left(mathcalH^alphau_alpha(x) + frac12u_alpha(x)^2right)quad\nN_alpha(x) = fracx2u_alpha(x)\n\nthe equation can be rewritten as:\n\n(I - T_alpha)v = -F_alpha - N_alphav^2\n\nAssuming that I - T_alpha is invertible, we have:\n\nv = (I - T_alpha)^-1left(-F_alpha - N_alphav^2right) = G_alphav\n\nProving the existence of a solution thus reduces to proving the existence of a fixed point for the operator G_alpha.\n\nFinally, one can show that if we let:\n\nn_alpha = N_alpha_L^infty(mathbbT)quad\ndelta_alpha = F_alpha_L^infty(mathbbT)quad\nD_alpha = T_alpha\n\nthen G_alpha has a fixed point provided D_alpha  1 (ensuring I - T_alpha is invertible) and:\n\ndelta_alpha  frac(1 - D_alpha)^24n_alpha\n\nNote that delta_alpha, D_alpha, and n_alpha depend only on our approximation u_alpha. We have thus completed Step 2: we have reduced the problem of proving existence to bounding specific properties of our numerical approximation.","category":"section"},{"location":"week-3-lecture-2/#Step-3:-Bounding-\\delta_{\\alpha},-D_{\\alpha}-and-n_{\\alpha}","page":"Week 3 Lecture 2: Computer-assisted proofs for continuous problems","title":"Step 3: Bounding delta_alpha, D_alpha and n_alpha","text":"The values delta_alpha, D_alpha, and n_alpha are all given by the supremum of a function on the interval 0 pi. More precisely:\n\nn_alpha = sup_x in 0 pi N_alpha(x)quad\ndelta_alpha = sup_x in 0 pi F_alpha(x)quad\nD_alpha = sup_x in 0 pi mathcalT_alpha(x)\n\nwith N_alpha and F_alpha as defined above, and:\n\nmathcalT_alpha(x) = frac1pi xu_alpha(x)\nint_0^piI_alpha(x y)y dy\n\nComputing bounds therefore reduces to bounding these three functions on the interval 0 pi. How to achieve this is what we will cover later in the course.\n\nnote: Note\nIn this specific case, when the weight is simply x, computing the supremum away from x = 0 is somewhat straightforward. However, near x = 0, more work is required due to the existence of removable singularities.When the weight is not just x, significantly more work is required to handle mathcalT_alpha. Near the endpoints alpha = -1 and alpha = 0, the approach also requires adjustments to succeed.","category":"section"},{"location":"week-5-lab/#Week-5-Lab:-Playing-with-floating-points","page":"Week 5 Lab: Playing with floating points","title":"Week 5 Lab: Playing with floating points","text":"In this we will experiment a bit with rounding and arithmetic for floating points. We will look at\n\nHow rounding interacts with basic arithmetic\nImplement a poor mans interval arithmetic version of sin\n\nFor this we will make use of the lab-5.jl Pluto notebook that you can find in the notebooks directory. Recall that you can start Pluto with\n\nusing Pkg\nPkg.activate(\".\")\nusing Pluto\nPluto.run()\n\nSee the Lab 1 and Lab 2 instructions for more details on how to start Pluto and potential issues you can encounter.","category":"section"},{"location":"week-8-lab/#Week-8-Lab:-TODO","page":"Week 8 Lab: TODO","title":"Week 8 Lab: TODO","text":"","category":"section"},{"location":"week-7-lecture-2/#Week-7-Lecture-2:-TODO","page":"Week 7 Lecture 2: TODO","title":"Week 7 Lecture 2: TODO","text":"","category":"section"}]
}
